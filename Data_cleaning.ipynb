{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c05870a-f905-4390-8266-88c0d56db13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading NOAA data: name 'noaa_file_path' is not defined\n",
      "Performing initial cleaning and type conversions...\n",
      "\n",
      "--- Unique Timezones Found in CZ_TIMEZONE ---\n",
      "['EST-5' 'CST-6' 'PST-8' 'MST-7' 'HST-10' 'AKST-9' 'AST-4' 'GST10']\n",
      "Number of unique timezone entries: 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# [Previous loading code remains the same]\n",
    "# ...\n",
    "try:\n",
    "    df_noaa = pd.read_csv(noaa_file_path, low_memory=False)\n",
    "    print(\"NOAA data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {noaa_file_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NOAA data: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Performing initial cleaning and type conversions...\")\n",
    "\n",
    "# --- Datetime Conversion ---\n",
    "noaa_datetime_format = '%d-%b-%y %H:%M:%S'\n",
    "df_noaa['BEGIN_DT'] = pd.to_datetime(df_noaa['BEGIN_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "df_noaa['END_DT'] = pd.to_datetime(df_noaa['END_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "\n",
    "begin_nat_count = df_noaa['BEGIN_DT'].isna().sum()\n",
    "end_nat_count = df_noaa['END_DT'].isna().sum()\n",
    "# ... (rest of initial datetime parsing checks) ...\n",
    "\n",
    "# --- *** INSPECT TIMEZONES *** ---\n",
    "print(\"\\n--- Unique Timezones Found in CZ_TIMEZONE ---\")\n",
    "unique_timezones = df_noaa['CZ_TIMEZONE'].unique()\n",
    "print(unique_timezones)\n",
    "print(f\"Number of unique timezone entries: {len(unique_timezones)}\")\n",
    "# --- *** END INSPECTION *** ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e6e334-201f-4388-8cb8-48d6f0bacc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NOAA data from: data/NOAA_StormEvents/StormEvents_2014_2024.csv\n",
      "NOAA data loaded successfully.\n",
      "Performing initial cleaning and type conversions...\n",
      "[Timestamp('2014-02-18 10:00:00-0500', tz='America/New_York')\n",
      " Timestamp('2014-03-30 08:31:00-0400', tz='America/New_York')\n",
      " Timestamp('2014-04-27 23:06:00-0500', tz='America/Chicago') ...\n",
      " Timestamp('2024-05-09 12:53:00-0500', tz='America/Chicago')\n",
      " Timestamp('2024-05-22 18:09:00-0400', tz='America/New_York')\n",
      " Timestamp('2024-08-06 07:52:00-0400', tz='America/New_York')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Define the path to the NOAA data file\n",
    "noaa_file_path = 'data/NOAA_StormEvents/StormEvents_2014_2024.csv'\n",
    "\n",
    "# Define columns that are likely numeric but might have issues during load\n",
    "numeric_cols_to_check = [\n",
    "    'BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH', 'END_DAY', 'END_TIME',\n",
    "    'EPISODE_ID', 'EVENT_ID', 'STATE_FIPS', 'CZ_FIPS',\n",
    "    'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT',\n",
    "    'MAGNITUDE', 'TOR_LENGTH', 'TOR_WIDTH',\n",
    "    'BEGIN_RANGE', 'END_RANGE', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON'\n",
    "]\n",
    "\n",
    "# Define the expected datetime format\n",
    "# Using errors='coerce' will turn unparseable dates into NaT (Not a Time)\n",
    "noaa_datetime_format = '%d-%b-%y %H:%M:%S'\n",
    "\n",
    "print(f\"Loading NOAA data from: {noaa_file_path}\")\n",
    "\n",
    "# --- Load the data ---\n",
    "# Consider using low_memory=False if dtype warnings appear, or specify dtypes more precisely\n",
    "# For very large files, consider chunking or libraries like Dask/Polars\n",
    "try:\n",
    "    df_noaa = pd.read_csv(noaa_file_path, low_memory=False)\n",
    "    print(\"NOAA data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {noaa_file_path}\")\n",
    "    # Exit or handle error appropriately\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NOAA data: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Performing initial cleaning and type conversions...\")\n",
    "\n",
    "# --- Datetime Conversion ---\n",
    "# Convert BEGIN_DATE_TIME and END_DATE_TIME\n",
    "# errors='coerce' handles unparseable formats by setting them to NaT\n",
    "df_noaa['BEGIN_DT'] = pd.to_datetime(df_noaa['BEGIN_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "df_noaa['END_DT'] = pd.to_datetime(df_noaa['END_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "\n",
    "# Check for parsing errors (NaT values)\n",
    "begin_nat_count = df_noaa['BEGIN_DT'].isna().sum()\n",
    "end_nat_count = df_noaa['END_DT'].isna().sum()\n",
    "if begin_nat_count > 0 or end_nat_count > 0:\n",
    "    print(f\"Warning: Found {begin_nat_count} NaT values in BEGIN_DT after parsing.\")\n",
    "    print(f\"Warning: Found {end_nat_count} NaT values in END_DT after parsing.\")\n",
    "    # Consider dropping or investigating rows with NaT datetimes if they are critical\n",
    "    # df_noaa.dropna(subset=['BEGIN_DT', 'END_DT'], inplace=True)\n",
    "\n",
    "\n",
    "# --- Timezone Handling ---\n",
    "# Map common timezone abbreviations to standard Olson names usable by pandas\n",
    "# This might need expansion based on unique values in CZ_TIMEZONE\n",
    "tz_map = {\n",
    "    # Standard US Timezones (using Olson names that handle DST)\n",
    "    'EST-5': 'America/New_York',    # Eastern Time\n",
    "    'EDT-4': 'America/New_York',    # Eastern Time (Daylight)\n",
    "    'CST-6': 'America/Chicago',     # Central Time\n",
    "    'CDT-5': 'America/Chicago',     # Central Time (Daylight)\n",
    "    'MST-7': 'America/Denver',      # Mountain Time (most areas)\n",
    "    'MDT-6': 'America/Denver',      # Mountain Time (most areas - Daylight) - Added MDT just in case although not in list\n",
    "    'PST-8': 'America/Los_Angeles', # Pacific Time\n",
    "    'PDT-7': 'America/Los_Angeles', # Pacific Time (Daylight)\n",
    "    'AKST-9': 'America/Anchorage',  # Alaska Time\n",
    "    'AKDT-8': 'America/Anchorage',  # Alaska Time (Daylight) - Added AKDT just in case\n",
    "    'HST-10': 'Pacific/Honolulu',   # Hawaii Standard Time (no DST)\n",
    "\n",
    "    # Atlantic & Territories\n",
    "    'AST-4': 'America/Puerto_Rico', # Atlantic Standard Time (no DST in PR)\n",
    "    'GST10': 'Pacific/Guam',        # Guam Standard Time (UTC+10)\n",
    "    'SST-11': 'Pacific/Pago_Pago',   # Samoa Standard Time (UTC-11)\n",
    "\n",
    "    # Add mappings for any potential NaN or empty strings if they exist\n",
    "    '': None, # Map empty string explicitly if needed\n",
    "    # np.nan: None # pd.isna() check in function should handle actual NaN objects\n",
    "}\n",
    "\n",
    "# Function to apply timezone localization\n",
    "def localize_datetime(row):\n",
    "    tz_str = row['CZ_TIMEZONE']\n",
    "    dt = row['datetime_col']\n",
    "    if pd.isna(dt) or pd.isna(tz_str):\n",
    "        return pd.NaT\n",
    "\n",
    "    tz_name = tz_map.get(tz_str)\n",
    "    if tz_name:\n",
    "        try:\n",
    "            # Localize the naive datetime\n",
    "            return dt.tz_localize(tz_name, ambiguous='NaT', nonexistent='NaT')\n",
    "        except Exception as e:\n",
    "            # Log warning for specific row/error if needed\n",
    "            # warnings.warn(f\"Could not localize timezone '{tz_str}' for datetime {dt}: {e}\")\n",
    "            return pd.NaT # Failed to localize\n",
    "    else:\n",
    "        # Log warning for unmapped timezone if needed\n",
    "        # warnings.warn(f\"Timezone '{tz_str}' not found in tz_map.\")\n",
    "        return pd.NaT # Timezone not in map\n",
    "\n",
    "# Apply localization - requires iterating or a more complex apply\n",
    "# Create temporary column for the function\n",
    "df_noaa['datetime_col'] = df_noaa['BEGIN_DT']\n",
    "df_noaa['BEGIN_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "df_noaa['datetime_col'] = df_noaa['END_DT']\n",
    "df_noaa['END_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "unique_dt = df_noaa['BEGIN_DT_LOC'].unique()\n",
    "\n",
    "print(unique_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07f51b1-e50e-480c-92ab-2feada827b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2014-02-18 10:00:00-05:00\n",
       "1         2014-03-30 08:31:00-04:00\n",
       "2         2014-04-27 23:06:00-05:00\n",
       "3         2014-04-27 23:03:00-05:00\n",
       "4         2014-02-15 13:00:00-08:00\n",
       "                    ...            \n",
       "691429    2024-05-26 11:48:00-04:00\n",
       "691430    2024-05-22 18:09:00-04:00\n",
       "691431    2024-05-22 17:57:00-04:00\n",
       "691432    2024-06-23 17:45:00-04:00\n",
       "691433    2024-08-06 07:52:00-04:00\n",
       "Name: BEGIN_DT_LOC, Length: 691434, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noaa['BEGIN_DT_LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33d7e51-7dd6-4e79-bf4f-633dbc9c4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NOAA data from: data/NOAA_StormEvents/StormEvents_2014_2024.csv\n",
      "NOAA data loaded successfully.\n",
      "Performing initial cleaning and type conversions...\n",
      "Parsing original datetime strings...\n",
      "Mapping timezones...\n",
      "Applying timezone localization (this may take time)...\n",
      "Timezone localization applied.\n",
      "Attempting to convert localized columns directly to UTC...\n",
      "Direct UTC conversion attempted.\n",
      "Converting numeric columns...\n",
      "Converting FIPS codes to string...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104346/3760516150.py:138: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_noaa[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NOAA DataFrame Info after initial cleaning ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 691434 entries, 0 to 691433\n",
      "Data columns (total 57 columns):\n",
      " #   Column              Non-Null Count   Dtype              \n",
      "---  ------              --------------   -----              \n",
      " 0   BEGIN_YEARMONTH     691434 non-null  int64              \n",
      " 1   BEGIN_DAY           691434 non-null  int64              \n",
      " 2   BEGIN_TIME          691434 non-null  int64              \n",
      " 3   END_YEARMONTH       691434 non-null  int64              \n",
      " 4   END_DAY             691434 non-null  int64              \n",
      " 5   END_TIME            691434 non-null  int64              \n",
      " 6   EPISODE_ID          691434 non-null  int64              \n",
      " 7   EVENT_ID            691434 non-null  int64              \n",
      " 8   STATE               691434 non-null  object             \n",
      " 9   STATE_FIPS          691434 non-null  object             \n",
      " 10  YEAR                691434 non-null  int64              \n",
      " 11  MONTH_NAME          691434 non-null  object             \n",
      " 12  EVENT_TYPE          691434 non-null  object             \n",
      " 13  CZ_TYPE             691434 non-null  object             \n",
      " 14  CZ_FIPS             691434 non-null  object             \n",
      " 15  CZ_NAME             691434 non-null  object             \n",
      " 16  WFO                 691434 non-null  object             \n",
      " 17  BEGIN_DATE_TIME     691434 non-null  object             \n",
      " 18  CZ_TIMEZONE         691434 non-null  object             \n",
      " 19  END_DATE_TIME       691434 non-null  object             \n",
      " 20  INJURIES_DIRECT     691434 non-null  int64              \n",
      " 21  INJURIES_INDIRECT   691434 non-null  int64              \n",
      " 22  DEATHS_DIRECT       691434 non-null  int64              \n",
      " 23  DEATHS_INDIRECT     691434 non-null  int64              \n",
      " 24  DAMAGE_PROPERTY     550448 non-null  object             \n",
      " 25  DAMAGE_CROPS        552275 non-null  object             \n",
      " 26  SOURCE              691434 non-null  object             \n",
      " 27  MAGNITUDE           360316 non-null  float64            \n",
      " 28  MAGNITUDE_TYPE      262379 non-null  object             \n",
      " 29  FLOOD_CAUSE         75222 non-null   object             \n",
      " 30  CATEGORY            369 non-null     float64            \n",
      " 31  TOR_F_SCALE         15641 non-null   object             \n",
      " 32  TOR_LENGTH          15641 non-null   float64            \n",
      " 33  TOR_WIDTH           15641 non-null   float64            \n",
      " 34  TOR_OTHER_WFO       2043 non-null    object             \n",
      " 35  TOR_OTHER_CZ_STATE  2043 non-null    object             \n",
      " 36  TOR_OTHER_CZ_FIPS   2043 non-null    float64            \n",
      " 37  TOR_OTHER_CZ_NAME   2043 non-null    object             \n",
      " 38  BEGIN_RANGE         425309 non-null  float64            \n",
      " 39  BEGIN_AZIMUTH       425309 non-null  object             \n",
      " 40  BEGIN_LOCATION      425309 non-null  object             \n",
      " 41  END_RANGE           425309 non-null  float64            \n",
      " 42  END_AZIMUTH         425309 non-null  object             \n",
      " 43  END_LOCATION        425309 non-null  object             \n",
      " 44  BEGIN_LAT           425309 non-null  float64            \n",
      " 45  BEGIN_LON           425309 non-null  float64            \n",
      " 46  END_LAT             425309 non-null  float64            \n",
      " 47  END_LON             425309 non-null  float64            \n",
      " 48  EPISODE_NARRATIVE   691434 non-null  object             \n",
      " 49  EVENT_NARRATIVE     547265 non-null  object             \n",
      " 50  DATA_SOURCE         691434 non-null  object             \n",
      " 51  BEGIN_DT            691434 non-null  datetime64[ns]     \n",
      " 52  END_DT              691434 non-null  datetime64[ns]     \n",
      " 53  BEGIN_DT_LOC        691397 non-null  object             \n",
      " 54  END_DT_LOC          691398 non-null  object             \n",
      " 55  BEGIN_DT_UTC        691397 non-null  datetime64[ns, UTC]\n",
      " 56  END_DT_UTC          691398 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](2), datetime64[ns](2), float64(11), int64(13), object(29)\n",
      "memory usage: 300.7+ MB\n",
      "\n",
      "--- NOAA DataFrame Head (focus on datetimes) ---\n",
      "   EVENT_ID CZ_TIMEZONE            BEGIN_DT               BEGIN_DT_LOC  \\\n",
      "0    503953       EST-5 2014-02-18 10:00:00  2014-02-18 10:00:00-05:00   \n",
      "1    507163       EST-5 2014-03-30 08:31:00  2014-03-30 08:31:00-04:00   \n",
      "2    506236       CST-6 2014-04-27 23:06:00  2014-04-27 23:06:00-05:00   \n",
      "3    506237       CST-6 2014-04-27 23:03:00  2014-04-27 23:03:00-05:00   \n",
      "4    501499       PST-8 2014-02-15 13:00:00  2014-02-15 13:00:00-08:00   \n",
      "\n",
      "               BEGIN_DT_UTC                END_DT_UTC  \n",
      "0 2014-02-18 15:00:00+00:00 2014-02-19 01:00:00+00:00  \n",
      "1 2014-03-30 12:31:00+00:00 2014-03-30 13:31:00+00:00  \n",
      "2 2014-04-28 04:06:00+00:00 2014-04-28 04:06:00+00:00  \n",
      "3 2014-04-28 04:03:00+00:00 2014-04-28 04:03:00+00:00  \n",
      "4 2014-02-15 21:00:00+00:00 2014-02-16 05:00:00+00:00  \n",
      "\n",
      "--- Data Types of Final Datetime Columns ---\n",
      "BEGIN_DT_UTC    datetime64[ns, UTC]\n",
      "END_DT_UTC      datetime64[ns, UTC]\n",
      "dtype: object\n",
      "\n",
      "Step 1 (NOAA Load/Clean) Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Define the path to the NOAA data file\n",
    "noaa_file_path = 'data/NOAA_StormEvents/StormEvents_2014_2024.csv'\n",
    "\n",
    "# Define columns that are likely numeric but might have issues during load\n",
    "numeric_cols_to_check = [\n",
    "    'BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH', 'END_DAY', 'END_TIME',\n",
    "    'EPISODE_ID', 'EVENT_ID', 'STATE_FIPS', 'CZ_FIPS',\n",
    "    'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT',\n",
    "    'MAGNITUDE', 'TOR_LENGTH', 'TOR_WIDTH',\n",
    "    'BEGIN_RANGE', 'END_RANGE', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON'\n",
    "]\n",
    "\n",
    "# Define the expected datetime format\n",
    "# Using errors='coerce' will turn unparseable dates into NaT (Not a Time)\n",
    "noaa_datetime_format = '%d-%b-%y %H:%M:%S'\n",
    "\n",
    "print(f\"Loading NOAA data from: {noaa_file_path}\")\n",
    "\n",
    "# --- Load the data ---\n",
    "try:\n",
    "    df_noaa = pd.read_csv(noaa_file_path, low_memory=False)\n",
    "    print(\"NOAA data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {noaa_file_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NOAA data: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Performing initial cleaning and type conversions...\")\n",
    "\n",
    "# --- Datetime Conversion (Initial Parsing) ---\n",
    "print(\"Parsing original datetime strings...\")\n",
    "df_noaa['BEGIN_DT'] = pd.to_datetime(df_noaa['BEGIN_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "df_noaa['END_DT'] = pd.to_datetime(df_noaa['END_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "\n",
    "# Check for initial parsing errors (NaT values)\n",
    "begin_nat_count = df_noaa['BEGIN_DT'].isna().sum()\n",
    "end_nat_count = df_noaa['END_DT'].isna().sum()\n",
    "if begin_nat_count > 0 or end_nat_count > 0:\n",
    "    print(f\"Warning: Found {begin_nat_count} NaT values in BEGIN_DT after initial parsing.\")\n",
    "    print(f\"Warning: Found {end_nat_count} NaT values in END_DT after initial parsing.\")\n",
    "\n",
    "# --- Timezone Handling ---\n",
    "print(\"Mapping timezones...\")\n",
    "# Using the tz_map confirmed from your data\n",
    "tz_map = {\n",
    "    'EST-5': 'America/New_York',\n",
    "    'EDT-4': 'America/New_York',\n",
    "    'CST-6': 'America/Chicago',\n",
    "    'CDT-5': 'America/Chicago',\n",
    "    'MST-7': 'America/Denver',\n",
    "    'MDT-6': 'America/Denver', # Keep just in case\n",
    "    'PST-8': 'America/Los_Angeles',\n",
    "    'PDT-7': 'America/Los_Angeles',\n",
    "    'AKST-9': 'America/Anchorage',\n",
    "    'AKDT-8': 'America/Anchorage', # Keep just in case\n",
    "    'HST-10': 'Pacific/Honolulu',\n",
    "    'AST-4': 'America/Puerto_Rico',\n",
    "    'GST10': 'Pacific/Guam',\n",
    "    'SST-11': 'Pacific/Pago_Pago',\n",
    "    '': None, # Map empty string explicitly if needed\n",
    "}\n",
    "\n",
    "# Function to apply timezone localization\n",
    "def localize_datetime(row):\n",
    "    tz_str = row['CZ_TIMEZONE']\n",
    "    dt = row['datetime_col']\n",
    "    if pd.isna(dt) or pd.isna(tz_str):\n",
    "        return pd.NaT\n",
    "\n",
    "    # Ensure tz_str is string and strip whitespace for lookup\n",
    "    tz_str = str(tz_str).strip()\n",
    "    tz_name = tz_map.get(tz_str)\n",
    "\n",
    "    if tz_name:\n",
    "        try:\n",
    "            return dt.tz_localize(tz_name, ambiguous='NaT', nonexistent='NaT')\n",
    "        except Exception as e:\n",
    "            # warnings.warn(f\"Could not localize timezone '{tz_str}' for datetime {dt}: {e}\")\n",
    "            return pd.NaT\n",
    "    else:\n",
    "        if tz_str: # Avoid warning for known blanks mapped to None\n",
    "             warnings.warn(f\"Timezone '{tz_str}' not found in tz_map.\", UserWarning)\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply localization using the helper column\n",
    "print(\"Applying timezone localization (this may take time)...\")\n",
    "df_noaa['datetime_col'] = df_noaa['BEGIN_DT']\n",
    "df_noaa['BEGIN_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "df_noaa['datetime_col'] = df_noaa['END_DT']\n",
    "df_noaa['END_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "df_noaa.drop(columns=['datetime_col'], inplace=True)\n",
    "print(\"Timezone localization applied.\")\n",
    "\n",
    "# --- Convert Mixed Timezone 'Object' Columns Directly to UTC ---\n",
    "# This step addresses the 'dtype: object' issue after localization\n",
    "print(\"Attempting to convert localized columns directly to UTC...\")\n",
    "try:\n",
    "    # Record NaNs before conversion\n",
    "    original_loc_nan_begin = df_noaa['BEGIN_DT_LOC'].isna().sum()\n",
    "    original_loc_nan_end = df_noaa['END_DT_LOC'].isna().sum()\n",
    "\n",
    "    # Use pd.to_datetime with utc=True to handle the object column containing tz-aware objects\n",
    "    df_noaa['BEGIN_DT_UTC'] = pd.to_datetime(df_noaa['BEGIN_DT_LOC'], errors='coerce', utc=True)\n",
    "    df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_LOC'], errors='coerce', utc=True)\n",
    "\n",
    "    print(\"Direct UTC conversion attempted.\")\n",
    "\n",
    "    # Check for new NaNs potentially introduced\n",
    "    begin_utc_nat_count = df_noaa['BEGIN_DT_UTC'].isna().sum()\n",
    "    end_utc_nat_count = df_noaa['END_DT_UTC'].isna().sum()\n",
    "    if begin_utc_nat_count > original_loc_nan_begin or end_utc_nat_count > original_loc_nan_end:\n",
    "         print(f\"Warning: Additional NaTs potentially introduced during UTC conversion.\")\n",
    "         print(f\"         ({original_loc_nan_begin} -> {begin_utc_nat_count} NaTs in BEGIN_DT_UTC)\")\n",
    "         print(f\"         ({original_loc_nan_end} -> {end_utc_nat_count} NaTs in END_DT_UTC)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to convert localized object columns to UTC. Error: {e}\")\n",
    "    # Assign NaT if conversion fails catastrophically\n",
    "    df_noaa['BEGIN_DT_UTC'] = pd.NaT\n",
    "    df_noaa['END_DT_UTC'] = pd.NaT\n",
    "\n",
    "\n",
    "# --- Numeric Conversions ---\n",
    "print(\"Converting numeric columns...\")\n",
    "for col in numeric_cols_to_check:\n",
    "    if col in df_noaa.columns:\n",
    "        df_noaa[col] = pd.to_numeric(df_noaa[col], errors='coerce')\n",
    "        if 'INJURIES' in col or 'DEATHS' in col:\n",
    "            # Fill NaNs only for specific columns where 0 makes sense\n",
    "            df_noaa[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# --- FIPS Code to String ---\n",
    "print(\"Converting FIPS codes to string...\")\n",
    "if 'STATE_FIPS' in df_noaa.columns:\n",
    "    # Use .astype(str).str.split('.').str[0] to handle potential floats before converting to string\n",
    "    df_noaa['STATE_FIPS'] = df_noaa['STATE_FIPS'].astype(str).str.split('.').str[0]\n",
    "if 'CZ_FIPS' in df_noaa.columns:\n",
    "    df_noaa['CZ_FIPS'] = df_noaa['CZ_FIPS'].astype(str).str.split('.').str[0]\n",
    "\n",
    "\n",
    "# --- Display Info and Head ---\n",
    "print(\"\\n--- NOAA DataFrame Info after initial cleaning ---\")\n",
    "df_noaa.info(verbose=True, show_counts=True)\n",
    "\n",
    "print(\"\\n--- NOAA DataFrame Head (focus on datetimes) ---\")\n",
    "print(df_noaa[['EVENT_ID', 'CZ_TIMEZONE', 'BEGIN_DT', 'BEGIN_DT_LOC', 'BEGIN_DT_UTC', 'END_DT_UTC']].head())\n",
    "\n",
    "print(\"\\n--- Data Types of Final Datetime Columns ---\")\n",
    "print(df_noaa[['BEGIN_DT_UTC', 'END_DT_UTC']].dtypes)\n",
    "\n",
    "print(\"\\nStep 1 (NOAA Load/Clean) Complete.\")\n",
    "# df_noaa now contains the loaded and initially cleaned NOAA data\n",
    "# Key final columns: BEGIN_DT_UTC, END_DT_UTC (should be datetime64[ns, UTC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2800bd23-e29a-480b-af34-e6d018dbadc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "      <th>BEGIN_DT</th>\n",
       "      <th>END_DT</th>\n",
       "      <th>BEGIN_DT_LOC</th>\n",
       "      <th>END_DT_LOC</th>\n",
       "      <th>BEGIN_DT_UTC</th>\n",
       "      <th>END_DT_UTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201402</td>\n",
       "      <td>18</td>\n",
       "      <td>1000</td>\n",
       "      <td>201402</td>\n",
       "      <td>18</td>\n",
       "      <td>2000</td>\n",
       "      <td>83473</td>\n",
       "      <td>503953</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure developing south of Long Island a...</td>\n",
       "      <td>Eight to twelve inches of snow fell across eas...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-02-18 10:00:00</td>\n",
       "      <td>2014-02-18 20:00:00</td>\n",
       "      <td>2014-02-18 10:00:00-05:00</td>\n",
       "      <td>2014-02-18 20:00:00-05:00</td>\n",
       "      <td>2014-02-18 15:00:00+00:00</td>\n",
       "      <td>2014-02-19 01:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201403</td>\n",
       "      <td>30</td>\n",
       "      <td>831</td>\n",
       "      <td>201403</td>\n",
       "      <td>30</td>\n",
       "      <td>931</td>\n",
       "      <td>83971</td>\n",
       "      <td>507163</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.3469</td>\n",
       "      <td>A stacked low pressure system passed south and...</td>\n",
       "      <td>Boston Road was closed near Brian Road due to ...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-03-30 08:31:00</td>\n",
       "      <td>2014-03-30 09:31:00</td>\n",
       "      <td>2014-03-30 08:31:00-04:00</td>\n",
       "      <td>2014-03-30 09:31:00-04:00</td>\n",
       "      <td>2014-03-30 12:31:00+00:00</td>\n",
       "      <td>2014-03-30 13:31:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2306</td>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2306</td>\n",
       "      <td>83517</td>\n",
       "      <td>506236</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.6600</td>\n",
       "      <td>A powerful storm system and a dry line produce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-04-27 23:06:00</td>\n",
       "      <td>2014-04-27 23:06:00</td>\n",
       "      <td>2014-04-27 23:06:00-05:00</td>\n",
       "      <td>2014-04-27 23:06:00-05:00</td>\n",
       "      <td>2014-04-28 04:06:00+00:00</td>\n",
       "      <td>2014-04-28 04:06:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2303</td>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2303</td>\n",
       "      <td>83517</td>\n",
       "      <td>506237</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.6600</td>\n",
       "      <td>A powerful storm system and a dry line produce...</td>\n",
       "      <td>Several power poles snapped and trees blown down.</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-04-27 23:03:00</td>\n",
       "      <td>2014-04-27 23:03:00</td>\n",
       "      <td>2014-04-27 23:03:00-05:00</td>\n",
       "      <td>2014-04-27 23:03:00-05:00</td>\n",
       "      <td>2014-04-28 04:03:00+00:00</td>\n",
       "      <td>2014-04-28 04:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201402</td>\n",
       "      <td>15</td>\n",
       "      <td>1300</td>\n",
       "      <td>201402</td>\n",
       "      <td>15</td>\n",
       "      <td>2100</td>\n",
       "      <td>83132</td>\n",
       "      <td>501499</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A strong cold front produced strong winds for ...</td>\n",
       "      <td>Two stations measured strong wind gusts in the...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-02-15 13:00:00</td>\n",
       "      <td>2014-02-15 21:00:00</td>\n",
       "      <td>2014-02-15 13:00:00-08:00</td>\n",
       "      <td>2014-02-15 21:00:00-08:00</td>\n",
       "      <td>2014-02-15 21:00:00+00:00</td>\n",
       "      <td>2014-02-16 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691429</th>\n",
       "      <td>202405</td>\n",
       "      <td>26</td>\n",
       "      <td>1148</td>\n",
       "      <td>202405</td>\n",
       "      <td>26</td>\n",
       "      <td>1148</td>\n",
       "      <td>192532</td>\n",
       "      <td>1188957</td>\n",
       "      <td>KENTUCKY</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>-84.7200</td>\n",
       "      <td>A strong storm system moved across the Ohio an...</td>\n",
       "      <td>A trained spotter estimated 60 mph wind gusts ...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-05-26 11:48:00</td>\n",
       "      <td>2024-05-26 11:48:00</td>\n",
       "      <td>2024-05-26 11:48:00-04:00</td>\n",
       "      <td>2024-05-26 11:48:00-04:00</td>\n",
       "      <td>2024-05-26 15:48:00+00:00</td>\n",
       "      <td>2024-05-26 15:48:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691430</th>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1809</td>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1809</td>\n",
       "      <td>192530</td>\n",
       "      <td>1188234</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>-85.7364</td>\n",
       "      <td>A cold front moved into the Ohio Valley during...</td>\n",
       "      <td>A tree was down at Lovers Lane and Prewitt Lane.</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-05-22 18:09:00</td>\n",
       "      <td>2024-05-22 18:09:00</td>\n",
       "      <td>2024-05-22 18:09:00-04:00</td>\n",
       "      <td>2024-05-22 18:09:00-04:00</td>\n",
       "      <td>2024-05-22 22:09:00+00:00</td>\n",
       "      <td>2024-05-22 22:09:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691431</th>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1757</td>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1757</td>\n",
       "      <td>192530</td>\n",
       "      <td>1188232</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>-86.7247</td>\n",
       "      <td>A cold front moved into the Ohio Valley during...</td>\n",
       "      <td>A tree was reported down over Chestnut Grove R...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-05-22 17:57:00</td>\n",
       "      <td>2024-05-22 17:57:00</td>\n",
       "      <td>2024-05-22 17:57:00-04:00</td>\n",
       "      <td>2024-05-22 17:57:00-04:00</td>\n",
       "      <td>2024-05-22 21:57:00+00:00</td>\n",
       "      <td>2024-05-22 21:57:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691432</th>\n",
       "      <td>202406</td>\n",
       "      <td>23</td>\n",
       "      <td>1745</td>\n",
       "      <td>202406</td>\n",
       "      <td>23</td>\n",
       "      <td>1750</td>\n",
       "      <td>191388</td>\n",
       "      <td>1192879</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>-70.8400</td>\n",
       "      <td>A supercell thunderstorm developed across sout...</td>\n",
       "      <td>A supercell thunderstorm dropped hail the size...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-06-23 17:45:00</td>\n",
       "      <td>2024-06-23 17:50:00</td>\n",
       "      <td>2024-06-23 17:45:00-04:00</td>\n",
       "      <td>2024-06-23 17:50:00-04:00</td>\n",
       "      <td>2024-06-23 21:45:00+00:00</td>\n",
       "      <td>2024-06-23 21:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691433</th>\n",
       "      <td>202408</td>\n",
       "      <td>6</td>\n",
       "      <td>752</td>\n",
       "      <td>202408</td>\n",
       "      <td>6</td>\n",
       "      <td>852</td>\n",
       "      <td>195694</td>\n",
       "      <td>1214246</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.1375</td>\n",
       "      <td>Debby first developed into a tropical storm ab...</td>\n",
       "      <td>A Chatham County emergency manager reported a ...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-08-06 07:52:00</td>\n",
       "      <td>2024-08-06 08:52:00</td>\n",
       "      <td>2024-08-06 07:52:00-04:00</td>\n",
       "      <td>2024-08-06 08:52:00-04:00</td>\n",
       "      <td>2024-08-06 11:52:00+00:00</td>\n",
       "      <td>2024-08-06 12:52:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691434 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  \\\n",
       "0                201402         18        1000         201402       18   \n",
       "1                201403         30         831         201403       30   \n",
       "2                201404         27        2306         201404       27   \n",
       "3                201404         27        2303         201404       27   \n",
       "4                201402         15        1300         201402       15   \n",
       "...                 ...        ...         ...            ...      ...   \n",
       "691429           202405         26        1148         202405       26   \n",
       "691430           202405         22        1809         202405       22   \n",
       "691431           202405         22        1757         202405       22   \n",
       "691432           202406         23        1745         202406       23   \n",
       "691433           202408          6         752         202408        6   \n",
       "\n",
       "        END_TIME  EPISODE_ID  EVENT_ID          STATE STATE_FIPS  ...  \\\n",
       "0           2000       83473    503953  NEW HAMPSHIRE         33  ...   \n",
       "1            931       83971    507163  MASSACHUSETTS         25  ...   \n",
       "2           2306       83517    506236       MISSOURI         29  ...   \n",
       "3           2303       83517    506237       MISSOURI         29  ...   \n",
       "4           2100       83132    501499     WASHINGTON         53  ...   \n",
       "...          ...         ...       ...            ...        ...  ...   \n",
       "691429      1148      192532   1188957       KENTUCKY         21  ...   \n",
       "691430      1809      192530   1188234        INDIANA         18  ...   \n",
       "691431      1757      192530   1188232        INDIANA         18  ...   \n",
       "691432      1750      191388   1192879  NEW HAMPSHIRE         33  ...   \n",
       "691433       852      195694   1214246        GEORGIA         13  ...   \n",
       "\n",
       "        END_LON                                  EPISODE_NARRATIVE  \\\n",
       "0           NaN  Low pressure developing south of Long Island a...   \n",
       "1      -71.3469  A stacked low pressure system passed south and...   \n",
       "2      -92.6600  A powerful storm system and a dry line produce...   \n",
       "3      -92.6600  A powerful storm system and a dry line produce...   \n",
       "4           NaN  A strong cold front produced strong winds for ...   \n",
       "...         ...                                                ...   \n",
       "691429 -84.7200  A strong storm system moved across the Ohio an...   \n",
       "691430 -85.7364  A cold front moved into the Ohio Valley during...   \n",
       "691431 -86.7247  A cold front moved into the Ohio Valley during...   \n",
       "691432 -70.8400  A supercell thunderstorm developed across sout...   \n",
       "691433 -81.1375  Debby first developed into a tropical storm ab...   \n",
       "\n",
       "                                          EVENT_NARRATIVE DATA_SOURCE  \\\n",
       "0       Eight to twelve inches of snow fell across eas...         CSV   \n",
       "1       Boston Road was closed near Brian Road due to ...         CSV   \n",
       "2                                                     NaN         CSV   \n",
       "3       Several power poles snapped and trees blown down.         CSV   \n",
       "4       Two stations measured strong wind gusts in the...         CSV   \n",
       "...                                                   ...         ...   \n",
       "691429  A trained spotter estimated 60 mph wind gusts ...         CSV   \n",
       "691430   A tree was down at Lovers Lane and Prewitt Lane.         CSV   \n",
       "691431  A tree was reported down over Chestnut Grove R...         CSV   \n",
       "691432  A supercell thunderstorm dropped hail the size...         CSV   \n",
       "691433  A Chatham County emergency manager reported a ...         CSV   \n",
       "\n",
       "                  BEGIN_DT              END_DT               BEGIN_DT_LOC  \\\n",
       "0      2014-02-18 10:00:00 2014-02-18 20:00:00  2014-02-18 10:00:00-05:00   \n",
       "1      2014-03-30 08:31:00 2014-03-30 09:31:00  2014-03-30 08:31:00-04:00   \n",
       "2      2014-04-27 23:06:00 2014-04-27 23:06:00  2014-04-27 23:06:00-05:00   \n",
       "3      2014-04-27 23:03:00 2014-04-27 23:03:00  2014-04-27 23:03:00-05:00   \n",
       "4      2014-02-15 13:00:00 2014-02-15 21:00:00  2014-02-15 13:00:00-08:00   \n",
       "...                    ...                 ...                        ...   \n",
       "691429 2024-05-26 11:48:00 2024-05-26 11:48:00  2024-05-26 11:48:00-04:00   \n",
       "691430 2024-05-22 18:09:00 2024-05-22 18:09:00  2024-05-22 18:09:00-04:00   \n",
       "691431 2024-05-22 17:57:00 2024-05-22 17:57:00  2024-05-22 17:57:00-04:00   \n",
       "691432 2024-06-23 17:45:00 2024-06-23 17:50:00  2024-06-23 17:45:00-04:00   \n",
       "691433 2024-08-06 07:52:00 2024-08-06 08:52:00  2024-08-06 07:52:00-04:00   \n",
       "\n",
       "                       END_DT_LOC              BEGIN_DT_UTC  \\\n",
       "0       2014-02-18 20:00:00-05:00 2014-02-18 15:00:00+00:00   \n",
       "1       2014-03-30 09:31:00-04:00 2014-03-30 12:31:00+00:00   \n",
       "2       2014-04-27 23:06:00-05:00 2014-04-28 04:06:00+00:00   \n",
       "3       2014-04-27 23:03:00-05:00 2014-04-28 04:03:00+00:00   \n",
       "4       2014-02-15 21:00:00-08:00 2014-02-15 21:00:00+00:00   \n",
       "...                           ...                       ...   \n",
       "691429  2024-05-26 11:48:00-04:00 2024-05-26 15:48:00+00:00   \n",
       "691430  2024-05-22 18:09:00-04:00 2024-05-22 22:09:00+00:00   \n",
       "691431  2024-05-22 17:57:00-04:00 2024-05-22 21:57:00+00:00   \n",
       "691432  2024-06-23 17:50:00-04:00 2024-06-23 21:45:00+00:00   \n",
       "691433  2024-08-06 08:52:00-04:00 2024-08-06 11:52:00+00:00   \n",
       "\n",
       "                      END_DT_UTC  \n",
       "0      2014-02-19 01:00:00+00:00  \n",
       "1      2014-03-30 13:31:00+00:00  \n",
       "2      2014-04-28 04:06:00+00:00  \n",
       "3      2014-04-28 04:03:00+00:00  \n",
       "4      2014-02-16 05:00:00+00:00  \n",
       "...                          ...  \n",
       "691429 2024-05-26 15:48:00+00:00  \n",
       "691430 2024-05-22 22:09:00+00:00  \n",
       "691431 2024-05-22 21:57:00+00:00  \n",
       "691432 2024-06-23 21:50:00+00:00  \n",
       "691433 2024-08-06 12:52:00+00:00  \n",
       "\n",
       "[691434 rows x 57 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58423f87-7c00-4d4b-8c21-1f34b3926482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noaa.to_csv(\"NOAA_timezone_cleaned.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4e7c44-fee3-4a9b-b2c3-45befebe0cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Step 3: Feature Engineering on NOAA Data...\n",
      "Parsing DAMAGE_PROPERTY and DAMAGE_CROPS...\n",
      "Damage columns parsed.\n",
      "Calculating event duration...\n",
      "  Ensuring UTC columns are datetime type...\n",
      "Event duration calculated.\n",
      "Handling MAGNITUDE NaNs...\n",
      "  Filled 331118 NaNs in MAGNITUDE with 0.\n",
      "MAGNITUDE NaNs handled.\n",
      "Filtering NOAA data for County-level events (CZ_TYPE == 'C')...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104346/1085264281.py:60: SettingWithCopyWarning: modifications to a method of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  duration_seconds[duration_seconds < 0] = 0\n",
      "/tmp/ipykernel_104346/1085264281.py:60: SettingWithCopyWarning: modifications to a method of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  duration_seconds[duration_seconds < 0] = 0\n",
      "/tmp/ipykernel_104346/1085264281.py:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_noaa['EVENT_DURATION_HOURS'].fillna(0, inplace=True) # Fill NaN durations with 0\n",
      "/tmp/ipykernel_104346/1085264281.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_noaa['MAGNITUDE'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retained 397066 rows out of 691434 (County-level events).\n",
      "\n",
      "--- NOAA DataFrame Info after Step 3 Feature Engineering ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 397066 entries, 1 to 691433\n",
      "Data columns (total 60 columns):\n",
      " #   Column                   Non-Null Count   Dtype              \n",
      "---  ------                   --------------   -----              \n",
      " 0   BEGIN_YEARMONTH          397066 non-null  int64              \n",
      " 1   BEGIN_DAY                397066 non-null  int64              \n",
      " 2   BEGIN_TIME               397066 non-null  int64              \n",
      " 3   END_YEARMONTH            397066 non-null  int64              \n",
      " 4   END_DAY                  397066 non-null  int64              \n",
      " 5   END_TIME                 397066 non-null  int64              \n",
      " 6   EPISODE_ID               397066 non-null  int64              \n",
      " 7   EVENT_ID                 397066 non-null  int64              \n",
      " 8   STATE                    397066 non-null  object             \n",
      " 9   STATE_FIPS               397066 non-null  object             \n",
      " 10  YEAR                     397066 non-null  int64              \n",
      " 11  MONTH_NAME               397066 non-null  object             \n",
      " 12  EVENT_TYPE               397066 non-null  object             \n",
      " 13  CZ_TYPE                  397066 non-null  object             \n",
      " 14  CZ_FIPS                  397066 non-null  object             \n",
      " 15  CZ_NAME                  397066 non-null  object             \n",
      " 16  WFO                      397066 non-null  object             \n",
      " 17  BEGIN_DATE_TIME          397066 non-null  object             \n",
      " 18  CZ_TIMEZONE              397066 non-null  object             \n",
      " 19  END_DATE_TIME            397066 non-null  object             \n",
      " 20  INJURIES_DIRECT          397066 non-null  int64              \n",
      " 21  INJURIES_INDIRECT        397066 non-null  int64              \n",
      " 22  DEATHS_DIRECT            397066 non-null  int64              \n",
      " 23  DEATHS_INDIRECT          397066 non-null  int64              \n",
      " 24  DAMAGE_PROPERTY          324398 non-null  object             \n",
      " 25  DAMAGE_CROPS             323921 non-null  object             \n",
      " 26  SOURCE                   397066 non-null  object             \n",
      " 27  MAGNITUDE                397066 non-null  float64            \n",
      " 28  MAGNITUDE_TYPE           186460 non-null  object             \n",
      " 29  FLOOD_CAUSE              75222 non-null   object             \n",
      " 30  CATEGORY                 0 non-null       float64            \n",
      " 31  TOR_F_SCALE              15641 non-null   object             \n",
      " 32  TOR_LENGTH               15641 non-null   float64            \n",
      " 33  TOR_WIDTH                15641 non-null   float64            \n",
      " 34  TOR_OTHER_WFO            2043 non-null    object             \n",
      " 35  TOR_OTHER_CZ_STATE       2043 non-null    object             \n",
      " 36  TOR_OTHER_CZ_FIPS        2043 non-null    float64            \n",
      " 37  TOR_OTHER_CZ_NAME        2043 non-null    object             \n",
      " 38  BEGIN_RANGE              397015 non-null  float64            \n",
      " 39  BEGIN_AZIMUTH            397015 non-null  object             \n",
      " 40  BEGIN_LOCATION           397015 non-null  object             \n",
      " 41  END_RANGE                397015 non-null  float64            \n",
      " 42  END_AZIMUTH              397015 non-null  object             \n",
      " 43  END_LOCATION             397015 non-null  object             \n",
      " 44  BEGIN_LAT                397015 non-null  float64            \n",
      " 45  BEGIN_LON                397015 non-null  float64            \n",
      " 46  END_LAT                  397015 non-null  float64            \n",
      " 47  END_LON                  397015 non-null  float64            \n",
      " 48  EPISODE_NARRATIVE        397066 non-null  object             \n",
      " 49  EVENT_NARRATIVE          333768 non-null  object             \n",
      " 50  DATA_SOURCE              397066 non-null  object             \n",
      " 51  BEGIN_DT                 397066 non-null  datetime64[ns]     \n",
      " 52  END_DT                   397066 non-null  datetime64[ns]     \n",
      " 53  BEGIN_DT_LOC             397040 non-null  object             \n",
      " 54  END_DT_LOC               397039 non-null  object             \n",
      " 55  BEGIN_DT_UTC             397040 non-null  datetime64[ns, UTC]\n",
      " 56  END_DT_UTC               397039 non-null  datetime64[ns, UTC]\n",
      " 57  DAMAGE_PROPERTY_NUMERIC  397066 non-null  float64            \n",
      " 58  DAMAGE_CROPS_NUMERIC     397066 non-null  float64            \n",
      " 59  EVENT_DURATION_HOURS     397066 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](2), datetime64[ns](2), float64(14), int64(13), object(29)\n",
      "memory usage: 184.8+ MB\n",
      "\n",
      "--- NOAA DataFrame Head (New Features) ---\n",
      "    EVENT_ID CZ_FIPS              BEGIN_DT_UTC                END_DT_UTC  \\\n",
      "1     507163      17 2014-03-30 12:31:00+00:00 2014-03-30 13:31:00+00:00   \n",
      "2     506236      67 2014-04-28 04:06:00+00:00 2014-04-28 04:06:00+00:00   \n",
      "3     506237      67 2014-04-28 04:03:00+00:00 2014-04-28 04:03:00+00:00   \n",
      "10    506362     191 2014-05-12 03:30:00+00:00 2014-05-12 03:30:00+00:00   \n",
      "11    506238      67 2014-04-28 03:50:00+00:00 2014-04-28 03:50:00+00:00   \n",
      "\n",
      "   DAMAGE_PROPERTY  DAMAGE_PROPERTY_NUMERIC DAMAGE_CROPS  \\\n",
      "1           35.00K                  35000.0        0.00K   \n",
      "2            0.00K                      0.0        0.00K   \n",
      "3           10.00K                  10000.0        0.00K   \n",
      "10           1.00K                   1000.0        0.00K   \n",
      "11          10.00K                  10000.0        0.00K   \n",
      "\n",
      "    DAMAGE_CROPS_NUMERIC  EVENT_DURATION_HOURS  MAGNITUDE  \n",
      "1                    0.0                   1.0       0.00  \n",
      "2                    0.0                   0.0       0.88  \n",
      "3                    0.0                   0.0      61.00  \n",
      "10                   0.0                   0.0      50.00  \n",
      "11                   0.0                   0.0      65.00  \n",
      "\n",
      "Step 3 (NOAA Feature Engineering) Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re # Import regex module for damage parsing\n",
    "\n",
    "# Assume df_noaa exists from Step 1\n",
    "\n",
    "print(\"\\nStarting Step 3: Feature Engineering on NOAA Data...\")\n",
    "\n",
    "# --- 1. Parse Damage Columns ---\n",
    "print(\"Parsing DAMAGE_PROPERTY and DAMAGE_CROPS...\")\n",
    "\n",
    "def parse_damage_value(value):\n",
    "    \"\"\"Converts damage strings (e.g., '10.00K', '1.5M', '50B') to numeric.\"\"\"\n",
    "    if pd.isna(value) or value == '0.00K' or value == 0: # Handle NaNs and explicit zero\n",
    "        return 0.0\n",
    "    value_str = str(value).strip().upper()\n",
    "    # Use regex to find number and optional multiplier (K, M, B)\n",
    "    match = re.match(r'([\\d\\.]+)([KMB]?)', value_str)\n",
    "    if match:\n",
    "        number, multiplier = match.groups()\n",
    "        number = float(number)\n",
    "        if multiplier == 'K':\n",
    "            return number * 1_000\n",
    "        elif multiplier == 'M':\n",
    "            return number * 1_000_000\n",
    "        elif multiplier == 'B':\n",
    "            return number * 1_000_000_000\n",
    "        else: # No multiplier, assume direct value\n",
    "            return number\n",
    "    else:\n",
    "        # warnings.warn(f\"Could not parse damage value: {value}\", UserWarning)\n",
    "        return 0.0 # Return 0 if pattern doesn't match\n",
    "\n",
    "# Apply the function\n",
    "if 'DAMAGE_PROPERTY' in df_noaa.columns:\n",
    "    df_noaa['DAMAGE_PROPERTY_NUMERIC'] = df_noaa['DAMAGE_PROPERTY'].apply(parse_damage_value)\n",
    "else:\n",
    "    print(\"Warning: DAMAGE_PROPERTY column not found.\")\n",
    "    df_noaa['DAMAGE_PROPERTY_NUMERIC'] = 0.0\n",
    "\n",
    "if 'DAMAGE_CROPS' in df_noaa.columns:\n",
    "    df_noaa['DAMAGE_CROPS_NUMERIC'] = df_noaa['DAMAGE_CROPS'].apply(parse_damage_value)\n",
    "else:\n",
    "    print(\"Warning: DAMAGE_CROPS column not found.\")\n",
    "    df_noaa['DAMAGE_CROPS_NUMERIC'] = 0.0\n",
    "\n",
    "print(\"Damage columns parsed.\")\n",
    "\n",
    "\n",
    "# --- 2. Calculate Event Duration ---\n",
    "print(\"Calculating event duration...\")\n",
    "if 'BEGIN_DT_UTC' in df_noaa.columns and 'END_DT_UTC' in df_noaa.columns:\n",
    "    # Calculate duration in seconds, then convert to hours\n",
    "    print(\"  Ensuring UTC columns are datetime type...\")\n",
    "    df_noaa['BEGIN_DT_UTC'] = pd.to_datetime(df_noaa['BEGIN_DT_UTC'])\n",
    "    df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC'])\n",
    "    duration_seconds = (df_noaa['END_DT_UTC'] - df_noaa['BEGIN_DT_UTC']).dt.total_seconds()\n",
    "    # Handle potential negative durations (if end < begin) by setting them to 0 or NaN\n",
    "    duration_seconds[duration_seconds < 0] = 0\n",
    "    df_noaa['EVENT_DURATION_HOURS'] = duration_seconds / 3600.0\n",
    "    df_noaa['EVENT_DURATION_HOURS'].fillna(0, inplace=True) # Fill NaN durations with 0\n",
    "else:\n",
    "    print(\"Warning: BEGIN_DT_UTC or END_DT_UTC columns not found. Cannot calculate duration.\")\n",
    "    df_noaa['EVENT_DURATION_HOURS'] = 0.0\n",
    "print(\"Event duration calculated.\")\n",
    "\n",
    "\n",
    "# --- 3. Handle Magnitude NaN ---\n",
    "print(\"Handling MAGNITUDE NaNs...\")\n",
    "if 'MAGNITUDE' in df_noaa.columns:\n",
    "    # Impute with 0 based on previous discussion (NaN often means not applicable)\n",
    "    nan_before = df_noaa['MAGNITUDE'].isna().sum()\n",
    "    df_noaa['MAGNITUDE'].fillna(0, inplace=True)\n",
    "    print(f\"  Filled {nan_before} NaNs in MAGNITUDE with 0.\")\n",
    "else:\n",
    "    print(\"Warning: MAGNITUDE column not found.\")\n",
    "print(\"MAGNITUDE NaNs handled.\")\n",
    "\n",
    "\n",
    "# --- 4. Filter by CZ_TYPE (Recommended) ---\n",
    "print(\"Filtering NOAA data for County-level events (CZ_TYPE == 'C')...\")\n",
    "if 'CZ_TYPE' in df_noaa.columns:\n",
    "    rows_before = len(df_noaa)\n",
    "    df_noaa = df_noaa[df_noaa['CZ_TYPE'] == 'C'].copy() # Filter and create a copy\n",
    "    rows_after = len(df_noaa)\n",
    "    print(f\"  Retained {rows_after} rows out of {rows_before} (County-level events).\")\n",
    "else:\n",
    "    print(\"Warning: CZ_TYPE column not found. Cannot filter by county-level events.\")\n",
    "\n",
    "\n",
    "# --- Display Info and Head ---\n",
    "print(\"\\n--- NOAA DataFrame Info after Step 3 Feature Engineering ---\")\n",
    "df_noaa.info(verbose=True, show_counts=True)\n",
    "\n",
    "print(\"\\n--- NOAA DataFrame Head (New Features) ---\")\n",
    "print(df_noaa[[\n",
    "    'EVENT_ID', 'CZ_FIPS', 'BEGIN_DT_UTC', 'END_DT_UTC',\n",
    "    'DAMAGE_PROPERTY', 'DAMAGE_PROPERTY_NUMERIC',\n",
    "    'DAMAGE_CROPS', 'DAMAGE_CROPS_NUMERIC',\n",
    "    'EVENT_DURATION_HOURS', 'MAGNITUDE'\n",
    "]].head())\n",
    "\n",
    "print(\"\\nStep 3 (NOAA Feature Engineering) Complete.\")\n",
    "# df_noaa now has numeric damage columns, event duration, imputed magnitude,\n",
    "# and is potentially filtered to only county-level events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320b07b-0d2e-4365-9a90-181c51842a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob # To find the files\n",
    "import os\n",
    "\n",
    "# Define the directory containing the Eaglei data\n",
    "eaglei_dir = 'data/eaglei_data/'\n",
    "\n",
    "# Find all eaglei outage CSV files\n",
    "eaglei_files = glob.glob(os.path.join(eaglei_dir, 'eaglei_outages_*.csv'))\n",
    "\n",
    "if not eaglei_files:\n",
    "    print(f\"Error: No 'eaglei_outages_*.csv' files found in {eaglei_dir}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"Found {len(eaglei_files)} Eaglei outage files:\")\n",
    "    eaglei_files.sort()\n",
    "    for f in eaglei_files[:3]: print(f\"  - {os.path.basename(f)}\")\n",
    "    if len(eaglei_files) > 3: print(\"  - ... and others\")\n",
    "\n",
    "\n",
    "# --- Load and Concatenate Data ---\n",
    "print(\"\\nLoading and concatenating Eaglei data...\")\n",
    "list_of_dfs = []\n",
    "for f in eaglei_files:\n",
    "    try:\n",
    "        df_temp = pd.read_csv(f)\n",
    "        list_of_dfs.append(df_temp)\n",
    "        print(f\"  Loaded {os.path.basename(f)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Warning: File {f} not found during loading loop.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {os.path.basename(f)}: {e}\")\n",
    "\n",
    "if not list_of_dfs:\n",
    "    print(\"Error: Failed to load any Eaglei data.\")\n",
    "    exit()\n",
    "\n",
    "df_eaglei = pd.concat(list_of_dfs, ignore_index=True)\n",
    "print(\"Eaglei data concatenated successfully.\")\n",
    "\n",
    "\n",
    "# --- Initial Cleaning and Type Conversions ---\n",
    "print(\"\\nPerforming initial cleaning and type conversions on Eaglei data...\")\n",
    "\n",
    "# Datetime Conversion (Knowing Input is UTC)\n",
    "eaglei_datetime_format = '%Y-%m-%d %H:%M:%S'\n",
    "print(f\"Converting 'run_start_time' (known UTC) using format: {eaglei_datetime_format}\")\n",
    "\n",
    "# Parse the datetime string. Initially, this creates a naive datetime object\n",
    "# because the string format itself doesn't contain timezone info.\n",
    "df_eaglei['run_start_time_parsed'] = pd.to_datetime(df_eaglei['run_start_time'], format=eaglei_datetime_format, errors='coerce')\n",
    "\n",
    "# Check for parsing errors\n",
    "naive_nat_count = df_eaglei['run_start_time_parsed'].isna().sum()\n",
    "if naive_nat_count > 0:\n",
    "    print(f\"Warning: Found {naive_nat_count} NaT values after parsing 'run_start_time'.\")\n",
    "    # Optional: drop rows with invalid dates if needed\n",
    "    # df_eaglei.dropna(subset=['run_start_time_parsed'], inplace=True)\n",
    "\n",
    "# *** Assign UTC timezone ***\n",
    "# Since we know the original times represent UTC moments, we *localize* the naive\n",
    "# datetime objects to UTC. This tells pandas these times are UTC.\n",
    "print(\"Assigning UTC timezone to parsed 'run_start_time'...\")\n",
    "try:\n",
    "    df_eaglei['EAGLEI_DT_UTC'] = df_eaglei['run_start_time_parsed'].dt.tz_localize('UTC')\n",
    "except AttributeError as e:\n",
    "     # Fallback in case parsing resulted in 'object' dtype\n",
    "     print(f\"Error: Failed using .dt accessor. Attempting direct conversion specifying UTC...\")\n",
    "     df_eaglei['EAGLEI_DT_UTC'] = pd.to_datetime(df_eaglei['run_start_time_parsed'], errors='coerce', utc=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error during UTC assignment: {e}\")\n",
    "    df_eaglei['EAGLEI_DT_UTC'] = pd.NaT\n",
    "\n",
    "utc_nat_count = df_eaglei['EAGLEI_DT_UTC'].isna().sum()\n",
    "if utc_nat_count > naive_nat_count:\n",
    "     print(f\"Warning: {utc_nat_count - naive_nat_count} additional NaTs introduced during UTC assignment.\")\n",
    "\n",
    "# Can drop the intermediate parsed column now\n",
    "df_eaglei.drop(columns=['run_start_time_parsed'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# FIPS Code Formatting\n",
    "print(\"Formatting 'fips_code'...\")\n",
    "if 'fips_code' in df_eaglei.columns:\n",
    "    df_eaglei['fips_code'] = df_eaglei['fips_code'].astype(str)\n",
    "    df_eaglei['fips_code'] = df_eaglei['fips_code'].str.zfill(5)\n",
    "else:\n",
    "    print(\"Warning: 'fips_code' column not found in Eaglei data!\")\n",
    "\n",
    "\n",
    "# Customers Out Type Check\n",
    "print(\"Checking 'customers_out' type...\")\n",
    "if 'customers_out' in df_eaglei.columns:\n",
    "    df_eaglei['customers_out'] = pd.to_numeric(df_eaglei['customers_out'], errors='coerce')\n",
    "    customers_out_nan_before = df_eaglei['customers_out'].isna().sum()\n",
    "    df_eaglei['customers_out'].fillna(0, inplace=True)\n",
    "    customers_out_nan_after = df_eaglei['customers_out'].isna().sum()\n",
    "    if customers_out_nan_before > customers_out_nan_after:\n",
    "        print(f\"  Filled {customers_out_nan_before - customers_out_nan_after} NaN values in 'customers_out' with 0.\")\n",
    "    # Optionally convert to integer if appropriate\n",
    "    # df_eaglei['customers_out'] = df_eaglei['customers_out'].astype(int)\n",
    "else:\n",
    "    print(\"Warning: 'customers_out' column not found in Eaglei data!\")\n",
    "\n",
    "\n",
    "# --- Display Info and Head ---\n",
    "print(\"\\n--- Eaglei DataFrame Info after initial cleaning ---\")\n",
    "df_eaglei.info(verbose=True, show_counts=True)\n",
    "\n",
    "print(\"\\n--- Eaglei DataFrame Head (Cleaned) ---\")\n",
    "print(df_eaglei[['fips_code', 'run_start_time', 'EAGLEI_DT_UTC', 'customers_out']].head())\n",
    "\n",
    "print(\"\\n--- Data Types of Final Eaglei Columns ---\")\n",
    "print(df_eaglei[['fips_code', 'EAGLEI_DT_UTC', 'customers_out']].dtypes)\n",
    "\n",
    "print(\"\\nStep 2 (Eaglei Load/Clean) Complete - Processed as UTC.\")\n",
    "# df_eaglei now contains the loaded and cleaned Eaglei data\n",
    "# Key columns: fips_code (string, padded), EAGLEI_DT_UTC (datetime64[ns, UTC]), customers_out (numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffaf1bd9-8ea0-499f-ba7c-0ed4e112decf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips_code</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>customers_out</th>\n",
       "      <th>run_start_time</th>\n",
       "      <th>run_start_time_naive</th>\n",
       "      <th>EAGLEI_DT_UTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01037</td>\n",
       "      <td>Coosa</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01051</td>\n",
       "      <td>Elmore</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01109</td>\n",
       "      <td>Pike</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01121</td>\n",
       "      <td>Talladega</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04017</td>\n",
       "      <td>Navajo</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133063</th>\n",
       "      <td>55095</td>\n",
       "      <td>Polk</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133064</th>\n",
       "      <td>55105</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133065</th>\n",
       "      <td>55109</td>\n",
       "      <td>St. Croix</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133066</th>\n",
       "      <td>55129</td>\n",
       "      <td>Washburn</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133067</th>\n",
       "      <td>56039</td>\n",
       "      <td>Teton</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191133068 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fips_code     county      state  customers_out       run_start_time  \\\n",
       "0             01037      Coosa    Alabama           12.0  2014-11-01 04:00:00   \n",
       "1             01051     Elmore    Alabama            7.0  2014-11-01 04:00:00   \n",
       "2             01109       Pike    Alabama            1.0  2014-11-01 04:00:00   \n",
       "3             01121  Talladega    Alabama           31.0  2014-11-01 04:00:00   \n",
       "4             04017     Navajo    Arizona            1.0  2014-11-01 04:00:00   \n",
       "...             ...        ...        ...            ...                  ...   \n",
       "191133063     55095       Polk  Wisconsin            0.0  2023-12-31 23:45:00   \n",
       "191133064     55105       Rock  Wisconsin            1.0  2023-12-31 23:45:00   \n",
       "191133065     55109  St. Croix  Wisconsin            0.0  2023-12-31 23:45:00   \n",
       "191133066     55129   Washburn  Wisconsin            0.0  2023-12-31 23:45:00   \n",
       "191133067     56039      Teton    Wyoming            2.0  2023-12-31 23:45:00   \n",
       "\n",
       "          run_start_time_naive             EAGLEI_DT_UTC  \n",
       "0          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "1          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "2          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "3          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "4          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "...                        ...                       ...  \n",
       "191133063  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133064  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133065  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133066  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133067  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "\n",
       "[191133068 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eaglei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1df24cac-23e3-4be3-87ad-8597511a17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eaglei.to_csv(\"eaglei_2014_2024.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e87ec6-1bec-4b91-93ec-e19a9e4fb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coosa' 'Elmore' 'Pike' ... 'Taos' 'Toole' 'Prentiss']\n"
     ]
    }
   ],
   "source": [
    "df_eaglei = pd.read_csv(\"eaglei_2014_2024.csv\")\n",
    "unq = df_noaa['CZ_NAME'].unique()\n",
    "unq1 = df_eaglei['county'].unique()\n",
    "print(unq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e626cd-2f38-468e-90d7-43124fb73581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference count: 220\n",
      "Unique to NOAA: 176\n",
      "Unique to EagleI: 44\n",
      "\n",
      "Sample unique to NOAA: ['ALASKA PENINSULA', 'VIEQUES', 'QUEBRADILLAS']\n",
      "Sample unique to EagleI: ['CARSON CITY', 'KALAWAO', 'COLONIAL HEIGHTS']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values (with NaN handling)\n",
    "noaa_counties = set(df_noaa['CZ_NAME'].dropna().str.upper().unique())\n",
    "eaglei_counties = set(df_eaglei['county'].dropna().str.upper().unique())\n",
    "\n",
    "# Check equality\n",
    "if noaa_counties == eaglei_counties:\n",
    "    print(\"Both datasets contain EXACTLY the same county names\")\n",
    "else:\n",
    "    # Calculate differences\n",
    "    only_in_noaa = noaa_counties - eaglei_counties\n",
    "    only_in_eaglei = eaglei_counties - noaa_counties\n",
    "    \n",
    "    print(f\"Difference count: {len(only_in_noaa) + len(only_in_eaglei)}\")\n",
    "    print(f\"Unique to NOAA: {len(only_in_noaa)}\")\n",
    "    print(f\"Unique to EagleI: {len(only_in_eaglei)}\")\n",
    "    \n",
    "    # Optional: Show examples of mismatches\n",
    "    print(\"\\nSample unique to NOAA:\", list(only_in_noaa)[:3])\n",
    "    print(\"Sample unique to EagleI:\", list(only_in_eaglei)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b3de1bc-6437-494d-9cbc-d8768b82e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noaa.to_csv('noaa_minimal_feature_eng.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c868884e-04be-4145-bf53-c61e1feea62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Step 4: Merging NOAA and Eaglei Data...\n",
      "Creating 5-digit FIPS code in NOAA data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104346/1207266933.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['01001' '01001' '01001' ... '99153' '99153' '99153']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_noaa.loc[valid_fips_mask, 'full_fips_code'] = state_fips_padded[valid_fips_mask] + cz_fips_padded[valid_fips_mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'full_fips_code' created.\n",
      "  Example NOAA full_fips_code: 01001\n",
      "  Example Eaglei fips_code: 1037\n",
      "Prepared Eaglei subset with 191133068 rows.\n",
      "Sorting DataFrames (this might take time)...\n",
      "DataFrames sorted.\n",
      "Merge tolerance set to: 0 days 06:00:00\n",
      "datetime64[ns, UTC]\n",
      "object\n",
      "datetime64[ns, UTC]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Assume df_noaa exists from Step 3\n",
    "# Assume df_eaglei exists from Step 2\n",
    "df_noaa = pd.read_csv(\"noaa_minimal_feature_eng.csv\")\n",
    "df_eaglei = pd.read_csv(\"eaglei_2014_2024.csv\")\n",
    "print(\"\\nStarting Step 4: Merging NOAA and Eaglei Data...\")\n",
    "\n",
    "# --- 1. Create full 5-digit FIPS code in df_noaa ---\n",
    "print(\"Creating 5-digit FIPS code in NOAA data...\")\n",
    "if 'STATE_FIPS' in df_noaa.columns and 'CZ_FIPS' in df_noaa.columns:\n",
    "    # Ensure source columns are strings and handle potential floats/NaNs gracefully first\n",
    "    df_noaa['STATE_FIPS_STR'] = df_noaa['STATE_FIPS'].astype(str).str.split('.').str[0]\n",
    "    df_noaa['CZ_FIPS_STR'] = df_noaa['CZ_FIPS'].astype(str).str.split('.').str[0]\n",
    "\n",
    "    # Pad STATE_FIPS to 2 digits, CZ_FIPS to 3 digits\n",
    "    state_fips_padded = df_noaa['STATE_FIPS_STR'].str.zfill(2)\n",
    "    cz_fips_padded = df_noaa['CZ_FIPS_STR'].str.zfill(3)\n",
    "\n",
    "    # Concatenate, but only if both parts are valid (not NaN after conversion)\n",
    "    # Create mask for valid rows\n",
    "    valid_fips_mask = state_fips_padded.notna() & cz_fips_padded.notna()\n",
    "    df_noaa['full_fips_code'] = np.nan # Initialize with NaN\n",
    "    df_noaa.loc[valid_fips_mask, 'full_fips_code'] = state_fips_padded[valid_fips_mask] + cz_fips_padded[valid_fips_mask]\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df_noaa.drop(columns=['STATE_FIPS_STR', 'CZ_FIPS_STR'], inplace=True)\n",
    "\n",
    "    # Check how many NaNs were created\n",
    "    fips_nan_count = df_noaa['full_fips_code'].isna().sum()\n",
    "    if fips_nan_count > 0:\n",
    "        print(f\"  Warning: Created {fips_nan_count} NaN values in 'full_fips_code' due to missing STATE or CZ FIPS.\")\n",
    "        # Optional: Drop rows with missing FIPS before merging if needed\n",
    "        # df_noaa.dropna(subset=['full_fips_code'], inplace=True)\n",
    "\n",
    "    print(\"  'full_fips_code' created.\")\n",
    "    # Verify one FIPS code\n",
    "    print(f\"  Example NOAA full_fips_code: {df_noaa['full_fips_code'].iloc[0] if not df_noaa.empty else 'N/A'}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: STATE_FIPS or CZ_FIPS missing from df_noaa. Cannot create full FIPS code.\")\n",
    "    # Handle error - cannot proceed with merge\n",
    "    exit()\n",
    "\n",
    "# Verify Eaglei FIPS code formatting (should be done already)\n",
    "print(f\"  Example Eaglei fips_code: {df_eaglei['fips_code'].iloc[0] if not df_eaglei.empty else 'N/A'}\")\n",
    "\n",
    "\n",
    "# --- 2. Prepare df_eaglei Key (already done) ---\n",
    "# Ensure required columns exist\n",
    "if not all(col in df_eaglei.columns for col in ['fips_code', 'EAGLEI_DT_UTC', 'customers_out']):\n",
    "     print(\"Error: Required columns missing from df_eaglei.\")\n",
    "     exit()\n",
    "# Select only necessary columns from Eaglei to save memory during merge\n",
    "df_eaglei_subset = df_eaglei[['fips_code', 'EAGLEI_DT_UTC', 'customers_out']].copy()\n",
    "print(f\"Prepared Eaglei subset with {len(df_eaglei_subset)} rows.\")\n",
    "\n",
    "\n",
    "# --- 3. Sort DataFrames ---\n",
    "print(\"Sorting DataFrames (this might take time)...\")\n",
    "# Sort NOAA by the new full FIPS and storm END time\n",
    "df_noaa.sort_values(by=['full_fips_code', 'END_DT_UTC'], inplace=True)\n",
    "# Sort Eaglei subset by FIPS and outage time\n",
    "df_eaglei_subset.sort_values(by=['fips_code', 'EAGLEI_DT_UTC'], inplace=True)\n",
    "print(\"DataFrames sorted.\")\n",
    "\n",
    "df_eaglei_subset['EAGLEI_DT_UTC'] = pd.to_datetime(df_eaglei_subset['EAGLEI_DT_UTC'], errors='coerce', utc=True)\n",
    "df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC'], errors='coerce', utc=True)\n",
    "# And similarly for df_noaa['END_DT_UTC']\n",
    "df_eaglei_subset['fips_code'] = df_eaglei_subset['fips_code'].astype(str).str.zfill(5)\n",
    "\n",
    "# --- 4. Define Target & Tolerance ---\n",
    "# Let's look for outages within 6 hours AFTER a storm ends\n",
    "merge_tolerance = pd.Timedelta('6h')\n",
    "print(f\"Merge tolerance set to: {merge_tolerance}\")\n",
    "print(df_noaa['END_DT_UTC'].dtype)\n",
    "print(df_noaa['full_fips_code'].dtype)\n",
    "print(df_eaglei_subset['EAGLEI_DT_UTC'].dtype)\n",
    "print(df_eaglei_subset['fips_code'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5106c-d939-4a5b-87de-482bc38f946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Assume df_noaa exists from Step 3\n",
    "# Assume df_eaglei exists from Step 2\n",
    "df_noaa = pd.read_csv(\"noaa_minimal_feature_eng.csv\")\n",
    "df_eaglei = pd.read_csv(\"eaglei_2014_2024.csv\")\n",
    "print(\"\\nStarting Step 4: Merging NOAA and Eaglei Data...\")\n",
    "\n",
    "# --- 1. Create full 5-digit FIPS code in df_noaa ---\n",
    "print(\"Creating 5-digit FIPS code in NOAA data...\")\n",
    "if 'STATE_FIPS' in df_noaa.columns and 'CZ_FIPS' in df_noaa.columns:\n",
    "    # Ensure source columns are strings and handle potential floats/NaNs gracefully first\n",
    "    df_noaa['STATE_FIPS_STR'] = df_noaa['STATE_FIPS'].astype(str).str.split('.').str[0]\n",
    "    df_noaa['CZ_FIPS_STR'] = df_noaa['CZ_FIPS'].astype(str).str.split('.').str[0]\n",
    "\n",
    "    # Pad STATE_FIPS to 2 digits, CZ_FIPS to 3 digits\n",
    "    state_fips_padded = df_noaa['STATE_FIPS_STR'].str.zfill(2)\n",
    "    cz_fips_padded = df_noaa['CZ_FIPS_STR'].str.zfill(3)\n",
    "\n",
    "    # Concatenate, but only if both parts are valid (not NaN after conversion)\n",
    "    # Create mask for valid rows\n",
    "    valid_fips_mask = state_fips_padded.notna() & cz_fips_padded.notna()\n",
    "    df_noaa['full_fips_code'] = np.nan # Initialize with NaN\n",
    "    df_noaa.loc[valid_fips_mask, 'full_fips_code'] = state_fips_padded[valid_fips_mask] + cz_fips_padded[valid_fips_mask]\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df_noaa.drop(columns=['STATE_FIPS_STR', 'CZ_FIPS_STR'], inplace=True)\n",
    "\n",
    "    # Check how many NaNs were created\n",
    "    fips_nan_count = df_noaa['full_fips_code'].isna().sum()\n",
    "    if fips_nan_count > 0:\n",
    "        print(f\"  Warning: Created {fips_nan_count} NaN values in 'full_fips_code' due to missing STATE or CZ FIPS.\")\n",
    "        # Optional: Drop rows with missing FIPS before merging if needed\n",
    "        # df_noaa.dropna(subset=['full_fips_code'], inplace=True)\n",
    "\n",
    "    print(\"  'full_fips_code' created.\")\n",
    "    # Verify one FIPS code\n",
    "    print(f\"  Example NOAA full_fips_code: {df_noaa['full_fips_code'].iloc[0] if not df_noaa.empty else 'N/A'}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: STATE_FIPS or CZ_FIPS missing from df_noaa. Cannot create full FIPS code.\")\n",
    "    # Handle error - cannot proceed with merge\n",
    "    exit()\n",
    "\n",
    "# Verify Eaglei FIPS code formatting (should be done already)\n",
    "print(f\"  Example Eaglei fips_code: {df_eaglei['fips_code'].iloc[0] if not df_eaglei.empty else 'N/A'}\")\n",
    "\n",
    "\n",
    "# --- 2. Prepare df_eaglei Key (already done) ---\n",
    "# Ensure required columns exist\n",
    "if not all(col in df_eaglei.columns for col in ['fips_code', 'EAGLEI_DT_UTC', 'customers_out']):\n",
    "     print(\"Error: Required columns missing from df_eaglei.\")\n",
    "     exit()\n",
    "# Select only necessary columns from Eaglei to save memory during merge\n",
    "df_eaglei_subset = df_eaglei[['fips_code', 'EAGLEI_DT_UTC', 'customers_out']].copy()\n",
    "print(f\"Prepared Eaglei subset with {len(df_eaglei_subset)} rows.\")\n",
    "\n",
    "\n",
    "# --- 3. Sort DataFrames ---\n",
    "print(\"Sorting DataFrames (this might take time)...\")\n",
    "# Sort NOAA by the new full FIPS and storm END time\n",
    "df_noaa.sort_values(by=['full_fips_code', 'END_DT_UTC'], inplace=True)\n",
    "# Sort Eaglei subset by FIPS and outage time\n",
    "df_eaglei_subset.sort_values(by=['fips_code', 'EAGLEI_DT_UTC'], inplace=True)\n",
    "print(\"DataFrames sorted.\")\n",
    "\n",
    "df_eaglei_subset['EAGLEI_DT_UTC'] = pd.to_datetime(df_eaglei_subset['EAGLEI_DT_UTC'], errors='coerce', utc=True)\n",
    "df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC'], errors='coerce', utc=True)\n",
    "# And similarly for df_noaa['END_DT_UTC']\n",
    "df_eaglei_subset['fips_code'] = df_eaglei_subset['fips_code'].astype(str).str.zfill(5)\n",
    "\n",
    "# --- 4. Define Target & Tolerance ---\n",
    "# Let's look for outages within 6 hours AFTER a storm ends\n",
    "merge_tolerance = pd.Timedelta('6h')\n",
    "print(f\"Merge tolerance set to: {merge_tolerance}\")\n",
    "print(df_noaa['END_DT_UTC'].dtype)\n",
    "print(df_noaa['full_fips_code'].dtype)\n",
    "print(df_eaglei_subset['EAGLEI_DT_UTC'].dtype)\n",
    "print(df_eaglei_subset['fips_code'].dtype)\n",
    "\n",
    "print(\"Performing merge_asof (this is memory intensive and may take significant time)...\")\n",
    "try:\n",
    "    # Merge df_noaa (left) with df_eaglei_subset (right)\n",
    "    # For each storm in df_noaa, find the Eaglei record matching FIPS\n",
    "    # where Eaglei time is >= storm end time, within the tolerance.\n",
    "    merged_df = pd.merge_asof(\n",
    "        df_noaa,                     # Left DataFrame\n",
    "        df_eaglei_subset,            # Right DataFrame (subset)\n",
    "        left_on='END_DT_UTC',        # Time column in left df (storm end)\n",
    "        right_on='EAGLEI_DT_UTC',    # Time column in right df (outage time)\n",
    "        left_by='full_fips_code',    # Key column in left df (county FIPS)\n",
    "        right_by='fips_code',        # Key column in right df (county FIPS)\n",
    "        direction='forward',         # Find Eaglei time >= left time\n",
    "        tolerance=merge_tolerance    # Look ahead up to 6 hours\n",
    "    )\n",
    "    print(\"merge_asof complete.\")\n",
    "\n",
    "    # Rename columns coming from Eaglei for clarity, handling potential conflicts\n",
    "    merged_df.rename(columns={\n",
    "        'EAGLEI_DT_UTC': 'MATCHED_OUTAGE_DT_UTC',\n",
    "        'customers_out': 'MATCHED_CUSTOMERS_OUT'\n",
    "        # Note: 'fips_code' column from df_eaglei_subset will also be present\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Check results\n",
    "    print(f\"Merged DataFrame length: {len(merged_df)}\")\n",
    "    if len(merged_df) != len(df_noaa):\n",
    "         print(\"Warning: Length of merged df differs from original NOAA df. This shouldn't happen with merge_asof.\")\n",
    "\n",
    "    # See how many storms found a matching outage record within the window\n",
    "    matches_found = merged_df['MATCHED_OUTAGE_DT_UTC'].notna().sum()\n",
    "    print(f\"Found matching outage records for {matches_found} out of {len(merged_df)} storm events within the {merge_tolerance} window.\")\n",
    "\n",
    "except MemoryError:\n",
    "    print(\"!!! MEMORY ERROR during merge_asof !!!\")\n",
    "    print(\"The DataFrames are too large for a direct merge in available memory.\")\n",
    "    print(\"Next steps would require chunking, Dask, or more memory.\")\n",
    "    merged_df = None # Indicate failure\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during merge_asof: {e}\")\n",
    "    merged_df = None # Indicate failure\n",
    "\n",
    "\n",
    "# --- Display Info and Head of Merged Data (if successful) ---\n",
    "if merged_df is not None:\n",
    "    print(\"\\n--- Merged DataFrame Info ---\")\n",
    "    merged_df.info(verbose=True, show_counts=True)\n",
    "\n",
    "    print(\"\\n--- Merged DataFrame Head ---\")\n",
    "    print(merged_df[[\n",
    "        'EVENT_ID', 'full_fips_code', 'END_DT_UTC',\n",
    "        'MATCHED_OUTAGE_DT_UTC', 'MATCHED_CUSTOMERS_OUT'\n",
    "        # Add other relevant NOAA columns to view\n",
    "    ]].head())\n",
    "\n",
    "    print(\"\\nStep 4 (Merge) Complete.\")\n",
    "else:\n",
    "    print(\"\\nStep 4 (Merge) FAILED due to errors.\")\n",
    "\n",
    "# 'merged_df' now holds the result IF the merge was successful.\n",
    "# It has one row per original NOAA storm event.\n",
    "# Rows where no outage was found within the window will have NaT/NaN\n",
    "# in MATCHED_OUTAGE_DT_UTC and MATCHED_CUSTOMERS_OUT.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
