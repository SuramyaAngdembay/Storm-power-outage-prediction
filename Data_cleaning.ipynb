{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c05870a-f905-4390-8266-88c0d56db13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA data loaded successfully.\n",
      "Performing initial cleaning and type conversions...\n",
      "\n",
      "--- Unique Timezones Found in CZ_TIMEZONE ---\n",
      "['EST-5' 'CST-6' 'PST-8' 'MST-7' 'HST-10' 'AKST-9' 'AST-4' 'GST10'\n",
      " 'SST-11' 'PDT-7' 'CDT-5' 'EDT-4']\n",
      "Number of unique timezone entries: 12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "noaa_file_path = \"./data/NOAA_StormEvents/StormEvents_2014_2024.csv\"\n",
    "# [Previous loading code remains the same]\n",
    "# ...\n",
    "try:\n",
    "    df_noaa = pd.read_csv(noaa_file_path, low_memory=False)\n",
    "    print(\"NOAA data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {noaa_file_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NOAA data: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Performing initial cleaning and type conversions...\")\n",
    "\n",
    "# --- Datetime Conversion ---\n",
    "noaa_datetime_format = '%d-%b-%y %H:%M:%S'\n",
    "df_noaa['BEGIN_DT'] = pd.to_datetime(df_noaa['BEGIN_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "df_noaa['END_DT'] = pd.to_datetime(df_noaa['END_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "\n",
    "begin_nat_count = df_noaa['BEGIN_DT'].isna().sum()\n",
    "end_nat_count = df_noaa['END_DT'].isna().sum()\n",
    "# ... (rest of initial datetime parsing checks) ...\n",
    "\n",
    "# --- *** INSPECT TIMEZONES *** ---\n",
    "print(\"\\n--- Unique Timezones Found in CZ_TIMEZONE ---\")\n",
    "unique_timezones = df_noaa['CZ_TIMEZONE'].unique()\n",
    "print(unique_timezones)\n",
    "print(f\"Number of unique timezone entries: {len(unique_timezones)}\")\n",
    "# --- *** END INSPECTION *** ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e6e334-201f-4388-8cb8-48d6f0bacc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NOAA data from: data/NOAA_StormEvents/StormEvents_2014_2024.csv\n",
      "NOAA data loaded successfully.\n",
      "Performing initial cleaning and type conversions...\n",
      "[Timestamp('2014-02-18 10:00:00-0500', tz='America/New_York')\n",
      " Timestamp('2014-03-30 08:31:00-0400', tz='America/New_York')\n",
      " Timestamp('2014-04-27 23:06:00-0500', tz='America/Chicago') ...\n",
      " Timestamp('2024-05-09 12:53:00-0500', tz='America/Chicago')\n",
      " Timestamp('2024-05-22 18:09:00-0400', tz='America/New_York')\n",
      " Timestamp('2024-08-06 07:52:00-0400', tz='America/New_York')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Define the path to the NOAA data file\n",
    "noaa_file_path = 'data/NOAA_StormEvents/StormEvents_2014_2024.csv'\n",
    "\n",
    "# Define columns that are likely numeric but might have issues during load\n",
    "numeric_cols_to_check = [\n",
    "    'BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH', 'END_DAY', 'END_TIME',\n",
    "    'EPISODE_ID', 'EVENT_ID', 'STATE_FIPS', 'CZ_FIPS',\n",
    "    'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT',\n",
    "    'MAGNITUDE', 'TOR_LENGTH', 'TOR_WIDTH',\n",
    "    'BEGIN_RANGE', 'END_RANGE', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON'\n",
    "]\n",
    "\n",
    "# Define the expected datetime format\n",
    "# Using errors='coerce' will turn unparseable dates into NaT (Not a Time)\n",
    "noaa_datetime_format = '%d-%b-%y %H:%M:%S'\n",
    "\n",
    "print(f\"Loading NOAA data from: {noaa_file_path}\")\n",
    "\n",
    "# --- Load the data ---\n",
    "# Consider using low_memory=False if dtype warnings appear, or specify dtypes more precisely\n",
    "# For very large files, consider chunking or libraries like Dask/Polars\n",
    "try:\n",
    "    df_noaa = pd.read_csv(noaa_file_path, low_memory=False)\n",
    "    print(\"NOAA data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {noaa_file_path}\")\n",
    "    # Exit or handle error appropriately\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NOAA data: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Performing initial cleaning and type conversions...\")\n",
    "\n",
    "# --- Datetime Conversion ---\n",
    "# Convert BEGIN_DATE_TIME and END_DATE_TIME\n",
    "# errors='coerce' handles unparseable formats by setting them to NaT\n",
    "df_noaa['BEGIN_DT'] = pd.to_datetime(df_noaa['BEGIN_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "df_noaa['END_DT'] = pd.to_datetime(df_noaa['END_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "\n",
    "# Check for parsing errors (NaT values)\n",
    "begin_nat_count = df_noaa['BEGIN_DT'].isna().sum()\n",
    "end_nat_count = df_noaa['END_DT'].isna().sum()\n",
    "if begin_nat_count > 0 or end_nat_count > 0:\n",
    "    print(f\"Warning: Found {begin_nat_count} NaT values in BEGIN_DT after parsing.\")\n",
    "    print(f\"Warning: Found {end_nat_count} NaT values in END_DT after parsing.\")\n",
    "    # Consider dropping or investigating rows with NaT datetimes if they are critical\n",
    "    # df_noaa.dropna(subset=['BEGIN_DT', 'END_DT'], inplace=True)\n",
    "\n",
    "\n",
    "# --- Timezone Handling ---\n",
    "# Map common timezone abbreviations to standard Olson names usable by pandas\n",
    "# This might need expansion based on unique values in CZ_TIMEZONE\n",
    "tz_map = {\n",
    "    # Standard US Timezones (using Olson names that handle DST)\n",
    "    'EST-5': 'America/New_York',    # Eastern Time\n",
    "    'EDT-4': 'America/New_York',    # Eastern Time (Daylight)\n",
    "    'CST-6': 'America/Chicago',     # Central Time\n",
    "    'CDT-5': 'America/Chicago',     # Central Time (Daylight)\n",
    "    'MST-7': 'America/Denver',      # Mountain Time (most areas)\n",
    "    'MDT-6': 'America/Denver',      # Mountain Time (most areas - Daylight) - Added MDT just in case although not in list\n",
    "    'PST-8': 'America/Los_Angeles', # Pacific Time\n",
    "    'PDT-7': 'America/Los_Angeles', # Pacific Time (Daylight)\n",
    "    'AKST-9': 'America/Anchorage',  # Alaska Time\n",
    "    'AKDT-8': 'America/Anchorage',  # Alaska Time (Daylight) - Added AKDT just in case\n",
    "    'HST-10': 'Pacific/Honolulu',   # Hawaii Standard Time (no DST)\n",
    "\n",
    "    # Atlantic & Territories\n",
    "    'AST-4': 'America/Puerto_Rico', # Atlantic Standard Time (no DST in PR)\n",
    "    'GST10': 'Pacific/Guam',        # Guam Standard Time (UTC+10)\n",
    "    'SST-11': 'Pacific/Pago_Pago',   # Samoa Standard Time (UTC-11)\n",
    "\n",
    "    # Add mappings for any potential NaN or empty strings if they exist\n",
    "    '': None, # Map empty string explicitly if needed\n",
    "    # np.nan: None # pd.isna() check in function should handle actual NaN objects\n",
    "}\n",
    "\n",
    "# Function to apply timezone localization\n",
    "def localize_datetime(row):\n",
    "    tz_str = row['CZ_TIMEZONE']\n",
    "    dt = row['datetime_col']\n",
    "    if pd.isna(dt) or pd.isna(tz_str):\n",
    "        return pd.NaT\n",
    "\n",
    "    tz_name = tz_map.get(tz_str)\n",
    "    if tz_name:\n",
    "        try:\n",
    "            # Localize the naive datetime\n",
    "            return dt.tz_localize(tz_name, ambiguous='NaT', nonexistent='NaT')\n",
    "        except Exception as e:\n",
    "            # Log warning for specific row/error if needed\n",
    "            # warnings.warn(f\"Could not localize timezone '{tz_str}' for datetime {dt}: {e}\")\n",
    "            return pd.NaT # Failed to localize\n",
    "    else:\n",
    "        # Log warning for unmapped timezone if needed\n",
    "        # warnings.warn(f\"Timezone '{tz_str}' not found in tz_map.\")\n",
    "        return pd.NaT # Timezone not in map\n",
    "\n",
    "# Apply localization - requires iterating or a more complex apply\n",
    "# Create temporary column for the function\n",
    "df_noaa['datetime_col'] = df_noaa['BEGIN_DT']\n",
    "df_noaa['BEGIN_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "df_noaa['datetime_col'] = df_noaa['END_DT']\n",
    "df_noaa['END_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "unique_dt = df_noaa['BEGIN_DT_LOC'].unique()\n",
    "\n",
    "print(unique_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07f51b1-e50e-480c-92ab-2feada827b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2014-02-18 10:00:00-05:00\n",
       "1         2014-03-30 08:31:00-04:00\n",
       "2         2014-04-27 23:06:00-05:00\n",
       "3         2014-04-27 23:03:00-05:00\n",
       "4         2014-02-15 13:00:00-08:00\n",
       "                    ...            \n",
       "691429    2024-05-26 11:48:00-04:00\n",
       "691430    2024-05-22 18:09:00-04:00\n",
       "691431    2024-05-22 17:57:00-04:00\n",
       "691432    2024-06-23 17:45:00-04:00\n",
       "691433    2024-08-06 07:52:00-04:00\n",
       "Name: BEGIN_DT_LOC, Length: 691434, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noaa['BEGIN_DT_LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33d7e51-7dd6-4e79-bf4f-633dbc9c4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NOAA data from: data/NOAA_StormEvents/StormEvents_2014_2024.csv\n",
      "NOAA data loaded successfully.\n",
      "Performing initial cleaning and type conversions...\n",
      "Parsing original datetime strings...\n",
      "Mapping timezones...\n",
      "Applying timezone localization (this may take time)...\n",
      "Timezone localization applied.\n",
      "Attempting to convert localized columns directly to UTC...\n",
      "Direct UTC conversion attempted.\n",
      "Converting numeric columns...\n",
      "Converting FIPS codes to string...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40165/3760516150.py:138: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_noaa[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NOAA DataFrame Info after initial cleaning ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 691434 entries, 0 to 691433\n",
      "Data columns (total 57 columns):\n",
      " #   Column              Non-Null Count   Dtype              \n",
      "---  ------              --------------   -----              \n",
      " 0   BEGIN_YEARMONTH     691434 non-null  int64              \n",
      " 1   BEGIN_DAY           691434 non-null  int64              \n",
      " 2   BEGIN_TIME          691434 non-null  int64              \n",
      " 3   END_YEARMONTH       691434 non-null  int64              \n",
      " 4   END_DAY             691434 non-null  int64              \n",
      " 5   END_TIME            691434 non-null  int64              \n",
      " 6   EPISODE_ID          691434 non-null  int64              \n",
      " 7   EVENT_ID            691434 non-null  int64              \n",
      " 8   STATE               691434 non-null  object             \n",
      " 9   STATE_FIPS          691434 non-null  object             \n",
      " 10  YEAR                691434 non-null  int64              \n",
      " 11  MONTH_NAME          691434 non-null  object             \n",
      " 12  EVENT_TYPE          691434 non-null  object             \n",
      " 13  CZ_TYPE             691434 non-null  object             \n",
      " 14  CZ_FIPS             691434 non-null  object             \n",
      " 15  CZ_NAME             691434 non-null  object             \n",
      " 16  WFO                 691434 non-null  object             \n",
      " 17  BEGIN_DATE_TIME     691434 non-null  object             \n",
      " 18  CZ_TIMEZONE         691434 non-null  object             \n",
      " 19  END_DATE_TIME       691434 non-null  object             \n",
      " 20  INJURIES_DIRECT     691434 non-null  int64              \n",
      " 21  INJURIES_INDIRECT   691434 non-null  int64              \n",
      " 22  DEATHS_DIRECT       691434 non-null  int64              \n",
      " 23  DEATHS_INDIRECT     691434 non-null  int64              \n",
      " 24  DAMAGE_PROPERTY     550448 non-null  object             \n",
      " 25  DAMAGE_CROPS        552275 non-null  object             \n",
      " 26  SOURCE              691434 non-null  object             \n",
      " 27  MAGNITUDE           360316 non-null  float64            \n",
      " 28  MAGNITUDE_TYPE      262379 non-null  object             \n",
      " 29  FLOOD_CAUSE         75222 non-null   object             \n",
      " 30  CATEGORY            369 non-null     float64            \n",
      " 31  TOR_F_SCALE         15641 non-null   object             \n",
      " 32  TOR_LENGTH          15641 non-null   float64            \n",
      " 33  TOR_WIDTH           15641 non-null   float64            \n",
      " 34  TOR_OTHER_WFO       2043 non-null    object             \n",
      " 35  TOR_OTHER_CZ_STATE  2043 non-null    object             \n",
      " 36  TOR_OTHER_CZ_FIPS   2043 non-null    float64            \n",
      " 37  TOR_OTHER_CZ_NAME   2043 non-null    object             \n",
      " 38  BEGIN_RANGE         425309 non-null  float64            \n",
      " 39  BEGIN_AZIMUTH       425309 non-null  object             \n",
      " 40  BEGIN_LOCATION      425309 non-null  object             \n",
      " 41  END_RANGE           425309 non-null  float64            \n",
      " 42  END_AZIMUTH         425309 non-null  object             \n",
      " 43  END_LOCATION        425309 non-null  object             \n",
      " 44  BEGIN_LAT           425309 non-null  float64            \n",
      " 45  BEGIN_LON           425309 non-null  float64            \n",
      " 46  END_LAT             425309 non-null  float64            \n",
      " 47  END_LON             425309 non-null  float64            \n",
      " 48  EPISODE_NARRATIVE   691434 non-null  object             \n",
      " 49  EVENT_NARRATIVE     547265 non-null  object             \n",
      " 50  DATA_SOURCE         691434 non-null  object             \n",
      " 51  BEGIN_DT            691434 non-null  datetime64[ns]     \n",
      " 52  END_DT              691434 non-null  datetime64[ns]     \n",
      " 53  BEGIN_DT_LOC        691397 non-null  object             \n",
      " 54  END_DT_LOC          691398 non-null  object             \n",
      " 55  BEGIN_DT_UTC        691397 non-null  datetime64[ns, UTC]\n",
      " 56  END_DT_UTC          691398 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](2), datetime64[ns](2), float64(11), int64(13), object(29)\n",
      "memory usage: 300.7+ MB\n",
      "\n",
      "--- NOAA DataFrame Head (focus on datetimes) ---\n",
      "   EVENT_ID CZ_TIMEZONE            BEGIN_DT               BEGIN_DT_LOC  \\\n",
      "0    503953       EST-5 2014-02-18 10:00:00  2014-02-18 10:00:00-05:00   \n",
      "1    507163       EST-5 2014-03-30 08:31:00  2014-03-30 08:31:00-04:00   \n",
      "2    506236       CST-6 2014-04-27 23:06:00  2014-04-27 23:06:00-05:00   \n",
      "3    506237       CST-6 2014-04-27 23:03:00  2014-04-27 23:03:00-05:00   \n",
      "4    501499       PST-8 2014-02-15 13:00:00  2014-02-15 13:00:00-08:00   \n",
      "\n",
      "               BEGIN_DT_UTC                END_DT_UTC  \n",
      "0 2014-02-18 15:00:00+00:00 2014-02-19 01:00:00+00:00  \n",
      "1 2014-03-30 12:31:00+00:00 2014-03-30 13:31:00+00:00  \n",
      "2 2014-04-28 04:06:00+00:00 2014-04-28 04:06:00+00:00  \n",
      "3 2014-04-28 04:03:00+00:00 2014-04-28 04:03:00+00:00  \n",
      "4 2014-02-15 21:00:00+00:00 2014-02-16 05:00:00+00:00  \n",
      "\n",
      "--- Data Types of Final Datetime Columns ---\n",
      "BEGIN_DT_UTC    datetime64[ns, UTC]\n",
      "END_DT_UTC      datetime64[ns, UTC]\n",
      "dtype: object\n",
      "\n",
      "Step 1 (NOAA Load/Clean) Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Define the path to the NOAA data file\n",
    "noaa_file_path = 'data/NOAA_StormEvents/StormEvents_2014_2024.csv'\n",
    "\n",
    "# Define columns that are likely numeric but might have issues during load\n",
    "numeric_cols_to_check = [\n",
    "    'BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH', 'END_DAY', 'END_TIME',\n",
    "    'EPISODE_ID', 'EVENT_ID', 'STATE_FIPS', 'CZ_FIPS',\n",
    "    'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT',\n",
    "    'MAGNITUDE', 'TOR_LENGTH', 'TOR_WIDTH',\n",
    "    'BEGIN_RANGE', 'END_RANGE', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON'\n",
    "]\n",
    "\n",
    "# Define the expected datetime format\n",
    "# Using errors='coerce' will turn unparseable dates into NaT (Not a Time)\n",
    "noaa_datetime_format = '%d-%b-%y %H:%M:%S'\n",
    "\n",
    "print(f\"Loading NOAA data from: {noaa_file_path}\")\n",
    "\n",
    "# --- Load the data ---\n",
    "try:\n",
    "    df_noaa = pd.read_csv(noaa_file_path, low_memory=False)\n",
    "    print(\"NOAA data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {noaa_file_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NOAA data: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Performing initial cleaning and type conversions...\")\n",
    "\n",
    "# --- Datetime Conversion (Initial Parsing) ---\n",
    "print(\"Parsing original datetime strings...\")\n",
    "df_noaa['BEGIN_DT'] = pd.to_datetime(df_noaa['BEGIN_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "df_noaa['END_DT'] = pd.to_datetime(df_noaa['END_DATE_TIME'], format=noaa_datetime_format, errors='coerce')\n",
    "\n",
    "# Check for initial parsing errors (NaT values)\n",
    "begin_nat_count = df_noaa['BEGIN_DT'].isna().sum()\n",
    "end_nat_count = df_noaa['END_DT'].isna().sum()\n",
    "if begin_nat_count > 0 or end_nat_count > 0:\n",
    "    print(f\"Warning: Found {begin_nat_count} NaT values in BEGIN_DT after initial parsing.\")\n",
    "    print(f\"Warning: Found {end_nat_count} NaT values in END_DT after initial parsing.\")\n",
    "\n",
    "# --- Timezone Handling ---\n",
    "print(\"Mapping timezones...\")\n",
    "# Using the tz_map confirmed from your data\n",
    "tz_map = {\n",
    "    'EST-5': 'America/New_York',\n",
    "    'EDT-4': 'America/New_York',\n",
    "    'CST-6': 'America/Chicago',\n",
    "    'CDT-5': 'America/Chicago',\n",
    "    'MST-7': 'America/Denver',\n",
    "    'MDT-6': 'America/Denver', # Keep just in case\n",
    "    'PST-8': 'America/Los_Angeles',\n",
    "    'PDT-7': 'America/Los_Angeles',\n",
    "    'AKST-9': 'America/Anchorage',\n",
    "    'AKDT-8': 'America/Anchorage', # Keep just in case\n",
    "    'HST-10': 'Pacific/Honolulu',\n",
    "    'AST-4': 'America/Puerto_Rico',\n",
    "    'GST10': 'Pacific/Guam',\n",
    "    'SST-11': 'Pacific/Pago_Pago',\n",
    "    '': None, # Map empty string explicitly if needed\n",
    "}\n",
    "\n",
    "# Function to apply timezone localization\n",
    "def localize_datetime(row):\n",
    "    tz_str = row['CZ_TIMEZONE']\n",
    "    dt = row['datetime_col']\n",
    "    if pd.isna(dt) or pd.isna(tz_str):\n",
    "        return pd.NaT\n",
    "\n",
    "    # Ensure tz_str is string and strip whitespace for lookup\n",
    "    tz_str = str(tz_str).strip()\n",
    "    tz_name = tz_map.get(tz_str)\n",
    "\n",
    "    if tz_name:\n",
    "        try:\n",
    "            return dt.tz_localize(tz_name, ambiguous='NaT', nonexistent='NaT')\n",
    "        except Exception as e:\n",
    "            # warnings.warn(f\"Could not localize timezone '{tz_str}' for datetime {dt}: {e}\")\n",
    "            return pd.NaT\n",
    "    else:\n",
    "        if tz_str: # Avoid warning for known blanks mapped to None\n",
    "             warnings.warn(f\"Timezone '{tz_str}' not found in tz_map.\", UserWarning)\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply localization using the helper column\n",
    "print(\"Applying timezone localization (this may take time)...\")\n",
    "df_noaa['datetime_col'] = df_noaa['BEGIN_DT']\n",
    "df_noaa['BEGIN_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "df_noaa['datetime_col'] = df_noaa['END_DT']\n",
    "df_noaa['END_DT_LOC'] = df_noaa.apply(localize_datetime, axis=1)\n",
    "\n",
    "df_noaa.drop(columns=['datetime_col'], inplace=True)\n",
    "print(\"Timezone localization applied.\")\n",
    "\n",
    "# --- Convert Mixed Timezone 'Object' Columns Directly to UTC ---\n",
    "# This step addresses the 'dtype: object' issue after localization\n",
    "print(\"Attempting to convert localized columns directly to UTC...\")\n",
    "try:\n",
    "    # Record NaNs before conversion\n",
    "    original_loc_nan_begin = df_noaa['BEGIN_DT_LOC'].isna().sum()\n",
    "    original_loc_nan_end = df_noaa['END_DT_LOC'].isna().sum()\n",
    "\n",
    "    # Use pd.to_datetime with utc=True to handle the object column containing tz-aware objects\n",
    "    df_noaa['BEGIN_DT_UTC'] = pd.to_datetime(df_noaa['BEGIN_DT_LOC'], errors='coerce', utc=True)\n",
    "    df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_LOC'], errors='coerce', utc=True)\n",
    "\n",
    "    print(\"Direct UTC conversion attempted.\")\n",
    "\n",
    "    # Check for new NaNs potentially introduced\n",
    "    begin_utc_nat_count = df_noaa['BEGIN_DT_UTC'].isna().sum()\n",
    "    end_utc_nat_count = df_noaa['END_DT_UTC'].isna().sum()\n",
    "    if begin_utc_nat_count > original_loc_nan_begin or end_utc_nat_count > original_loc_nan_end:\n",
    "         print(f\"Warning: Additional NaTs potentially introduced during UTC conversion.\")\n",
    "         print(f\"         ({original_loc_nan_begin} -> {begin_utc_nat_count} NaTs in BEGIN_DT_UTC)\")\n",
    "         print(f\"         ({original_loc_nan_end} -> {end_utc_nat_count} NaTs in END_DT_UTC)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to convert localized object columns to UTC. Error: {e}\")\n",
    "    # Assign NaT if conversion fails catastrophically\n",
    "    df_noaa['BEGIN_DT_UTC'] = pd.NaT\n",
    "    df_noaa['END_DT_UTC'] = pd.NaT\n",
    "\n",
    "\n",
    "# --- Numeric Conversions ---\n",
    "print(\"Converting numeric columns...\")\n",
    "for col in numeric_cols_to_check:\n",
    "    if col in df_noaa.columns:\n",
    "        df_noaa[col] = pd.to_numeric(df_noaa[col], errors='coerce')\n",
    "        if 'INJURIES' in col or 'DEATHS' in col:\n",
    "            # Fill NaNs only for specific columns where 0 makes sense\n",
    "            df_noaa[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# --- FIPS Code to String ---\n",
    "print(\"Converting FIPS codes to string...\")\n",
    "if 'STATE_FIPS' in df_noaa.columns:\n",
    "    # Use .astype(str).str.split('.').str[0] to handle potential floats before converting to string\n",
    "    df_noaa['STATE_FIPS'] = df_noaa['STATE_FIPS'].astype(str).str.split('.').str[0]\n",
    "if 'CZ_FIPS' in df_noaa.columns:\n",
    "    df_noaa['CZ_FIPS'] = df_noaa['CZ_FIPS'].astype(str).str.split('.').str[0]\n",
    "\n",
    "\n",
    "# --- Display Info and Head ---\n",
    "print(\"\\n--- NOAA DataFrame Info after initial cleaning ---\")\n",
    "df_noaa.info(verbose=True, show_counts=True)\n",
    "\n",
    "print(\"\\n--- NOAA DataFrame Head (focus on datetimes) ---\")\n",
    "print(df_noaa[['EVENT_ID', 'CZ_TIMEZONE', 'BEGIN_DT', 'BEGIN_DT_LOC', 'BEGIN_DT_UTC', 'END_DT_UTC']].head())\n",
    "\n",
    "print(\"\\n--- Data Types of Final Datetime Columns ---\")\n",
    "print(df_noaa[['BEGIN_DT_UTC', 'END_DT_UTC']].dtypes)\n",
    "\n",
    "print(\"\\nStep 1 (NOAA Load/Clean) Complete.\")\n",
    "# df_noaa now contains the loaded and initially cleaned NOAA data\n",
    "# Key final columns: BEGIN_DT_UTC, END_DT_UTC (should be datetime64[ns, UTC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2800bd23-e29a-480b-af34-e6d018dbadc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "      <th>BEGIN_DT</th>\n",
       "      <th>END_DT</th>\n",
       "      <th>BEGIN_DT_LOC</th>\n",
       "      <th>END_DT_LOC</th>\n",
       "      <th>BEGIN_DT_UTC</th>\n",
       "      <th>END_DT_UTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201402</td>\n",
       "      <td>18</td>\n",
       "      <td>1000</td>\n",
       "      <td>201402</td>\n",
       "      <td>18</td>\n",
       "      <td>2000</td>\n",
       "      <td>83473</td>\n",
       "      <td>503953</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure developing south of Long Island a...</td>\n",
       "      <td>Eight to twelve inches of snow fell across eas...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-02-18 10:00:00</td>\n",
       "      <td>2014-02-18 20:00:00</td>\n",
       "      <td>2014-02-18 10:00:00-05:00</td>\n",
       "      <td>2014-02-18 20:00:00-05:00</td>\n",
       "      <td>2014-02-18 15:00:00+00:00</td>\n",
       "      <td>2014-02-19 01:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201403</td>\n",
       "      <td>30</td>\n",
       "      <td>831</td>\n",
       "      <td>201403</td>\n",
       "      <td>30</td>\n",
       "      <td>931</td>\n",
       "      <td>83971</td>\n",
       "      <td>507163</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.3469</td>\n",
       "      <td>A stacked low pressure system passed south and...</td>\n",
       "      <td>Boston Road was closed near Brian Road due to ...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-03-30 08:31:00</td>\n",
       "      <td>2014-03-30 09:31:00</td>\n",
       "      <td>2014-03-30 08:31:00-04:00</td>\n",
       "      <td>2014-03-30 09:31:00-04:00</td>\n",
       "      <td>2014-03-30 12:31:00+00:00</td>\n",
       "      <td>2014-03-30 13:31:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2306</td>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2306</td>\n",
       "      <td>83517</td>\n",
       "      <td>506236</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.6600</td>\n",
       "      <td>A powerful storm system and a dry line produce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-04-27 23:06:00</td>\n",
       "      <td>2014-04-27 23:06:00</td>\n",
       "      <td>2014-04-27 23:06:00-05:00</td>\n",
       "      <td>2014-04-27 23:06:00-05:00</td>\n",
       "      <td>2014-04-28 04:06:00+00:00</td>\n",
       "      <td>2014-04-28 04:06:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2303</td>\n",
       "      <td>201404</td>\n",
       "      <td>27</td>\n",
       "      <td>2303</td>\n",
       "      <td>83517</td>\n",
       "      <td>506237</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.6600</td>\n",
       "      <td>A powerful storm system and a dry line produce...</td>\n",
       "      <td>Several power poles snapped and trees blown down.</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-04-27 23:03:00</td>\n",
       "      <td>2014-04-27 23:03:00</td>\n",
       "      <td>2014-04-27 23:03:00-05:00</td>\n",
       "      <td>2014-04-27 23:03:00-05:00</td>\n",
       "      <td>2014-04-28 04:03:00+00:00</td>\n",
       "      <td>2014-04-28 04:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201402</td>\n",
       "      <td>15</td>\n",
       "      <td>1300</td>\n",
       "      <td>201402</td>\n",
       "      <td>15</td>\n",
       "      <td>2100</td>\n",
       "      <td>83132</td>\n",
       "      <td>501499</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A strong cold front produced strong winds for ...</td>\n",
       "      <td>Two stations measured strong wind gusts in the...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2014-02-15 13:00:00</td>\n",
       "      <td>2014-02-15 21:00:00</td>\n",
       "      <td>2014-02-15 13:00:00-08:00</td>\n",
       "      <td>2014-02-15 21:00:00-08:00</td>\n",
       "      <td>2014-02-15 21:00:00+00:00</td>\n",
       "      <td>2014-02-16 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691429</th>\n",
       "      <td>202405</td>\n",
       "      <td>26</td>\n",
       "      <td>1148</td>\n",
       "      <td>202405</td>\n",
       "      <td>26</td>\n",
       "      <td>1148</td>\n",
       "      <td>192532</td>\n",
       "      <td>1188957</td>\n",
       "      <td>KENTUCKY</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>-84.7200</td>\n",
       "      <td>A strong storm system moved across the Ohio an...</td>\n",
       "      <td>A trained spotter estimated 60 mph wind gusts ...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-05-26 11:48:00</td>\n",
       "      <td>2024-05-26 11:48:00</td>\n",
       "      <td>2024-05-26 11:48:00-04:00</td>\n",
       "      <td>2024-05-26 11:48:00-04:00</td>\n",
       "      <td>2024-05-26 15:48:00+00:00</td>\n",
       "      <td>2024-05-26 15:48:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691430</th>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1809</td>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1809</td>\n",
       "      <td>192530</td>\n",
       "      <td>1188234</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>-85.7364</td>\n",
       "      <td>A cold front moved into the Ohio Valley during...</td>\n",
       "      <td>A tree was down at Lovers Lane and Prewitt Lane.</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-05-22 18:09:00</td>\n",
       "      <td>2024-05-22 18:09:00</td>\n",
       "      <td>2024-05-22 18:09:00-04:00</td>\n",
       "      <td>2024-05-22 18:09:00-04:00</td>\n",
       "      <td>2024-05-22 22:09:00+00:00</td>\n",
       "      <td>2024-05-22 22:09:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691431</th>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1757</td>\n",
       "      <td>202405</td>\n",
       "      <td>22</td>\n",
       "      <td>1757</td>\n",
       "      <td>192530</td>\n",
       "      <td>1188232</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>-86.7247</td>\n",
       "      <td>A cold front moved into the Ohio Valley during...</td>\n",
       "      <td>A tree was reported down over Chestnut Grove R...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-05-22 17:57:00</td>\n",
       "      <td>2024-05-22 17:57:00</td>\n",
       "      <td>2024-05-22 17:57:00-04:00</td>\n",
       "      <td>2024-05-22 17:57:00-04:00</td>\n",
       "      <td>2024-05-22 21:57:00+00:00</td>\n",
       "      <td>2024-05-22 21:57:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691432</th>\n",
       "      <td>202406</td>\n",
       "      <td>23</td>\n",
       "      <td>1745</td>\n",
       "      <td>202406</td>\n",
       "      <td>23</td>\n",
       "      <td>1750</td>\n",
       "      <td>191388</td>\n",
       "      <td>1192879</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>-70.8400</td>\n",
       "      <td>A supercell thunderstorm developed across sout...</td>\n",
       "      <td>A supercell thunderstorm dropped hail the size...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-06-23 17:45:00</td>\n",
       "      <td>2024-06-23 17:50:00</td>\n",
       "      <td>2024-06-23 17:45:00-04:00</td>\n",
       "      <td>2024-06-23 17:50:00-04:00</td>\n",
       "      <td>2024-06-23 21:45:00+00:00</td>\n",
       "      <td>2024-06-23 21:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691433</th>\n",
       "      <td>202408</td>\n",
       "      <td>6</td>\n",
       "      <td>752</td>\n",
       "      <td>202408</td>\n",
       "      <td>6</td>\n",
       "      <td>852</td>\n",
       "      <td>195694</td>\n",
       "      <td>1214246</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.1375</td>\n",
       "      <td>Debby first developed into a tropical storm ab...</td>\n",
       "      <td>A Chatham County emergency manager reported a ...</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2024-08-06 07:52:00</td>\n",
       "      <td>2024-08-06 08:52:00</td>\n",
       "      <td>2024-08-06 07:52:00-04:00</td>\n",
       "      <td>2024-08-06 08:52:00-04:00</td>\n",
       "      <td>2024-08-06 11:52:00+00:00</td>\n",
       "      <td>2024-08-06 12:52:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691434 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  \\\n",
       "0                201402         18        1000         201402       18   \n",
       "1                201403         30         831         201403       30   \n",
       "2                201404         27        2306         201404       27   \n",
       "3                201404         27        2303         201404       27   \n",
       "4                201402         15        1300         201402       15   \n",
       "...                 ...        ...         ...            ...      ...   \n",
       "691429           202405         26        1148         202405       26   \n",
       "691430           202405         22        1809         202405       22   \n",
       "691431           202405         22        1757         202405       22   \n",
       "691432           202406         23        1745         202406       23   \n",
       "691433           202408          6         752         202408        6   \n",
       "\n",
       "        END_TIME  EPISODE_ID  EVENT_ID          STATE STATE_FIPS  ...  \\\n",
       "0           2000       83473    503953  NEW HAMPSHIRE         33  ...   \n",
       "1            931       83971    507163  MASSACHUSETTS         25  ...   \n",
       "2           2306       83517    506236       MISSOURI         29  ...   \n",
       "3           2303       83517    506237       MISSOURI         29  ...   \n",
       "4           2100       83132    501499     WASHINGTON         53  ...   \n",
       "...          ...         ...       ...            ...        ...  ...   \n",
       "691429      1148      192532   1188957       KENTUCKY         21  ...   \n",
       "691430      1809      192530   1188234        INDIANA         18  ...   \n",
       "691431      1757      192530   1188232        INDIANA         18  ...   \n",
       "691432      1750      191388   1192879  NEW HAMPSHIRE         33  ...   \n",
       "691433       852      195694   1214246        GEORGIA         13  ...   \n",
       "\n",
       "        END_LON                                  EPISODE_NARRATIVE  \\\n",
       "0           NaN  Low pressure developing south of Long Island a...   \n",
       "1      -71.3469  A stacked low pressure system passed south and...   \n",
       "2      -92.6600  A powerful storm system and a dry line produce...   \n",
       "3      -92.6600  A powerful storm system and a dry line produce...   \n",
       "4           NaN  A strong cold front produced strong winds for ...   \n",
       "...         ...                                                ...   \n",
       "691429 -84.7200  A strong storm system moved across the Ohio an...   \n",
       "691430 -85.7364  A cold front moved into the Ohio Valley during...   \n",
       "691431 -86.7247  A cold front moved into the Ohio Valley during...   \n",
       "691432 -70.8400  A supercell thunderstorm developed across sout...   \n",
       "691433 -81.1375  Debby first developed into a tropical storm ab...   \n",
       "\n",
       "                                          EVENT_NARRATIVE DATA_SOURCE  \\\n",
       "0       Eight to twelve inches of snow fell across eas...         CSV   \n",
       "1       Boston Road was closed near Brian Road due to ...         CSV   \n",
       "2                                                     NaN         CSV   \n",
       "3       Several power poles snapped and trees blown down.         CSV   \n",
       "4       Two stations measured strong wind gusts in the...         CSV   \n",
       "...                                                   ...         ...   \n",
       "691429  A trained spotter estimated 60 mph wind gusts ...         CSV   \n",
       "691430   A tree was down at Lovers Lane and Prewitt Lane.         CSV   \n",
       "691431  A tree was reported down over Chestnut Grove R...         CSV   \n",
       "691432  A supercell thunderstorm dropped hail the size...         CSV   \n",
       "691433  A Chatham County emergency manager reported a ...         CSV   \n",
       "\n",
       "                  BEGIN_DT              END_DT               BEGIN_DT_LOC  \\\n",
       "0      2014-02-18 10:00:00 2014-02-18 20:00:00  2014-02-18 10:00:00-05:00   \n",
       "1      2014-03-30 08:31:00 2014-03-30 09:31:00  2014-03-30 08:31:00-04:00   \n",
       "2      2014-04-27 23:06:00 2014-04-27 23:06:00  2014-04-27 23:06:00-05:00   \n",
       "3      2014-04-27 23:03:00 2014-04-27 23:03:00  2014-04-27 23:03:00-05:00   \n",
       "4      2014-02-15 13:00:00 2014-02-15 21:00:00  2014-02-15 13:00:00-08:00   \n",
       "...                    ...                 ...                        ...   \n",
       "691429 2024-05-26 11:48:00 2024-05-26 11:48:00  2024-05-26 11:48:00-04:00   \n",
       "691430 2024-05-22 18:09:00 2024-05-22 18:09:00  2024-05-22 18:09:00-04:00   \n",
       "691431 2024-05-22 17:57:00 2024-05-22 17:57:00  2024-05-22 17:57:00-04:00   \n",
       "691432 2024-06-23 17:45:00 2024-06-23 17:50:00  2024-06-23 17:45:00-04:00   \n",
       "691433 2024-08-06 07:52:00 2024-08-06 08:52:00  2024-08-06 07:52:00-04:00   \n",
       "\n",
       "                       END_DT_LOC              BEGIN_DT_UTC  \\\n",
       "0       2014-02-18 20:00:00-05:00 2014-02-18 15:00:00+00:00   \n",
       "1       2014-03-30 09:31:00-04:00 2014-03-30 12:31:00+00:00   \n",
       "2       2014-04-27 23:06:00-05:00 2014-04-28 04:06:00+00:00   \n",
       "3       2014-04-27 23:03:00-05:00 2014-04-28 04:03:00+00:00   \n",
       "4       2014-02-15 21:00:00-08:00 2014-02-15 21:00:00+00:00   \n",
       "...                           ...                       ...   \n",
       "691429  2024-05-26 11:48:00-04:00 2024-05-26 15:48:00+00:00   \n",
       "691430  2024-05-22 18:09:00-04:00 2024-05-22 22:09:00+00:00   \n",
       "691431  2024-05-22 17:57:00-04:00 2024-05-22 21:57:00+00:00   \n",
       "691432  2024-06-23 17:50:00-04:00 2024-06-23 21:45:00+00:00   \n",
       "691433  2024-08-06 08:52:00-04:00 2024-08-06 11:52:00+00:00   \n",
       "\n",
       "                      END_DT_UTC  \n",
       "0      2014-02-19 01:00:00+00:00  \n",
       "1      2014-03-30 13:31:00+00:00  \n",
       "2      2014-04-28 04:06:00+00:00  \n",
       "3      2014-04-28 04:03:00+00:00  \n",
       "4      2014-02-16 05:00:00+00:00  \n",
       "...                          ...  \n",
       "691429 2024-05-26 15:48:00+00:00  \n",
       "691430 2024-05-22 22:09:00+00:00  \n",
       "691431 2024-05-22 21:57:00+00:00  \n",
       "691432 2024-06-23 21:50:00+00:00  \n",
       "691433 2024-08-06 12:52:00+00:00  \n",
       "\n",
       "[691434 rows x 57 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58423f87-7c00-4d4b-8c21-1f34b3926482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noaa.to_csv(\"NOAA_timezone_cleaned.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4e7c44-fee3-4a9b-b2c3-45befebe0cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Step 3: Feature Engineering on NOAA Data...\n",
      "Parsing DAMAGE_PROPERTY and DAMAGE_CROPS...\n",
      "Damage columns parsed.\n",
      "Calculating event duration...\n",
      "  Ensuring UTC columns are datetime type...\n",
      "Event duration calculated.\n",
      "Handling MAGNITUDE NaNs...\n",
      "  Filled 331118 NaNs in MAGNITUDE with 0.\n",
      "MAGNITUDE NaNs handled.\n",
      "Filtering NOAA data for County-level events (CZ_TYPE == 'C')...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40165/1085264281.py:60: SettingWithCopyWarning: modifications to a method of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  duration_seconds[duration_seconds < 0] = 0\n",
      "/tmp/ipykernel_40165/1085264281.py:60: SettingWithCopyWarning: modifications to a method of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  duration_seconds[duration_seconds < 0] = 0\n",
      "/tmp/ipykernel_40165/1085264281.py:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_noaa['EVENT_DURATION_HOURS'].fillna(0, inplace=True) # Fill NaN durations with 0\n",
      "/tmp/ipykernel_40165/1085264281.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_noaa['MAGNITUDE'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retained 397066 rows out of 691434 (County-level events).\n",
      "\n",
      "--- NOAA DataFrame Info after Step 3 Feature Engineering ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 397066 entries, 1 to 691433\n",
      "Data columns (total 60 columns):\n",
      " #   Column                   Non-Null Count   Dtype              \n",
      "---  ------                   --------------   -----              \n",
      " 0   BEGIN_YEARMONTH          397066 non-null  int64              \n",
      " 1   BEGIN_DAY                397066 non-null  int64              \n",
      " 2   BEGIN_TIME               397066 non-null  int64              \n",
      " 3   END_YEARMONTH            397066 non-null  int64              \n",
      " 4   END_DAY                  397066 non-null  int64              \n",
      " 5   END_TIME                 397066 non-null  int64              \n",
      " 6   EPISODE_ID               397066 non-null  int64              \n",
      " 7   EVENT_ID                 397066 non-null  int64              \n",
      " 8   STATE                    397066 non-null  object             \n",
      " 9   STATE_FIPS               397066 non-null  object             \n",
      " 10  YEAR                     397066 non-null  int64              \n",
      " 11  MONTH_NAME               397066 non-null  object             \n",
      " 12  EVENT_TYPE               397066 non-null  object             \n",
      " 13  CZ_TYPE                  397066 non-null  object             \n",
      " 14  CZ_FIPS                  397066 non-null  object             \n",
      " 15  CZ_NAME                  397066 non-null  object             \n",
      " 16  WFO                      397066 non-null  object             \n",
      " 17  BEGIN_DATE_TIME          397066 non-null  object             \n",
      " 18  CZ_TIMEZONE              397066 non-null  object             \n",
      " 19  END_DATE_TIME            397066 non-null  object             \n",
      " 20  INJURIES_DIRECT          397066 non-null  int64              \n",
      " 21  INJURIES_INDIRECT        397066 non-null  int64              \n",
      " 22  DEATHS_DIRECT            397066 non-null  int64              \n",
      " 23  DEATHS_INDIRECT          397066 non-null  int64              \n",
      " 24  DAMAGE_PROPERTY          324398 non-null  object             \n",
      " 25  DAMAGE_CROPS             323921 non-null  object             \n",
      " 26  SOURCE                   397066 non-null  object             \n",
      " 27  MAGNITUDE                397066 non-null  float64            \n",
      " 28  MAGNITUDE_TYPE           186460 non-null  object             \n",
      " 29  FLOOD_CAUSE              75222 non-null   object             \n",
      " 30  CATEGORY                 0 non-null       float64            \n",
      " 31  TOR_F_SCALE              15641 non-null   object             \n",
      " 32  TOR_LENGTH               15641 non-null   float64            \n",
      " 33  TOR_WIDTH                15641 non-null   float64            \n",
      " 34  TOR_OTHER_WFO            2043 non-null    object             \n",
      " 35  TOR_OTHER_CZ_STATE       2043 non-null    object             \n",
      " 36  TOR_OTHER_CZ_FIPS        2043 non-null    float64            \n",
      " 37  TOR_OTHER_CZ_NAME        2043 non-null    object             \n",
      " 38  BEGIN_RANGE              397015 non-null  float64            \n",
      " 39  BEGIN_AZIMUTH            397015 non-null  object             \n",
      " 40  BEGIN_LOCATION           397015 non-null  object             \n",
      " 41  END_RANGE                397015 non-null  float64            \n",
      " 42  END_AZIMUTH              397015 non-null  object             \n",
      " 43  END_LOCATION             397015 non-null  object             \n",
      " 44  BEGIN_LAT                397015 non-null  float64            \n",
      " 45  BEGIN_LON                397015 non-null  float64            \n",
      " 46  END_LAT                  397015 non-null  float64            \n",
      " 47  END_LON                  397015 non-null  float64            \n",
      " 48  EPISODE_NARRATIVE        397066 non-null  object             \n",
      " 49  EVENT_NARRATIVE          333768 non-null  object             \n",
      " 50  DATA_SOURCE              397066 non-null  object             \n",
      " 51  BEGIN_DT                 397066 non-null  datetime64[ns]     \n",
      " 52  END_DT                   397066 non-null  datetime64[ns]     \n",
      " 53  BEGIN_DT_LOC             397040 non-null  object             \n",
      " 54  END_DT_LOC               397039 non-null  object             \n",
      " 55  BEGIN_DT_UTC             397040 non-null  datetime64[ns, UTC]\n",
      " 56  END_DT_UTC               397039 non-null  datetime64[ns, UTC]\n",
      " 57  DAMAGE_PROPERTY_NUMERIC  397066 non-null  float64            \n",
      " 58  DAMAGE_CROPS_NUMERIC     397066 non-null  float64            \n",
      " 59  EVENT_DURATION_HOURS     397066 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](2), datetime64[ns](2), float64(14), int64(13), object(29)\n",
      "memory usage: 184.8+ MB\n",
      "\n",
      "--- NOAA DataFrame Head (New Features) ---\n",
      "    EVENT_ID CZ_FIPS              BEGIN_DT_UTC                END_DT_UTC  \\\n",
      "1     507163      17 2014-03-30 12:31:00+00:00 2014-03-30 13:31:00+00:00   \n",
      "2     506236      67 2014-04-28 04:06:00+00:00 2014-04-28 04:06:00+00:00   \n",
      "3     506237      67 2014-04-28 04:03:00+00:00 2014-04-28 04:03:00+00:00   \n",
      "10    506362     191 2014-05-12 03:30:00+00:00 2014-05-12 03:30:00+00:00   \n",
      "11    506238      67 2014-04-28 03:50:00+00:00 2014-04-28 03:50:00+00:00   \n",
      "\n",
      "   DAMAGE_PROPERTY  DAMAGE_PROPERTY_NUMERIC DAMAGE_CROPS  \\\n",
      "1           35.00K                  35000.0        0.00K   \n",
      "2            0.00K                      0.0        0.00K   \n",
      "3           10.00K                  10000.0        0.00K   \n",
      "10           1.00K                   1000.0        0.00K   \n",
      "11          10.00K                  10000.0        0.00K   \n",
      "\n",
      "    DAMAGE_CROPS_NUMERIC  EVENT_DURATION_HOURS  MAGNITUDE  \n",
      "1                    0.0                   1.0       0.00  \n",
      "2                    0.0                   0.0       0.88  \n",
      "3                    0.0                   0.0      61.00  \n",
      "10                   0.0                   0.0      50.00  \n",
      "11                   0.0                   0.0      65.00  \n",
      "\n",
      "Step 3 (NOAA Feature Engineering) Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re # Import regex module for damage parsing\n",
    "\n",
    "# Assume df_noaa exists from Step 1\n",
    "\n",
    "print(\"\\nStarting Step 3: Feature Engineering on NOAA Data...\")\n",
    "\n",
    "# --- 1. Parse Damage Columns ---\n",
    "print(\"Parsing DAMAGE_PROPERTY and DAMAGE_CROPS...\")\n",
    "\n",
    "def parse_damage_value(value):\n",
    "    \"\"\"Converts damage strings (e.g., '10.00K', '1.5M', '50B') to numeric.\"\"\"\n",
    "    if pd.isna(value) or value == '0.00K' or value == 0: # Handle NaNs and explicit zero\n",
    "        return 0.0\n",
    "    value_str = str(value).strip().upper()\n",
    "    # Use regex to find number and optional multiplier (K, M, B)\n",
    "    match = re.match(r'([\\d\\.]+)([KMB]?)', value_str)\n",
    "    if match:\n",
    "        number, multiplier = match.groups()\n",
    "        number = float(number)\n",
    "        if multiplier == 'K':\n",
    "            return number * 1_000\n",
    "        elif multiplier == 'M':\n",
    "            return number * 1_000_000\n",
    "        elif multiplier == 'B':\n",
    "            return number * 1_000_000_000\n",
    "        else: # No multiplier, assume direct value\n",
    "            return number\n",
    "    else:\n",
    "        # warnings.warn(f\"Could not parse damage value: {value}\", UserWarning)\n",
    "        return 0.0 # Return 0 if pattern doesn't match\n",
    "\n",
    "# Apply the function\n",
    "if 'DAMAGE_PROPERTY' in df_noaa.columns:\n",
    "    df_noaa['DAMAGE_PROPERTY_NUMERIC'] = df_noaa['DAMAGE_PROPERTY'].apply(parse_damage_value)\n",
    "else:\n",
    "    print(\"Warning: DAMAGE_PROPERTY column not found.\")\n",
    "    df_noaa['DAMAGE_PROPERTY_NUMERIC'] = 0.0\n",
    "\n",
    "if 'DAMAGE_CROPS' in df_noaa.columns:\n",
    "    df_noaa['DAMAGE_CROPS_NUMERIC'] = df_noaa['DAMAGE_CROPS'].apply(parse_damage_value)\n",
    "else:\n",
    "    print(\"Warning: DAMAGE_CROPS column not found.\")\n",
    "    df_noaa['DAMAGE_CROPS_NUMERIC'] = 0.0\n",
    "\n",
    "print(\"Damage columns parsed.\")\n",
    "\n",
    "\n",
    "# --- 2. Calculate Event Duration ---\n",
    "print(\"Calculating event duration...\")\n",
    "if 'BEGIN_DT_UTC' in df_noaa.columns and 'END_DT_UTC' in df_noaa.columns:\n",
    "    # Calculate duration in seconds, then convert to hours\n",
    "    print(\"  Ensuring UTC columns are datetime type...\")\n",
    "    df_noaa['BEGIN_DT_UTC'] = pd.to_datetime(df_noaa['BEGIN_DT_UTC'])\n",
    "    df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC'])\n",
    "    duration_seconds = (df_noaa['END_DT_UTC'] - df_noaa['BEGIN_DT_UTC']).dt.total_seconds()\n",
    "    # Handle potential negative durations (if end < begin) by setting them to 0 or NaN\n",
    "    duration_seconds[duration_seconds < 0] = 0\n",
    "    df_noaa['EVENT_DURATION_HOURS'] = duration_seconds / 3600.0\n",
    "    df_noaa['EVENT_DURATION_HOURS'].fillna(0, inplace=True) # Fill NaN durations with 0\n",
    "else:\n",
    "    print(\"Warning: BEGIN_DT_UTC or END_DT_UTC columns not found. Cannot calculate duration.\")\n",
    "    df_noaa['EVENT_DURATION_HOURS'] = 0.0\n",
    "print(\"Event duration calculated.\")\n",
    "\n",
    "\n",
    "# --- 3. Handle Magnitude NaN ---\n",
    "print(\"Handling MAGNITUDE NaNs...\")\n",
    "if 'MAGNITUDE' in df_noaa.columns:\n",
    "    # Impute with 0 based on previous discussion (NaN often means not applicable)\n",
    "    nan_before = df_noaa['MAGNITUDE'].isna().sum()\n",
    "    df_noaa['MAGNITUDE'].fillna(0, inplace=True)\n",
    "    print(f\"  Filled {nan_before} NaNs in MAGNITUDE with 0.\")\n",
    "else:\n",
    "    print(\"Warning: MAGNITUDE column not found.\")\n",
    "print(\"MAGNITUDE NaNs handled.\")\n",
    "\n",
    "\n",
    "# --- 4. Filter by CZ_TYPE (Recommended) ---\n",
    "print(\"Filtering NOAA data for County-level events (CZ_TYPE == 'C')...\")\n",
    "if 'CZ_TYPE' in df_noaa.columns:\n",
    "    rows_before = len(df_noaa)\n",
    "    df_noaa = df_noaa[df_noaa['CZ_TYPE'] == 'C'].copy() # Filter and create a copy\n",
    "    rows_after = len(df_noaa)\n",
    "    print(f\"  Retained {rows_after} rows out of {rows_before} (County-level events).\")\n",
    "else:\n",
    "    print(\"Warning: CZ_TYPE column not found. Cannot filter by county-level events.\")\n",
    "\n",
    "\n",
    "# --- Display Info and Head ---\n",
    "print(\"\\n--- NOAA DataFrame Info after Step 3 Feature Engineering ---\")\n",
    "df_noaa.info(verbose=True, show_counts=True)\n",
    "\n",
    "print(\"\\n--- NOAA DataFrame Head (New Features) ---\")\n",
    "print(df_noaa[[\n",
    "    'EVENT_ID', 'CZ_FIPS', 'BEGIN_DT_UTC', 'END_DT_UTC',\n",
    "    'DAMAGE_PROPERTY', 'DAMAGE_PROPERTY_NUMERIC',\n",
    "    'DAMAGE_CROPS', 'DAMAGE_CROPS_NUMERIC',\n",
    "    'EVENT_DURATION_HOURS', 'MAGNITUDE'\n",
    "]].head())\n",
    "\n",
    "print(\"\\nStep 3 (NOAA Feature Engineering) Complete.\")\n",
    "# df_noaa now has numeric damage columns, event duration, imputed magnitude,\n",
    "# and is potentially filtered to only county-level events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f320b07b-0d2e-4365-9a90-181c51842a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Eaglei outage files:\n",
      "  - eaglei_outages_2014.csv\n",
      "  - eaglei_outages_2015.csv\n",
      "  - eaglei_outages_2016.csv\n",
      "  - ... and others\n",
      "\n",
      "Loading and concatenating Eaglei data...\n",
      "  Loaded eaglei_outages_2014.csv\n",
      "  Loaded eaglei_outages_2015.csv\n",
      "  Loaded eaglei_outages_2016.csv\n",
      "  Loaded eaglei_outages_2017.csv\n",
      "  Loaded eaglei_outages_2018.csv\n",
      "  Loaded eaglei_outages_2019.csv\n",
      "  Loaded eaglei_outages_2020.csv\n",
      "  Loaded eaglei_outages_2021.csv\n",
      "  Loaded eaglei_outages_2022.csv\n",
      "  Loaded eaglei_outages_2023.csv\n",
      "Eaglei data concatenated successfully.\n",
      "\n",
      "Performing initial cleaning and type conversions on Eaglei data...\n",
      "Converting 'run_start_time' (known UTC) using format: %Y-%m-%d %H:%M:%S\n",
      "Assigning UTC timezone to parsed 'run_start_time'...\n",
      "Formatting 'fips_code'...\n",
      "Checking 'customers_out' type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40165/1915395210.py:96: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_eaglei['customers_out'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Filled 6763505 NaN values in 'customers_out' with 0.\n",
      "\n",
      "--- Eaglei DataFrame Info after initial cleaning ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191133068 entries, 0 to 191133067\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count      Dtype              \n",
      "---  ------          --------------      -----              \n",
      " 0   fips_code       191133068 non-null  object             \n",
      " 1   county          191133068 non-null  object             \n",
      " 2   state           191133068 non-null  object             \n",
      " 3   customers_out   191133068 non-null  float64            \n",
      " 4   run_start_time  191133068 non-null  object             \n",
      " 5   EAGLEI_DT_UTC   191133068 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(4)\n",
      "memory usage: 8.5+ GB\n",
      "\n",
      "--- Eaglei DataFrame Head (Cleaned) ---\n",
      "  fips_code       run_start_time             EAGLEI_DT_UTC  customers_out\n",
      "0     01037  2014-11-01 04:00:00 2014-11-01 04:00:00+00:00           12.0\n",
      "1     01051  2014-11-01 04:00:00 2014-11-01 04:00:00+00:00            7.0\n",
      "2     01109  2014-11-01 04:00:00 2014-11-01 04:00:00+00:00            1.0\n",
      "3     01121  2014-11-01 04:00:00 2014-11-01 04:00:00+00:00           31.0\n",
      "4     04017  2014-11-01 04:00:00 2014-11-01 04:00:00+00:00            1.0\n",
      "\n",
      "--- Data Types of Final Eaglei Columns ---\n",
      "fips_code                     object\n",
      "EAGLEI_DT_UTC    datetime64[ns, UTC]\n",
      "customers_out                float64\n",
      "dtype: object\n",
      "\n",
      "Step 2 (Eaglei Load/Clean) Complete - Processed as UTC.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob # To find the files\n",
    "import os\n",
    "\n",
    "# Define the directory containing the Eaglei data\n",
    "eaglei_dir = 'data/eaglei_data/'\n",
    "\n",
    "# Find all eaglei outage CSV files\n",
    "eaglei_files = glob.glob(os.path.join(eaglei_dir, 'eaglei_outages_*.csv'))\n",
    "\n",
    "if not eaglei_files:\n",
    "    print(f\"Error: No 'eaglei_outages_*.csv' files found in {eaglei_dir}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"Found {len(eaglei_files)} Eaglei outage files:\")\n",
    "    eaglei_files.sort()\n",
    "    for f in eaglei_files[:3]: print(f\"  - {os.path.basename(f)}\")\n",
    "    if len(eaglei_files) > 3: print(\"  - ... and others\")\n",
    "\n",
    "\n",
    "# --- Load and Concatenate Data ---\n",
    "print(\"\\nLoading and concatenating Eaglei data...\")\n",
    "list_of_dfs = []\n",
    "for f in eaglei_files:\n",
    "    try:\n",
    "        df_temp = pd.read_csv(f)\n",
    "        list_of_dfs.append(df_temp)\n",
    "        print(f\"  Loaded {os.path.basename(f)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Warning: File {f} not found during loading loop.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {os.path.basename(f)}: {e}\")\n",
    "\n",
    "if not list_of_dfs:\n",
    "    print(\"Error: Failed to load any Eaglei data.\")\n",
    "    exit()\n",
    "\n",
    "df_eaglei = pd.concat(list_of_dfs, ignore_index=True)\n",
    "print(\"Eaglei data concatenated successfully.\")\n",
    "\n",
    "\n",
    "# --- Initial Cleaning and Type Conversions ---\n",
    "print(\"\\nPerforming initial cleaning and type conversions on Eaglei data...\")\n",
    "\n",
    "# Datetime Conversion (Knowing Input is UTC)\n",
    "eaglei_datetime_format = '%Y-%m-%d %H:%M:%S'\n",
    "print(f\"Converting 'run_start_time' (known UTC) using format: {eaglei_datetime_format}\")\n",
    "\n",
    "# Parse the datetime string. Initially, this creates a naive datetime object\n",
    "# because the string format itself doesn't contain timezone info.\n",
    "df_eaglei['run_start_time_parsed'] = pd.to_datetime(df_eaglei['run_start_time'], format=eaglei_datetime_format, errors='coerce')\n",
    "\n",
    "# Check for parsing errors\n",
    "naive_nat_count = df_eaglei['run_start_time_parsed'].isna().sum()\n",
    "if naive_nat_count > 0:\n",
    "    print(f\"Warning: Found {naive_nat_count} NaT values after parsing 'run_start_time'.\")\n",
    "    # Optional: drop rows with invalid dates if needed\n",
    "    # df_eaglei.dropna(subset=['run_start_time_parsed'], inplace=True)\n",
    "\n",
    "# *** Assign UTC timezone ***\n",
    "# Since we know the original times represent UTC moments, we *localize* the naive\n",
    "# datetime objects to UTC. This tells pandas these times are UTC.\n",
    "print(\"Assigning UTC timezone to parsed 'run_start_time'...\")\n",
    "try:\n",
    "    df_eaglei['EAGLEI_DT_UTC'] = df_eaglei['run_start_time_parsed'].dt.tz_localize('UTC')\n",
    "except AttributeError as e:\n",
    "     # Fallback in case parsing resulted in 'object' dtype\n",
    "     print(f\"Error: Failed using .dt accessor. Attempting direct conversion specifying UTC...\")\n",
    "     df_eaglei['EAGLEI_DT_UTC'] = pd.to_datetime(df_eaglei['run_start_time_parsed'], errors='coerce', utc=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error during UTC assignment: {e}\")\n",
    "    df_eaglei['EAGLEI_DT_UTC'] = pd.NaT\n",
    "\n",
    "utc_nat_count = df_eaglei['EAGLEI_DT_UTC'].isna().sum()\n",
    "if utc_nat_count > naive_nat_count:\n",
    "     print(f\"Warning: {utc_nat_count - naive_nat_count} additional NaTs introduced during UTC assignment.\")\n",
    "\n",
    "# Can drop the intermediate parsed column now\n",
    "df_eaglei.drop(columns=['run_start_time_parsed'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# FIPS Code Formatting\n",
    "print(\"Formatting 'fips_code'...\")\n",
    "if 'fips_code' in df_eaglei.columns:\n",
    "    df_eaglei['fips_code'] = df_eaglei['fips_code'].astype(str)\n",
    "    df_eaglei['fips_code'] = df_eaglei['fips_code'].str.zfill(5)\n",
    "else:\n",
    "    print(\"Warning: 'fips_code' column not found in Eaglei data!\")\n",
    "\n",
    "\n",
    "# Customers Out Type Check\n",
    "print(\"Checking 'customers_out' type...\")\n",
    "if 'customers_out' in df_eaglei.columns:\n",
    "    df_eaglei['customers_out'] = pd.to_numeric(df_eaglei['customers_out'], errors='coerce')\n",
    "    customers_out_nan_before = df_eaglei['customers_out'].isna().sum()\n",
    "    df_eaglei['customers_out'].fillna(0, inplace=True)\n",
    "    customers_out_nan_after = df_eaglei['customers_out'].isna().sum()\n",
    "    if customers_out_nan_before > customers_out_nan_after:\n",
    "        print(f\"  Filled {customers_out_nan_before - customers_out_nan_after} NaN values in 'customers_out' with 0.\")\n",
    "    # Optionally convert to integer if appropriate\n",
    "    # df_eaglei['customers_out'] = df_eaglei['customers_out'].astype(int)\n",
    "else:\n",
    "    print(\"Warning: 'customers_out' column not found in Eaglei data!\")\n",
    "\n",
    "\n",
    "# --- Display Info and Head ---\n",
    "print(\"\\n--- Eaglei DataFrame Info after initial cleaning ---\")\n",
    "df_eaglei.info(verbose=True, show_counts=True)\n",
    "\n",
    "print(\"\\n--- Eaglei DataFrame Head (Cleaned) ---\")\n",
    "print(df_eaglei[['fips_code', 'run_start_time', 'EAGLEI_DT_UTC', 'customers_out']].head())\n",
    "\n",
    "print(\"\\n--- Data Types of Final Eaglei Columns ---\")\n",
    "print(df_eaglei[['fips_code', 'EAGLEI_DT_UTC', 'customers_out']].dtypes)\n",
    "\n",
    "print(\"\\nStep 2 (Eaglei Load/Clean) Complete - Processed as UTC.\")\n",
    "# df_eaglei now contains the loaded and cleaned Eaglei data\n",
    "# Key columns: fips_code (string, padded), EAGLEI_DT_UTC (datetime64[ns, UTC]), customers_out (numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffaf1bd9-8ea0-499f-ba7c-0ed4e112decf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips_code</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>customers_out</th>\n",
       "      <th>run_start_time</th>\n",
       "      <th>run_start_time_naive</th>\n",
       "      <th>EAGLEI_DT_UTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01037</td>\n",
       "      <td>Coosa</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01051</td>\n",
       "      <td>Elmore</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01109</td>\n",
       "      <td>Pike</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01121</td>\n",
       "      <td>Talladega</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04017</td>\n",
       "      <td>Navajo</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2014-11-01 04:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133063</th>\n",
       "      <td>55095</td>\n",
       "      <td>Polk</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133064</th>\n",
       "      <td>55105</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133065</th>\n",
       "      <td>55109</td>\n",
       "      <td>St. Croix</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133066</th>\n",
       "      <td>55129</td>\n",
       "      <td>Washburn</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133067</th>\n",
       "      <td>56039</td>\n",
       "      <td>Teton</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>2023-12-31 23:45:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191133068 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fips_code     county      state  customers_out       run_start_time  \\\n",
       "0             01037      Coosa    Alabama           12.0  2014-11-01 04:00:00   \n",
       "1             01051     Elmore    Alabama            7.0  2014-11-01 04:00:00   \n",
       "2             01109       Pike    Alabama            1.0  2014-11-01 04:00:00   \n",
       "3             01121  Talladega    Alabama           31.0  2014-11-01 04:00:00   \n",
       "4             04017     Navajo    Arizona            1.0  2014-11-01 04:00:00   \n",
       "...             ...        ...        ...            ...                  ...   \n",
       "191133063     55095       Polk  Wisconsin            0.0  2023-12-31 23:45:00   \n",
       "191133064     55105       Rock  Wisconsin            1.0  2023-12-31 23:45:00   \n",
       "191133065     55109  St. Croix  Wisconsin            0.0  2023-12-31 23:45:00   \n",
       "191133066     55129   Washburn  Wisconsin            0.0  2023-12-31 23:45:00   \n",
       "191133067     56039      Teton    Wyoming            2.0  2023-12-31 23:45:00   \n",
       "\n",
       "          run_start_time_naive             EAGLEI_DT_UTC  \n",
       "0          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "1          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "2          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "3          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "4          2014-11-01 04:00:00 2014-11-01 04:00:00+00:00  \n",
       "...                        ...                       ...  \n",
       "191133063  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133064  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133065  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133066  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "191133067  2023-12-31 23:45:00 2023-12-31 23:45:00+00:00  \n",
       "\n",
       "[191133068 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eaglei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df24cac-23e3-4be3-87ad-8597511a17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eaglei.to_pickle(\"eaglei_2014_2024.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e87ec6-1bec-4b91-93ec-e19a9e4fb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coosa' 'Elmore' 'Pike' ... 'Taos' 'Toole' 'Prentiss']\n"
     ]
    }
   ],
   "source": [
    "df_eaglei = pd.read_csv(\"eaglei_2014_2024.csv\")\n",
    "unq = df_noaa['CZ_NAME'].unique()\n",
    "unq1 = df_eaglei['county'].unique()\n",
    "print(unq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e626cd-2f38-468e-90d7-43124fb73581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference count: 220\n",
      "Unique to NOAA: 176\n",
      "Unique to EagleI: 44\n",
      "\n",
      "Sample unique to NOAA: ['ALASKA PENINSULA', 'VIEQUES', 'QUEBRADILLAS']\n",
      "Sample unique to EagleI: ['CARSON CITY', 'KALAWAO', 'COLONIAL HEIGHTS']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values (with NaN handling)\n",
    "noaa_counties = set(df_noaa['CZ_NAME'].dropna().str.upper().unique())\n",
    "eaglei_counties = set(df_eaglei['county'].dropna().str.upper().unique())\n",
    "\n",
    "# Check equality\n",
    "if noaa_counties == eaglei_counties:\n",
    "    print(\"Both datasets contain EXACTLY the same county names\")\n",
    "else:\n",
    "    # Calculate differences\n",
    "    only_in_noaa = noaa_counties - eaglei_counties\n",
    "    only_in_eaglei = eaglei_counties - noaa_counties\n",
    "    \n",
    "    print(f\"Difference count: {len(only_in_noaa) + len(only_in_eaglei)}\")\n",
    "    print(f\"Unique to NOAA: {len(only_in_noaa)}\")\n",
    "    print(f\"Unique to EagleI: {len(only_in_eaglei)}\")\n",
    "    \n",
    "    # Optional: Show examples of mismatches\n",
    "    print(\"\\nSample unique to NOAA:\", list(only_in_noaa)[:3])\n",
    "    print(\"Sample unique to EagleI:\", list(only_in_eaglei)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3de1bc-6437-494d-9cbc-d8768b82e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noaa.to_pickle('noaa_feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723577e3-bc2e-4740-8c6c-40a0f92f561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_noaa = pd.read_csv(\"noaa_minimal_feature_eng.csv\")\n",
    "df_eaglei = pd.read_csv(\"eaglei_2014_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d89c72-62cb-4c1e-83ab-88aa0c5cf0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    01037\n",
       "1    01051\n",
       "Name: fips_code, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eaglei['fips_code'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68209d86-ae4e-4c4f-aeb5-307c6a7f91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for non-numeric values in STATE_FIPS and CZ_FIPS...\n",
      "Rows with non-numeric STATE_FIPS: 0\n",
      "\n",
      "Rows with non-numeric CZ_FIPS: 0\n",
      "\n",
      "Rows with non-numeric fp_FIPS: 0\n"
     ]
    }
   ],
   "source": [
    "# --- Check for non-numeric values in original STATE_FIPS and CZ_FIPS ---\n",
    "print(\"\\nChecking for non-numeric values in STATE_FIPS and CZ_FIPS...\")\n",
    "\n",
    "# Check STATE_FIPS\n",
    "non_numeric_state = df_noaa[~df_noaa['STATE_FIPS'].apply(lambda x: str(x).replace('.', '', 1).isdigit())]\n",
    "print(f\"Rows with non-numeric STATE_FIPS: {len(non_numeric_state)}\")\n",
    "if not non_numeric_state.empty:\n",
    "    print(\"Examples of invalid STATE_FIPS:\")\n",
    "    print(non_numeric_state[['STATE_FIPS']].head())\n",
    "\n",
    "# Check CZ_FIPS\n",
    "non_numeric_cz = df_noaa[~df_noaa['CZ_FIPS'].apply(lambda x: str(x).replace('.', '', 1).isdigit())]\n",
    "print(f\"\\nRows with non-numeric CZ_FIPS: {len(non_numeric_cz)}\")\n",
    "if not non_numeric_cz.empty:\n",
    "    print(\"Examples of invalid CZ_FIPS:\")\n",
    "    print(non_numeric_cz[['CZ_FIPS']].head())\n",
    "non_numeric_fp = df_eaglei[~df_eaglei['fips_code'].apply(lambda x: str(x).replace('.', '', 1).isdigit())]\n",
    "print(f\"\\nRows with non-numeric fp_FIPS: {len(non_numeric_fp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e298437-98c4-468e-9576-c353396b70e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Step 4: Merging NOAA and Eaglei Data...\n",
      "Creating 5-digit FIPS code in NOAA data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35761/561022568.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['25017' '29067' '29067' ... '18123' '33015' '13051']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_noaa.loc[valid_fips_mask, 'full_fips_code'] = state_fips_padded[valid_fips_mask] + cz_fips_padded[valid_fips_mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'full_fips_code' created.\n",
      "  Example NOAA full_fips_code: 25017\n",
      "  Example Eaglei fips_code: 01037\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Assume df_noaa exists from Step 3\n",
    "# Assume df_eaglei exists from Step 2\n",
    "df_noaa = pd.read_pickle('noaa_feature.pkl')\n",
    "df_eaglei = pd.read_pickle('eaglei_2014_2024.pkl')\n",
    "\n",
    "print(\"\\nStarting Step 4: Merging NOAA and Eaglei Data...\")\n",
    "\n",
    "# --- 1. Create full 5-digit FIPS code in df_noaa ---\n",
    "print(\"Creating 5-digit FIPS code in NOAA data...\")\n",
    "if 'STATE_FIPS' in df_noaa.columns and 'CZ_FIPS' in df_noaa.columns:\n",
    "    # Ensure source columns are strings and handle potential floats/NaNs gracefully first\n",
    "    df_noaa['STATE_FIPS_STR'] = df_noaa['STATE_FIPS'].astype(str).str.split('.').str[0]\n",
    "    df_noaa['CZ_FIPS_STR'] = df_noaa['CZ_FIPS'].astype(str).str.split('.').str[0]\n",
    "\n",
    "    # Pad STATE_FIPS to 2 digits, CZ_FIPS to 3 digits\n",
    "    state_fips_padded = df_noaa['STATE_FIPS_STR'].str.zfill(2)\n",
    "    cz_fips_padded = df_noaa['CZ_FIPS_STR'].str.zfill(3)\n",
    "\n",
    "    # Concatenate, but only if both parts are valid (not NaN after conversion)\n",
    "    # Create mask for valid rows\n",
    "    valid_fips_mask = state_fips_padded.notna() & cz_fips_padded.notna()\n",
    "    df_noaa['full_fips_code'] = np.nan # Initialize with NaN\n",
    "    df_noaa.loc[valid_fips_mask, 'full_fips_code'] = state_fips_padded[valid_fips_mask] + cz_fips_padded[valid_fips_mask]\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df_noaa.drop(columns=['STATE_FIPS_STR', 'CZ_FIPS_STR'], inplace=True)\n",
    "\n",
    "    # Check how many NaNs were created\n",
    "    fips_nan_count = df_noaa['full_fips_code'].isna().sum()\n",
    "    if fips_nan_count > 0:\n",
    "        print(f\"  Warning: Created {fips_nan_count} NaN values in 'full_fips_code' due to missing STATE or CZ FIPS.\")\n",
    "        # Optional: Drop rows with missing FIPS before merging if needed\n",
    "        # df_noaa.dropna(subset=['full_fips_code'], inplace=True)\n",
    "\n",
    "    print(\"  'full_fips_code' created.\")\n",
    "    # Verify one FIPS code\n",
    "    print(f\"  Example NOAA full_fips_code: {df_noaa['full_fips_code'].iloc[0] if not df_noaa.empty else 'N/A'}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: STATE_FIPS or CZ_FIPS missing from df_noaa. Cannot create full FIPS code.\")\n",
    "    # Handle error - cannot proceed with merge\n",
    "    exit()\n",
    "\n",
    "# Verify Eaglei FIPS code formatting (should be done already)\n",
    "print(f\"  Example Eaglei fips_code: {df_eaglei['fips_code'].iloc[0] if not df_eaglei.empty else 'N/A'}\")\n",
    "\n",
    "df_noaa.to_pickle(\"noaa_fips.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34637e92-9ea5-4dce-afbd-b1e010ba8805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Eaglei subset with 191133068 rows.\n",
      "Merge tolerance set to: 0 days 06:00:00\n",
      "datetime64[ns, UTC]\n",
      "object\n",
      "datetime64[ns, UTC]\n",
      "object\n",
      "Sorting DataFrames (this might take time)...\n",
      "DataFrames sorted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>BEGIN_DT</th>\n",
       "      <th>END_DT</th>\n",
       "      <th>BEGIN_DT_LOC</th>\n",
       "      <th>END_DT_LOC</th>\n",
       "      <th>BEGIN_DT_UTC</th>\n",
       "      <th>END_DT_UTC</th>\n",
       "      <th>DAMAGE_PROPERTY_NUMERIC</th>\n",
       "      <th>DAMAGE_CROPS_NUMERIC</th>\n",
       "      <th>EVENT_DURATION_HOURS</th>\n",
       "      <th>full_fips_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44383</th>\n",
       "      <td>201401</td>\n",
       "      <td>11</td>\n",
       "      <td>615</td>\n",
       "      <td>201401</td>\n",
       "      <td>11</td>\n",
       "      <td>615</td>\n",
       "      <td>81761</td>\n",
       "      <td>494246</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-01-11 06:15:00</td>\n",
       "      <td>2014-01-11 06:15:00</td>\n",
       "      <td>2014-01-11 06:15:00-06:00</td>\n",
       "      <td>2014-01-11 06:15:00-06:00</td>\n",
       "      <td>2014-01-11 12:15:00+00:00</td>\n",
       "      <td>2014-01-11 12:15:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>01001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44384</th>\n",
       "      <td>201401</td>\n",
       "      <td>11</td>\n",
       "      <td>620</td>\n",
       "      <td>201401</td>\n",
       "      <td>11</td>\n",
       "      <td>620</td>\n",
       "      <td>81761</td>\n",
       "      <td>494247</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-01-11 06:20:00</td>\n",
       "      <td>2014-01-11 06:20:00</td>\n",
       "      <td>2014-01-11 06:20:00-06:00</td>\n",
       "      <td>2014-01-11 06:20:00-06:00</td>\n",
       "      <td>2014-01-11 12:20:00+00:00</td>\n",
       "      <td>2014-01-11 12:20:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>01001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17465</th>\n",
       "      <td>201404</td>\n",
       "      <td>7</td>\n",
       "      <td>715</td>\n",
       "      <td>201404</td>\n",
       "      <td>7</td>\n",
       "      <td>1335</td>\n",
       "      <td>85674</td>\n",
       "      <td>521252</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-07 07:15:00</td>\n",
       "      <td>2014-04-07 13:35:00</td>\n",
       "      <td>2014-04-07 07:15:00-05:00</td>\n",
       "      <td>2014-04-07 13:35:00-05:00</td>\n",
       "      <td>2014-04-07 12:15:00+00:00</td>\n",
       "      <td>2014-04-07 18:35:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>01001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18130</th>\n",
       "      <td>201404</td>\n",
       "      <td>7</td>\n",
       "      <td>900</td>\n",
       "      <td>201404</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>85674</td>\n",
       "      <td>524756</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-07 09:00:00</td>\n",
       "      <td>2014-04-08 14:00:00</td>\n",
       "      <td>2014-04-07 09:00:00-05:00</td>\n",
       "      <td>2014-04-08 14:00:00-05:00</td>\n",
       "      <td>2014-04-07 14:00:00+00:00</td>\n",
       "      <td>2014-04-08 19:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>01001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29615</th>\n",
       "      <td>201404</td>\n",
       "      <td>30</td>\n",
       "      <td>230</td>\n",
       "      <td>201404</td>\n",
       "      <td>30</td>\n",
       "      <td>230</td>\n",
       "      <td>83782</td>\n",
       "      <td>523313</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-30 02:30:00</td>\n",
       "      <td>2014-04-30 02:30:00</td>\n",
       "      <td>2014-04-30 02:30:00-05:00</td>\n",
       "      <td>2014-04-30 02:30:00-05:00</td>\n",
       "      <td>2014-04-30 07:30:00+00:00</td>\n",
       "      <td>2014-04-30 07:30:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>01001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635761</th>\n",
       "      <td>202405</td>\n",
       "      <td>7</td>\n",
       "      <td>1711</td>\n",
       "      <td>202405</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>192139</td>\n",
       "      <td>1184437</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-05-07 17:11:00</td>\n",
       "      <td>2024-05-07 20:15:00</td>\n",
       "      <td>2024-05-07 17:11:00-04:00</td>\n",
       "      <td>2024-05-07 20:15:00-04:00</td>\n",
       "      <td>2024-05-07 21:11:00+00:00</td>\n",
       "      <td>2024-05-08 00:15:00+00:00</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>99153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630167</th>\n",
       "      <td>202405</td>\n",
       "      <td>8</td>\n",
       "      <td>1410</td>\n",
       "      <td>202405</td>\n",
       "      <td>8</td>\n",
       "      <td>1800</td>\n",
       "      <td>192139</td>\n",
       "      <td>1184610</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-05-08 14:10:00</td>\n",
       "      <td>2024-05-08 18:00:00</td>\n",
       "      <td>2024-05-08 14:10:00-04:00</td>\n",
       "      <td>2024-05-08 18:00:00-04:00</td>\n",
       "      <td>2024-05-08 18:10:00+00:00</td>\n",
       "      <td>2024-05-08 22:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>99153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630182</th>\n",
       "      <td>202405</td>\n",
       "      <td>8</td>\n",
       "      <td>1450</td>\n",
       "      <td>202405</td>\n",
       "      <td>8</td>\n",
       "      <td>1800</td>\n",
       "      <td>192139</td>\n",
       "      <td>1184611</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-05-08 14:50:00</td>\n",
       "      <td>2024-05-08 18:00:00</td>\n",
       "      <td>2024-05-08 14:50:00-04:00</td>\n",
       "      <td>2024-05-08 18:00:00-04:00</td>\n",
       "      <td>2024-05-08 18:50:00+00:00</td>\n",
       "      <td>2024-05-08 22:00:00+00:00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>99153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630446</th>\n",
       "      <td>202405</td>\n",
       "      <td>8</td>\n",
       "      <td>1406</td>\n",
       "      <td>202405</td>\n",
       "      <td>8</td>\n",
       "      <td>1800</td>\n",
       "      <td>192139</td>\n",
       "      <td>1184609</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-05-08 14:06:00</td>\n",
       "      <td>2024-05-08 18:00:00</td>\n",
       "      <td>2024-05-08 14:06:00-04:00</td>\n",
       "      <td>2024-05-08 18:00:00-04:00</td>\n",
       "      <td>2024-05-08 18:06:00+00:00</td>\n",
       "      <td>2024-05-08 22:00:00+00:00</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>99153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672925</th>\n",
       "      <td>202408</td>\n",
       "      <td>14</td>\n",
       "      <td>930</td>\n",
       "      <td>202408</td>\n",
       "      <td>14</td>\n",
       "      <td>1600</td>\n",
       "      <td>195333</td>\n",
       "      <td>1214191</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-14 09:30:00</td>\n",
       "      <td>2024-08-14 16:00:00</td>\n",
       "      <td>2024-08-14 09:30:00-04:00</td>\n",
       "      <td>2024-08-14 16:00:00-04:00</td>\n",
       "      <td>2024-08-14 13:30:00+00:00</td>\n",
       "      <td>2024-08-14 20:00:00+00:00</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>99153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397039 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  \\\n",
       "44383            201401         11         615         201401       11   \n",
       "44384            201401         11         620         201401       11   \n",
       "17465            201404          7         715         201404        7   \n",
       "18130            201404          7         900         201404        8   \n",
       "29615            201404         30         230         201404       30   \n",
       "...                 ...        ...         ...            ...      ...   \n",
       "635761           202405          7        1711         202405        7   \n",
       "630167           202405          8        1410         202405        8   \n",
       "630182           202405          8        1450         202405        8   \n",
       "630446           202405          8        1406         202405        8   \n",
       "672925           202408         14         930         202408       14   \n",
       "\n",
       "        END_TIME  EPISODE_ID  EVENT_ID        STATE STATE_FIPS  ...  \\\n",
       "44383        615       81761    494246      ALABAMA          1  ...   \n",
       "44384        620       81761    494247      ALABAMA          1  ...   \n",
       "17465       1335       85674    521252      ALABAMA          1  ...   \n",
       "18130       1400       85674    524756      ALABAMA          1  ...   \n",
       "29615        230       83782    523313      ALABAMA          1  ...   \n",
       "...          ...         ...       ...          ...        ...  ...   \n",
       "635761      2015      192139   1184437  PUERTO RICO         99  ...   \n",
       "630167      1800      192139   1184610  PUERTO RICO         99  ...   \n",
       "630182      1800      192139   1184611  PUERTO RICO         99  ...   \n",
       "630446      1800      192139   1184609  PUERTO RICO         99  ...   \n",
       "672925      1600      195333   1214191  PUERTO RICO         99  ...   \n",
       "\n",
       "                  BEGIN_DT              END_DT               BEGIN_DT_LOC  \\\n",
       "44383  2014-01-11 06:15:00 2014-01-11 06:15:00  2014-01-11 06:15:00-06:00   \n",
       "44384  2014-01-11 06:20:00 2014-01-11 06:20:00  2014-01-11 06:20:00-06:00   \n",
       "17465  2014-04-07 07:15:00 2014-04-07 13:35:00  2014-04-07 07:15:00-05:00   \n",
       "18130  2014-04-07 09:00:00 2014-04-08 14:00:00  2014-04-07 09:00:00-05:00   \n",
       "29615  2014-04-30 02:30:00 2014-04-30 02:30:00  2014-04-30 02:30:00-05:00   \n",
       "...                    ...                 ...                        ...   \n",
       "635761 2024-05-07 17:11:00 2024-05-07 20:15:00  2024-05-07 17:11:00-04:00   \n",
       "630167 2024-05-08 14:10:00 2024-05-08 18:00:00  2024-05-08 14:10:00-04:00   \n",
       "630182 2024-05-08 14:50:00 2024-05-08 18:00:00  2024-05-08 14:50:00-04:00   \n",
       "630446 2024-05-08 14:06:00 2024-05-08 18:00:00  2024-05-08 14:06:00-04:00   \n",
       "672925 2024-08-14 09:30:00 2024-08-14 16:00:00  2024-08-14 09:30:00-04:00   \n",
       "\n",
       "                       END_DT_LOC              BEGIN_DT_UTC  \\\n",
       "44383   2014-01-11 06:15:00-06:00 2014-01-11 12:15:00+00:00   \n",
       "44384   2014-01-11 06:20:00-06:00 2014-01-11 12:20:00+00:00   \n",
       "17465   2014-04-07 13:35:00-05:00 2014-04-07 12:15:00+00:00   \n",
       "18130   2014-04-08 14:00:00-05:00 2014-04-07 14:00:00+00:00   \n",
       "29615   2014-04-30 02:30:00-05:00 2014-04-30 07:30:00+00:00   \n",
       "...                           ...                       ...   \n",
       "635761  2024-05-07 20:15:00-04:00 2024-05-07 21:11:00+00:00   \n",
       "630167  2024-05-08 18:00:00-04:00 2024-05-08 18:10:00+00:00   \n",
       "630182  2024-05-08 18:00:00-04:00 2024-05-08 18:50:00+00:00   \n",
       "630446  2024-05-08 18:00:00-04:00 2024-05-08 18:06:00+00:00   \n",
       "672925  2024-08-14 16:00:00-04:00 2024-08-14 13:30:00+00:00   \n",
       "\n",
       "                      END_DT_UTC DAMAGE_PROPERTY_NUMERIC DAMAGE_CROPS_NUMERIC  \\\n",
       "44383  2014-01-11 12:15:00+00:00                     0.0                  0.0   \n",
       "44384  2014-01-11 12:20:00+00:00                     0.0                  0.0   \n",
       "17465  2014-04-07 18:35:00+00:00                     0.0                  0.0   \n",
       "18130  2014-04-08 19:00:00+00:00                     0.0                  0.0   \n",
       "29615  2014-04-30 07:30:00+00:00                     0.0                  0.0   \n",
       "...                          ...                     ...                  ...   \n",
       "635761 2024-05-08 00:15:00+00:00                  3000.0                  0.0   \n",
       "630167 2024-05-08 22:00:00+00:00                     0.0                  0.0   \n",
       "630182 2024-05-08 22:00:00+00:00                  1000.0                  0.0   \n",
       "630446 2024-05-08 22:00:00+00:00                  5000.0                  0.0   \n",
       "672925 2024-08-14 20:00:00+00:00                 15000.0                  0.0   \n",
       "\n",
       "       EVENT_DURATION_HOURS full_fips_code  \n",
       "44383              0.000000          01001  \n",
       "44384              0.000000          01001  \n",
       "17465              6.333333          01001  \n",
       "18130             29.000000          01001  \n",
       "29615              0.000000          01001  \n",
       "...                     ...            ...  \n",
       "635761             3.066667          99153  \n",
       "630167             3.833333          99153  \n",
       "630182             3.166667          99153  \n",
       "630446             3.900000          99153  \n",
       "672925             6.500000          99153  \n",
       "\n",
       "[397039 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- 2. Prepare df_eaglei Key (already done) ---\n",
    "# Ensure required columns exist\n",
    "if not all(col in df_eaglei.columns for col in ['fips_code', 'EAGLEI_DT_UTC', 'customers_out']):\n",
    "     print(\"Error: Required columns missing from df_eaglei.\")\n",
    "     exit()\n",
    "# Select only necessary columns from Eaglei to save memory during merge\n",
    "df_eaglei_subset = df_eaglei[['fips_code', 'EAGLEI_DT_UTC', 'customers_out']].copy()\n",
    "print(f\"Prepared Eaglei subset with {len(df_eaglei_subset)} rows.\")\n",
    "\n",
    "#Handled mismatch in data_type\n",
    "df_eaglei_subset['EAGLEI_DT_UTC'] = pd.to_datetime(df_eaglei_subset['EAGLEI_DT_UTC'])\n",
    "df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC'])\n",
    "# And similarly for df_noaa['END_DT_UTC']\n",
    "# df_eaglei_subset['fips_code'] = df_eaglei_subset['fips_code'].astype(str).str.zfill(5)\n",
    "# This line below specifically targets the NaNs in END_DT_UTC\n",
    "df_noaa.dropna(subset=['END_DT_UTC', 'full_fips_code'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. Define Target & Tolerance ---\n",
    "# Let's look for outages within 6 hours AFTER a storm ends\n",
    "merge_tolerance = pd.Timedelta('6h')\n",
    "print(f\"Merge tolerance set to: {merge_tolerance}\")\n",
    "print(df_noaa['END_DT_UTC'].dtype)\n",
    "print(df_noaa['full_fips_code'].dtype)\n",
    "print(df_eaglei_subset['EAGLEI_DT_UTC'].dtype)\n",
    "print(df_eaglei_subset['fips_code'].dtype)\n",
    "\n",
    "# --- CRITICAL FIX: Remove rows with nulls in ANY key column ---\n",
    "# ...\n",
    "# --- 3. Sort DataFrames ---\n",
    "print(\"Sorting DataFrames (this might take time)...\")\n",
    "# Sort NOAA by the new full FIPS and storm END time\n",
    "df_noaa.sort_values(by=['full_fips_code', 'END_DT_UTC'], inplace=True)\n",
    "# Sort Eaglei subset by FIPS and outage time\n",
    "df_eaglei_subset.sort_values(by=['fips_code', 'EAGLEI_DT_UTC'], inplace=True)\n",
    "print(\"DataFrames sorted.\")\n",
    "df_noaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab5106c-d939-4a5b-87de-482bc38f946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Step 4: Merging NOAA and Eaglei Data...\n",
      "  Example Eaglei fips_code: 01037\n",
      "Prepared Eaglei subset with 191133068 rows.\n",
      "Merge tolerance set to: 0 days 06:00:00\n",
      "datetime64[ns, UTC]\n",
      "object\n",
      "datetime64[ns, UTC]\n",
      "object\n",
      "Sorting DataFrames (this might take time)...\n",
      "DataFrames sorted.\n",
      "  Example NOAA full_fips_code: 01001\n",
      "  Example Eaglei fips_code: 01037\n",
      "\n",
      "--- Verifying df_noaa is properly sorted for merge_asof ---\n",
      "1. full_fips_code is monotonically increasing: True\n",
      "2. END_DT_UTC is sorted within each full_fips_code group: True\n",
      "âœ“ df_noaa is properly sorted for merge_asof\n",
      "Performing merge_asof (this is memory intensive and may take significant time)...\n",
      "An unexpected error occurred during merge_asof: left keys must be sorted\n",
      "\n",
      "Step 4 (Merge) FAILED due to errors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Assume df_noaa exists from Step 3\n",
    "# Assume df_eaglei exists from Step 2\n",
    "df_noaa = pd.read_pickle(\"noaa_fips.pkl\")\n",
    "print(\"\\nStarting Step 4: Merging NOAA and Eaglei Data...\")\n",
    "\n",
    "# --- 1. Create full 5-digit FIPS code in df_noaa ---\n",
    "# print(\"Creating 5-digit FIPS code in NOAA data...\")\n",
    "# if 'STATE_FIPS' in df_noaa.columns and 'CZ_FIPS' in df_noaa.columns:\n",
    "#     # Ensure source columns are strings and handle potential floats/NaNs gracefully first\n",
    "#     df_noaa['STATE_FIPS_STR'] = df_noaa['STATE_FIPS'].astype(str).str.split('.').str[0]\n",
    "#     df_noaa['CZ_FIPS_STR'] = df_noaa['CZ_FIPS'].astype(str).str.split('.').str[0]\n",
    "\n",
    "#     # Pad STATE_FIPS to 2 digits, CZ_FIPS to 3 digits\n",
    "#     state_fips_padded = df_noaa['STATE_FIPS_STR'].str.zfill(2)\n",
    "#     cz_fips_padded = df_noaa['CZ_FIPS_STR'].str.zfill(3)\n",
    "\n",
    "#     # Concatenate, but only if both parts are valid (not NaN after conversion)\n",
    "#     # Create mask for valid rows\n",
    "#     valid_fips_mask = state_fips_padded.notna() & cz_fips_padded.notna()\n",
    "#     df_noaa['full_fips_code'] = np.nan # Initialize with NaN\n",
    "#     df_noaa.loc[valid_fips_mask, 'full_fips_code'] = state_fips_padded[valid_fips_mask] + cz_fips_padded[valid_fips_mask]\n",
    "\n",
    "#     # Drop intermediate columns\n",
    "#     df_noaa.drop(columns=['STATE_FIPS_STR', 'CZ_FIPS_STR'], inplace=True)\n",
    "\n",
    "#     # Check how many NaNs were created\n",
    "#     fips_nan_count = df_noaa['full_fips_code'].isna().sum()\n",
    "#     if fips_nan_count > 0:\n",
    "#         print(f\"  Warning: Created {fips_nan_count} NaN values in 'full_fips_code' due to missing STATE or CZ FIPS.\")\n",
    "#         # Optional: Drop rows with missing FIPS before merging if needed\n",
    "#         # df_noaa.dropna(subset=['full_fips_code'], inplace=True)\n",
    "\n",
    "#     print(\"  'full_fips_code' created.\")\n",
    "#     # Verify one FIPS code\n",
    "#     print(f\"  Example NOAA full_fips_code: {df_noaa['full_fips_code'].iloc[0] if not df_noaa.empty else 'N/A'}\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Error: STATE_FIPS or CZ_FIPS missing from df_noaa. Cannot create full FIPS code.\")\n",
    "#     # Handle error - cannot proceed with merge\n",
    "#     exit()\n",
    "\n",
    "# Verify Eaglei FIPS code formatting (should be done already)\n",
    "print(f\"  Example Eaglei fips_code: {df_eaglei['fips_code'].iloc[0] if not df_eaglei.empty else 'N/A'}\")\n",
    "\n",
    "\n",
    "# --- 2. Prepare df_eaglei Key (already done) ---\n",
    "# Ensure required columns exist\n",
    "if not all(col in df_eaglei.columns for col in ['fips_code', 'EAGLEI_DT_UTC', 'customers_out']):\n",
    "     print(\"Error: Required columns missing from df_eaglei.\")\n",
    "     exit()\n",
    "# Select only necessary columns from Eaglei to save memory during merge\n",
    "df_eaglei_subset = df_eaglei[['fips_code', 'EAGLEI_DT_UTC', 'customers_out']].copy()\n",
    "print(f\"Prepared Eaglei subset with {len(df_eaglei_subset)} rows.\")\n",
    "\n",
    "#Handled mismatch in data_type\n",
    "df_eaglei_subset['EAGLEI_DT_UTC'] = pd.to_datetime(df_eaglei_subset['EAGLEI_DT_UTC'])\n",
    "df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC'])\n",
    "# And similarly for df_noaa['END_DT_UTC']\n",
    "df_eaglei_subset['fips_code'] = df_eaglei_subset['fips_code'].astype(str).str.zfill(5)\n",
    "# This line below specifically targets the NaNs in END_DT_UTC\n",
    "df_noaa.dropna(subset=['END_DT_UTC', 'full_fips_code'], inplace=True)\n",
    "\n",
    "\n",
    "df_noaa.reset_index(drop=True, inplace=True)\n",
    "df_eaglei_subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# --- 4. Define Target & Tolerance ---\n",
    "# Let's look for outages within 6 hours AFTER a storm ends\n",
    "merge_tolerance = pd.Timedelta('6h')\n",
    "print(f\"Merge tolerance set to: {merge_tolerance}\")\n",
    "print(df_noaa['END_DT_UTC'].dtype)\n",
    "print(df_noaa['full_fips_code'].dtype)\n",
    "print(df_eaglei_subset['EAGLEI_DT_UTC'].dtype)\n",
    "print(df_eaglei_subset['fips_code'].dtype)\n",
    "\n",
    "# --- CRITICAL FIX: Remove rows with nulls in ANY key column ---\n",
    "# ...\n",
    "\n",
    "# Convert to integers (assuming no leading zeros are needed)\n",
    "df_noaa['full_fips_code_int'] = df_noaa['full_fips_code'].astype(int)\n",
    "df_eaglei_subset['fips_code_int'] = df_eaglei_subset['fips_code'].astype(int)\n",
    "\n",
    "# --- 3. Sort DataFrames ---\n",
    "print(\"Sorting DataFrames (this might take time)...\")\n",
    "# Sort NOAA by the new full FIPS and storm END time\n",
    "df_noaa = df_noaa.sort_values(by=['full_fips_code', 'END_DT_UTC'])\n",
    "# Sort Eaglei subset by FIPS and outage time\n",
    "df_eaglei_subset = df_eaglei_subset.sort_values(by=['fips_code', 'EAGLEI_DT_UTC'])\n",
    "print(\"DataFrames sorted.\")\n",
    "\n",
    "print(f\"  Example NOAA full_fips_code: {df_noaa['full_fips_code'].iloc[0] if not df_noaa.empty else 'N/A'}\")\n",
    "print(f\"  Example Eaglei fips_code: {df_eaglei['fips_code'].iloc[0] if not df_eaglei.empty else 'N/A'}\")\n",
    "\n",
    "print(\"\\n--- Verifying df_noaa is properly sorted for merge_asof ---\")\n",
    "\n",
    "# Check if full_fips_code is monotonically increasing\n",
    "fips_sorted = df_noaa['full_fips_code'].is_monotonic_increasing\n",
    "print(f\"1. full_fips_code is monotonically increasing: {fips_sorted}\")\n",
    "\n",
    "if not fips_sorted:\n",
    "    # Find where the sorting breaks\n",
    "    diff = df_noaa['full_fips_code'].iloc[1:] < df_noaa['full_fips_code'].iloc[:-1].values\n",
    "    if diff.any():\n",
    "        first_violation = diff.idxmax()\n",
    "        print(f\"  Sorting violation found at index {first_violation}:\")\n",
    "        print(f\"    {first_violation-1}: {df_noaa['full_fips_code'].iloc[first_violation-1]}\")\n",
    "        print(f\"    {first_violation}: {df_noaa['full_fips_code'].iloc[first_violation]}\")\n",
    "\n",
    "# Check if END_DT_UTC is sorted within each FIPS group\n",
    "group_checks = df_noaa.groupby('full_fips_code')['END_DT_UTC'].apply(lambda x: x.is_monotonic_increasing)\n",
    "all_groups_sorted = group_checks.all()\n",
    "print(f\"2. END_DT_UTC is sorted within each full_fips_code group: {all_groups_sorted}\")\n",
    "\n",
    "if not all_groups_sorted:\n",
    "    bad_groups = group_checks[~group_checks].index.tolist()\n",
    "    print(f\"  Found {len(bad_groups)} groups with sorting issues:\")\n",
    "    for i, fips in enumerate(bad_groups[:3]):  # Show first 3 problematic groups\n",
    "        group = df_noaa[df_noaa['full_fips_code'] == fips]\n",
    "        print(f\"  - FIPS {fips} (size {len(group)}):\")\n",
    "        # Find where the sorting breaks within this group\n",
    "        diff = group['END_DT_UTC'].iloc[1:] < group['END_DT_UTC'].iloc[:-1].values\n",
    "        if diff.any():\n",
    "            first_violation = diff.idxmax()\n",
    "            print(f\"    Out-of-order at index {first_violation}:\")\n",
    "            print(f\"      {first_violation-1}: {group.loc[first_violation-1, 'END_DT_UTC']}\")\n",
    "            print(f\"      {first_violation}: {group.loc[first_violation, 'END_DT_UTC']}\")\n",
    "\n",
    "# Only proceed with merge if both checks pass\n",
    "if fips_sorted and all_groups_sorted:\n",
    "    print(\"âœ“ df_noaa is properly sorted for merge_asof\")\n",
    "else:\n",
    "    print(\"âœ— df_noaa is NOT properly sorted for merge_asof - must fix before merging\")\n",
    "    # Consider adding: exit() or df_noaa = df_noaa.sort_values(...) to auto-fix\n",
    "\n",
    "print(\"Performing merge_asof (this is memory intensive and may take significant time)...\")\n",
    "try:\n",
    "    # Merge df_noaa (left) with df_eaglei_subset (right)\n",
    "    # For each storm in df_noaa, find the Eaglei record matching FIPS\n",
    "    # where Eaglei time is >= storm end time, within the tolerance.\n",
    "    merged_df = pd.merge_asof(\n",
    "        df_noaa,                     # Left DataFrame\n",
    "        df_eaglei_subset,            # Right DataFrame (subset)\n",
    "        left_on='END_DT_UTC',        # Time column in left df (storm end)\n",
    "        right_on='EAGLEI_DT_UTC',    # Time column in right df (outage time)\n",
    "        left_by='full_fips_code',    # Key column in left df (county FIPS)\n",
    "        right_by='fips_code',        # Key column in right df (county FIPS)\n",
    "        direction='forward',         # Find Eaglei time >= left time\n",
    "        tolerance=merge_tolerance    # Look ahead up to 6 hours\n",
    "    )\n",
    "    print(\"merge_asof complete.\")\n",
    "\n",
    "    # Rename columns coming from Eaglei for clarity, handling potential conflicts\n",
    "    merged_df.rename(columns={\n",
    "        'EAGLEI_DT_UTC': 'MATCHED_OUTAGE_DT_UTC',\n",
    "        'customers_out': 'MATCHED_CUSTOMERS_OUT'\n",
    "        # Note: 'fips_code' column from df_eaglei_subset will also be present\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Check results\n",
    "    print(f\"Merged DataFrame length: {len(merged_df)}\")\n",
    "    if len(merged_df) != len(df_noaa):\n",
    "         print(\"Warning: Length of merged df differs from original NOAA df. This shouldn't happen with merge_asof.\")\n",
    "\n",
    "    # See how many storms found a matching outage record within the window\n",
    "    matches_found = merged_df['MATCHED_OUTAGE_DT_UTC'].notna().sum()\n",
    "    print(f\"Found matching outage records for {matches_found} out of {len(merged_df)} storm events within the {merge_tolerance} window.\")\n",
    "\n",
    "except MemoryError:\n",
    "    print(\"!!! MEMORY ERROR during merge_asof !!!\")\n",
    "    print(\"The DataFrames are too large for a direct merge in available memory.\")\n",
    "    print(\"Next steps would require chunking, Dask, or more memory.\")\n",
    "    merged_df = None # Indicate failure\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during merge_asof: {e}\")\n",
    "    merged_df = None # Indicate failure\n",
    "\n",
    "\n",
    "# --- Display Info and Head of Merged Data (if successful) ---\n",
    "if merged_df is not None:\n",
    "    print(\"\\n--- Merged DataFrame Info ---\")\n",
    "    merged_df.info(verbose=True, show_counts=True)\n",
    "\n",
    "    print(\"\\n--- Merged DataFrame Head ---\")\n",
    "    print(merged_df[[\n",
    "        'EVENT_ID', 'full_fips_code', 'END_DT_UTC',\n",
    "        'MATCHED_OUTAGE_DT_UTC', 'MATCHED_CUSTOMERS_OUT'\n",
    "        # Add other relevant NOAA columns to view\n",
    "    ]].head())\n",
    "\n",
    "    print(\"\\nStep 4 (Merge) Complete.\")\n",
    "else:\n",
    "    print(\"\\nStep 4 (Merge) FAILED due to errors.\")\n",
    "\n",
    "# 'merged_df' now holds the result IF the merge was successful.\n",
    "# It has one row per original NOAA storm event.\n",
    "# Rows where no outage was found within the window will have NaT/NaN\n",
    "# in MATCHED_OUTAGE_DT_UTC and MATCHED_CUSTOMERS_OUT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ddadeec-855e-43f6-b5ed-057d9e2e1ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Eaglei subset with 191133068 rows.\n",
      "Merge tolerance set to: 0 days 06:00:00\n",
      "datetime64[ns, UTC]\n",
      "object\n",
      "datetime64[ns, UTC]\n",
      "object\n",
      "Sorting DataFrames (this might take time)...\n",
      "DataFrames sorted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_noaa = pd.read_pickle('noaa_fips.pkl')\n",
    "df_eaglei = pd.read_pickle('eaglei_2014_2024.pkl')\n",
    "\n",
    "# --- 2. Prepare df_eaglei Key (already done) ---\n",
    "# Ensure required columns exist\n",
    "if not all(col in df_eaglei.columns for col in ['fips_code', 'EAGLEI_DT_UTC', 'customers_out']):\n",
    "     print(\"Error: Required columns missing from df_eaglei.\")\n",
    "     exit()\n",
    "# Select only necessary columns from Eaglei to save memory during merge\n",
    "df_eaglei_subset = df_eaglei[['fips_code', 'EAGLEI_DT_UTC', 'customers_out']].copy()\n",
    "print(f\"Prepared Eaglei subset with {len(df_eaglei_subset)} rows.\")\n",
    "\n",
    "#Handled mismatch in data_type\n",
    "df_eaglei_subset['EAGLEI_DT_UTC'] = pd.to_datetime(df_eaglei_subset['EAGLEI_DT_UTC'])\n",
    "df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC'])\n",
    "# And similarly for df_noaa['END_DT_UTC']\n",
    "# df_eaglei_subset['fips_code'] = df_eaglei_subset['fips_code'].astype(str).str.zfill(5)\n",
    "# This line below specifically targets the NaNs in END_DT_UTC\n",
    "df_noaa.dropna(subset=['END_DT_UTC', 'full_fips_code'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. Define Target & Tolerance ---\n",
    "# Let's look for outages within 6 hours AFTER a storm ends\n",
    "merge_tolerance = pd.Timedelta('6h')\n",
    "print(f\"Merge tolerance set to: {merge_tolerance}\")\n",
    "print(df_noaa['END_DT_UTC'].dtype)\n",
    "print(df_noaa['full_fips_code'].dtype)\n",
    "print(df_eaglei_subset['EAGLEI_DT_UTC'].dtype)\n",
    "print(df_eaglei_subset['fips_code'].dtype)\n",
    "\n",
    "# --- CRITICAL FIX: Remove rows with nulls in ANY key column ---\n",
    "# ...\n",
    "# --- 3. Sort DataFrames ---\n",
    "print(\"Sorting DataFrames (this might take time)...\")\n",
    "# Sort NOAA by the new full FIPS and storm END time\n",
    "df_noaa.sort_values(by=['full_fips_code', 'END_DT_UTC'], inplace=True)\n",
    "# Sort Eaglei subset by FIPS and outage time\n",
    "df_eaglei_subset.sort_values(by=['fips_code', 'EAGLEI_DT_UTC'], inplace=True)\n",
    "print(\"DataFrames sorted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff23bae-2016-4280-863f-064c9672793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA clean count: 394824\n",
      "EagleI clean count: 190289642\n"
     ]
    }
   ],
   "source": [
    "# First, standardize all FIPS codes (including the \"bad\" ones)\n",
    "def standardize_fips(fips):\n",
    "    try:\n",
    "        fips = str(int(float(fips))).zfill(5)\n",
    "        return fips if fips[:2] in [f\"{i:02d}\" for i in range(1,57)] else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply standardization\n",
    "df_noaa['full_fips_code'] = df_noaa['full_fips_code'].apply(standardize_fips)\n",
    "df_eaglei_subset['fips_code'] = df_eaglei_subset['fips_code'].apply(standardize_fips)\n",
    "\n",
    "# Then filter to only valid FIPS codes\n",
    "valid_state_codes = [f\"{i:02d}\" for i in range(1,57)]  # 01-56\n",
    "df_noaa = df_noaa[df_noaa['full_fips_code'].str[:2].isin(valid_state_codes)]\n",
    "df_eaglei_subset = df_eaglei_subset[df_eaglei_subset['fips_code'].str[:2].isin(valid_state_codes)]\n",
    "\n",
    "# Verify\n",
    "print(f\"NOAA clean count: {len(df_noaa)}\")\n",
    "print(f\"EagleI clean count: {len(df_eaglei_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587d556c-11dd-460c-a0dc-8eec38995ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df_noaa = pd.read_pickle(\"noaa_fips_cleaned_sorted.pkl\")\n",
    "df_eaglei_subset = pd.read_pickle(\"eaglei_subset_clean_sorted.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae39bcda-2acf-4b65-830f-a910718ae40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key column verification:\n",
      "NOAA FIPS unique count: 3163\n",
      "EagleI FIPS unique count: 3086\n",
      "FIPS in NOAA but not EagleI: 87\n",
      "FIPS in EagleI but not NOAA: 10\n",
      "\n",
      "Dtypes:\n",
      "NOAA full_fips_code: object\n",
      "EagleI fips_code: object\n"
     ]
    }
   ],
   "source": [
    "# Check that the key columns contain identical values\n",
    "print(\"\\nKey column verification:\")\n",
    "print(f\"NOAA FIPS unique count: {df_noaa['full_fips_code'].nunique()}\")\n",
    "print(f\"EagleI FIPS unique count: {df_eaglei_subset['fips_code'].nunique()}\")\n",
    "\n",
    "# Check for values in one but not the other\n",
    "noaa_fips = set(df_noaa['full_fips_code'].unique())\n",
    "eaglei_fips = set(df_eaglei_subset['fips_code'].unique())\n",
    "print(f\"FIPS in NOAA but not EagleI: {len(noaa_fips - eaglei_fips)}\")\n",
    "print(f\"FIPS in EagleI but not NOAA: {len(eaglei_fips - noaa_fips)}\")\n",
    "\n",
    "# Check dtypes\n",
    "print(\"\\nDtypes:\")\n",
    "print(f\"NOAA full_fips_code: {df_noaa['full_fips_code'].dtype}\")\n",
    "print(f\"EagleI fips_code: {df_eaglei_subset['fips_code'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25fa18c-70a8-4d5c-8348-3fc4339e360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying df_noaa is properly sorted for merge_asof ---\n",
      "1. full_fips_code is monotonically increasing: True\n",
      "2. END_DT_UTC is sorted within each full_fips_code group: True\n",
      "âœ“ df_noaa is properly sorted for merge_asof\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Verifying df_noaa is properly sorted for merge_asof ---\")\n",
    "\n",
    "# Check if full_fips_code is monotonically increasing\n",
    "fips_sorted = df_noaa['full_fips_code'].is_monotonic_increasing\n",
    "print(f\"1. full_fips_code is monotonically increasing: {fips_sorted}\")\n",
    "\n",
    "if not fips_sorted:\n",
    "    # Find where the sorting breaks\n",
    "    diff = df_noaa['full_fips_code'].iloc[1:] < df_noaa['full_fips_code'].iloc[:-1].values\n",
    "    if diff.any():\n",
    "        first_violation = diff.idxmax()\n",
    "        print(f\"  Sorting violation found at index {first_violation}:\")\n",
    "        print(f\"    {first_violation-1}: {df_noaa['full_fips_code'].iloc[first_violation-1]}\")\n",
    "        print(f\"    {first_violation}: {df_noaa['full_fips_code'].iloc[first_violation]}\")\n",
    "\n",
    "# Check if END_DT_UTC is sorted within each FIPS group\n",
    "group_checks = df_noaa.groupby('full_fips_code')['END_DT_UTC'].apply(lambda x: x.is_monotonic_increasing)\n",
    "all_groups_sorted = group_checks.all()\n",
    "print(f\"2. END_DT_UTC is sorted within each full_fips_code group: {all_groups_sorted}\")\n",
    "\n",
    "if not all_groups_sorted:\n",
    "    bad_groups = group_checks[~group_checks].index.tolist()\n",
    "    print(f\"  Found {len(bad_groups)} groups with sorting issues:\")\n",
    "    for i, fips in enumerate(bad_groups[:3]):  # Show first 3 problematic groups\n",
    "        group = df_noaa[df_noaa['full_fips_code'] == fips]\n",
    "        print(f\"  - FIPS {fips} (size {len(group)}):\")\n",
    "        # Find where the sorting breaks within this group\n",
    "        diff = group['END_DT_UTC'].iloc[1:] < group['END_DT_UTC'].iloc[:-1].values\n",
    "        if diff.any():\n",
    "            first_violation = diff.idxmax()\n",
    "            print(f\"    Out-of-order at index {first_violation}:\")\n",
    "            print(f\"      {first_violation-1}: {group.loc[first_violation-1, 'END_DT_UTC']}\")\n",
    "            print(f\"      {first_violation}: {group.loc[first_violation, 'END_DT_UTC']}\")\n",
    "\n",
    "# Only proceed with merge if both checks pass\n",
    "if fips_sorted and all_groups_sorted:\n",
    "    print(\"âœ“ df_noaa is properly sorted for merge_asof\")\n",
    "else:\n",
    "    print(\"âœ— df_noaa is NOT properly sorted for merge_asof - must fix before merging\")\n",
    "    # Consider adding: exit() or df_noaa = df_noaa.sort_values(...) to auto-fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2215609c-cf81-4d89-999f-a05734724e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values check:\n",
      "full_fips_code nulls: 0\n",
      "END_DT_UTC nulls: 0\n",
      "\n",
      "Data type consistency:\n",
      "FIPS code sample: ['01001', '01001', '01001']\n",
      "Timestamp sample: [Timestamp('2014-01-11 12:15:00+0000', tz='UTC'), Timestamp('2014-01-11 12:20:00+0000', tz='UTC'), Timestamp('2014-04-07 18:35:00+0000', tz='UTC')]\n",
      "\n",
      "FIPS code formatting:\n",
      "Lengths: full_fips_code\n",
      "5    394824\n",
      "Name: count, dtype: int64\n",
      "Leading zeros sample: full_fips_code\n",
      "False    353274\n",
      "True      41550\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Timezone info:\n",
      "UTC\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for null values in key columns\n",
    "print(\"\\nNull values check:\")\n",
    "print(f\"full_fips_code nulls: {df_noaa['full_fips_code'].isnull().sum()}\")\n",
    "print(f\"END_DT_UTC nulls: {df_noaa['END_DT_UTC'].isnull().sum()}\")\n",
    "\n",
    "# 2. Verify consistent data types\n",
    "print(\"\\nData type consistency:\")\n",
    "print(\"FIPS code sample:\", df_noaa['full_fips_code'].head(3).tolist())\n",
    "print(\"Timestamp sample:\", df_noaa['END_DT_UTC'].head(3).tolist())\n",
    "\n",
    "# 3. Check string representations of FIPS codes\n",
    "print(\"\\nFIPS code formatting:\")\n",
    "print(\"Lengths:\", df_noaa['full_fips_code'].str.len().value_counts())\n",
    "print(\"Leading zeros sample:\", df_noaa['full_fips_code'].str.startswith('0').value_counts())\n",
    "\n",
    "# 4. Timezone check\n",
    "print(\"\\nTimezone info:\")\n",
    "print(df_noaa['END_DT_UTC'].dt.tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be9b2a41-1b95-4ab9-87ed-3755c131a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "df_noaa['END_DT_UTC'] = pd.to_datetime(df_noaa['END_DT_UTC']).dt.tz_localize(None)\n",
    "print(df_noaa['END_DT_UTC'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "801bc426-2a6e-4b16-bc6b-e1f354c8bdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noaa.sort_values(by=['full_fips_code', 'END_DT_UTC'], inplace=True)\n",
    "\n",
    "df_noaa['END_DT_UTC'].is_monotonic_increasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4e48f-5f1a-44f3-a324-39213f0e27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eaglei_subset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26440e8b-6d18-414e-ab37-3da47552943e",
   "metadata": {},
   "source": [
    "## Successful merge code below\n",
    "The merge logic using the numpy array for the dataframes of merge_asof worked over pandas df.\n",
    "\n",
    "I suspect the reason to be that the keys of merging(esp the timezone aware date-time column) contains nano second values which are not sorted by pandas(up to that s.f.) which was causing the sort error in merge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d671ac-35fe-4e16-8992-bc100177d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_for_merge_asof(df, by_col, on_col):\n",
    "    \"\"\"Ensure perfect sorting that merge_asof will accept\"\"\"\n",
    "    # Convert to numpy arrays for precise control\n",
    "    groups = df[by_col].values\n",
    "    times = df[on_col].values.astype('datetime64[ns]')\n",
    "    \n",
    "    # Create index array\n",
    "    index = np.arange(len(df))\n",
    "    \n",
    "    # Sort first by group, then by time using mergesort\n",
    "    # This creates a lexsort with group as primary key, time as secondary\n",
    "    sorted_indices = np.lexsort((times, groups))\n",
    "    \n",
    "    # Apply the sorting\n",
    "    sorted_df = df.iloc[sorted_indices].reset_index(drop=True)\n",
    "    \n",
    "    # Final verification\n",
    "    verify_sorting(sorted_df, by_col, on_col)\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "def verify_sorting(df, by_col, on_col):\n",
    "    \"\"\"Rigorous sorting verification\"\"\"\n",
    "    current_group = None\n",
    "    last_time = None\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df[by_col].iloc[i] != current_group:\n",
    "            current_group = df[by_col].iloc[i]\n",
    "            last_time = df[on_col].iloc[i]\n",
    "        else:\n",
    "            if df[on_col].iloc[i] < last_time:\n",
    "                raise ValueError(f\"Sort violation at index {i}\")\n",
    "            last_time = df[on_col].iloc[i]\n",
    "    print(\"âœ“ Perfect sorting verified\")\n",
    "\n",
    "# Prepare both DataFrames\n",
    "df_noaa_prepared = prepare_for_merge_asof(df_noaa, 'full_fips_code', 'END_DT_UTC')\n",
    "df_eaglei_prepared = prepare_for_merge_asof(df_eaglei_renamed, 'full_fips_code', 'END_DT_UTC')\n",
    "\n",
    "# Perform the merge\n",
    "try:\n",
    "    merged_df = pd.merge_asof(\n",
    "        df_noaa_prepared,\n",
    "        df_eaglei_prepared,\n",
    "        on='END_DT_UTC',\n",
    "        by='full_fips_code',\n",
    "        direction='forward',\n",
    "        tolerance=pd.Timedelta('2h')\n",
    "    )\n",
    "    print(\"âœ… Merge succeeded after rigorous sorting\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Unexpected error: {e}\")\n",
    "    print(\"\\nâš ï¸ Implementing manual merge_asof logic as fallback\")\n",
    "    \n",
    "    # Manual implementation of merge_asof\n",
    "    merged_records = []\n",
    "    for fips in df_noaa_prepared['full_fips_code'].unique():\n",
    "        noaa_group = df_noaa_prepared[df_noaa_prepared['full_fips_code'] == fips]\n",
    "        eaglei_group = df_eaglei_prepared[df_eaglei_prepared['full_fips_code'] == fips]\n",
    "        \n",
    "        for _, noaa_row in noaa_group.iterrows():\n",
    "            # Find closest matching row in eaglei data\n",
    "            matches = eaglei_group[\n",
    "                (eaglei_group['END_DT_UTC'] >= noaa_row['END_DT_UTC']) &\n",
    "                (eaglei_group['END_DT_UTC'] <= noaa_row['END_DT_UTC'] + pd.Timedelta('2h'))\n",
    "            ]\n",
    "            \n",
    "            if not matches.empty:\n",
    "                best_match = matches.iloc[0]  # First match due to sorting\n",
    "                merged_row = {**noaa_row.to_dict(), **best_match.to_dict()}\n",
    "                merged_records.append(merged_row)\n",
    "            else:\n",
    "                merged_records.append(noaa_row.to_dict())\n",
    "    \n",
    "    merged_df = pd.DataFrame(merged_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e58b0d-c062-4d2a-87f8-0a21f3a8c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fips_formatting(df, col_name):\n",
    "    \"\"\"Verify all FIPS codes are 5-digit strings\"\"\"\n",
    "    print(f\"\\nChecking FIPS formatting in column '{col_name}':\")\n",
    "    \n",
    "    # Check length\n",
    "    length_check = df[col_name].str.len() != 5\n",
    "    bad_length = df[length_check]\n",
    "    \n",
    "    # Check digit-only\n",
    "    digit_check = ~df[col_name].str.match(r'^\\d{5}$', na=False)\n",
    "    bad_digits = df[digit_check]\n",
    "    \n",
    "    # Check leading zeros (first 2 digits should be state code 01-56)\n",
    "    state_code_check = ~df[col_name].str[:2].isin([f\"{i:02d}\" for i in range(1,57)])\n",
    "    bad_state_codes = df[state_code_check]\n",
    "    \n",
    "    # Report findings\n",
    "    if len(bad_length) > 0:\n",
    "        print(f\"  Found {len(bad_length)} entries with wrong length:\")\n",
    "        print(bad_length[col_name].value_counts().head())\n",
    "    \n",
    "    if len(bad_digits) > 0:\n",
    "        print(f\"  Found {len(bad_digits)} non-numeric entries:\")\n",
    "        print(bad_digits[col_name].unique())\n",
    "    \n",
    "    if len(bad_state_codes) > 0:\n",
    "        print(f\"  Found {len(bad_state_codes)} questionable state codes:\")\n",
    "        print(bad_state_codes[col_name].str[:2].value_counts())\n",
    "    \n",
    "    # Return clean mask\n",
    "    return ~(length_check | digit_check | state_code_check)\n",
    "\n",
    "# Run checks on both datasets\n",
    "noaa_clean_mask = check_fips_formatting(df_noaa, 'full_fips_code')\n",
    "eaglei_clean_mask = check_fips_formatting(df_eaglei_subset, 'fips_code')\n",
    "\n",
    "# Show clean/dirty counts\n",
    "print(f\"\\nNOAA clean/dirty counts: {noaa_clean_mask.sum()} clean, {len(df_noaa)-noaa_clean_mask.sum()} dirty\")\n",
    "print(f\"EagleI clean/dirty counts: {eaglei_clean_mask.sum()} clean, {len(df_eaglei_subset)-eaglei_clean_mask.sum()} dirty\")\n",
    "\n",
    "# Optional: Filter to only clean FIPS codes\n",
    "df_noaa_clean = df_noaa[noaa_clean_mask].copy()\n",
    "df_eaglei_clean = df_eaglei_subset[eaglei_clean_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e98c3533-8c8d-440b-a93f-4dabfc4224a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA value types:\n",
      "full_fips_code\n",
      "<class 'str'>    394824\n",
      "Name: count, dtype: int64\n",
      "EagleI value types:\n",
      "fips_code\n",
      "<class 'str'>    190289642\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check actual types of values (not just dtypes)\n",
    "def check_value_types(series):\n",
    "    return series.apply(lambda x: type(x)).value_counts()\n",
    "\n",
    "print(\"NOAA value types:\")\n",
    "print(check_value_types(df_noaa['full_fips_code']))\n",
    "\n",
    "print(\"EagleI value types:\")\n",
    "print(check_value_types(df_eaglei_subset['fips_code']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f51e65d-13bc-4cff-bcfb-c873513df3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA END_DT_UTC sample: ['2014-01-11T12:15:00.000000000']\n",
      "EagleI EAGLEI_DT_UTC sample: ['2014-11-01T16:15:00.000000000']\n",
      "NOAA missing FIPS: 0\n",
      "EagleI missing FIPS: 0\n",
      "\n",
      "Memory usage:\n",
      "NOAA: 1011.52597 MB\n",
      "EagleI: 14842.592076 MB\n"
     ]
    }
   ],
   "source": [
    "# 1. Verify time column formats\n",
    "print(\"NOAA END_DT_UTC sample:\", df_noaa['END_DT_UTC'].head(1).values)\n",
    "print(\"EagleI EAGLEI_DT_UTC sample:\", df_eaglei_subset['EAGLEI_DT_UTC'].head(1).values)\n",
    "\n",
    "# 2. Confirm no missing keys\n",
    "print(\"NOAA missing FIPS:\", df_noaa['full_fips_code'].isna().sum())\n",
    "print(\"EagleI missing FIPS:\", df_eaglei_subset['fips_code'].isna().sum())\n",
    "\n",
    "# 3. Check memory usage (for large merges)\n",
    "print(\"\\nMemory usage:\")\n",
    "print(\"NOAA:\", df_noaa.memory_usage(deep=True).sum()/1e6, \"MB\")\n",
    "print(\"EagleI:\", df_eaglei_subset.memory_usage(deep=True).sum()/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94aff0f0-6109-4f12-9df0-0f572dcde964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noaa.to_pickle(\"noaa_fips_cleaned_sorted.pkl\")\n",
    "df_eaglei_subset.to_pickle(\"eaglei_subset_clean_sorted.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b3d4e46-d22b-427c-98c7-34d090bccc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA sort valid: True\n",
      "EagleI sort valid: True\n"
     ]
    }
   ],
   "source": [
    "def verify_merge_asof_sort(df, by_col, on_col):\n",
    "    \"\"\"Verify DataFrame is properly sorted for merge_asof\"\"\"\n",
    "    # 1. Check primary key is monotonic\n",
    "    if not df[by_col].is_monotonic_increasing:\n",
    "        print(f\"Primary sort key '{by_col}' is NOT monotonic!\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Check secondary key is sorted within primary key groups\n",
    "    group_status = df.groupby(by_col)[on_col].agg(lambda x: x.is_monotonic_increasing)\n",
    "    if not group_status.all():\n",
    "        bad_groups = group_status[~group_status].index.tolist()\n",
    "        print(f\"Found {len(bad_groups)} groups where '{on_col}' isn't sorted\")\n",
    "        print(\"First 3 bad groups:\", bad_groups[:3])\n",
    "        return False\n",
    "    \n",
    "    # 3. Check no NaN values in sort keys\n",
    "    if df[[by_col, on_col]].isna().any().any():\n",
    "        print(\"NaN values found in sort keys!\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Verify both DataFrames\n",
    "print(\"NOAA sort valid:\", verify_merge_asof_sort(df_noaa, 'full_fips_code', 'END_DT_UTC'))\n",
    "print(\"EagleI sort valid:\", verify_merge_asof_sort(df_eaglei_subset, 'fips_code', 'EAGLEI_DT_UTC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3d7a5a-ddb2-41fa-a331-948441ef8c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA actually sorted: True\n",
      "EagleI actually sorted: True\n"
     ]
    }
   ],
   "source": [
    "def is_actually_sorted(df, keys):\n",
    "    \"\"\"Absolute verification by re-sorting and comparing\"\"\"\n",
    "    sorted_copy = df.sort_values(by=keys, kind='mergesort').reset_index(drop=True)\n",
    "    return df.reset_index(drop=True).equals(sorted_copy)\n",
    "\n",
    "print(\"NOAA actually sorted:\", \n",
    "      is_actually_sorted(df_noaa, ['full_fips_code', 'END_DT_UTC']))\n",
    "print(\"EagleI actually sorted:\", \n",
    "      is_actually_sorted(df_eaglei_subset, ['fips_code', 'EAGLEI_DT_UTC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d519e85e-7e43-433b-ac07-9895ba193f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA timezone: UTC\n",
      "EagleI timezone: UTC\n"
     ]
    }
   ],
   "source": [
    "print(\"NOAA timezone:\", df_noaa['END_DT_UTC'].dt.tz)\n",
    "print(\"EagleI timezone:\", df_eaglei_subset['EAGLEI_DT_UTC'].dt.tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c31c3a8-bcec-4cbc-8023-ea33a8379254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical cores (threads): 20\n",
      "Physical cores: 20\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "logical_cores = psutil.cpu_count(logical=True)     # Threads (logical CPUs)\n",
    "physical_cores = psutil.cpu_count(logical=False)   # Physical cores\n",
    "\n",
    "print(f\"Logical cores (threads): {logical_cores}\")\n",
    "print(f\"Physical cores: {physical_cores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ff8650-988f-4244-8ada-f762a488d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask                      2023.8.1                 pypi_0    pypi\n",
      "dask-core                 2025.2.0        py312h06a4308_0  \n",
      "dask-expr                 2.0.0           py312h06a4308_0  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list | grep dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90f69ab-e88c-40cc-94c9-42192ca7fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas                    2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
